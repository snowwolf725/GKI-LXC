diff --git a/arch/arm64/configs/gki_defconfig b/arch/arm64/configs/gki_defconfig
index 99fe0c1c0..6e8563989 100644
--- a/arch/arm64/configs/gki_defconfig
+++ b/arch/arm64/configs/gki_defconfig
@@ -7,6 +7,8 @@ CONFIG_BPF_JIT=y
 CONFIG_BPF_JIT_ALWAYS_ON=y
 # CONFIG_BPF_UNPRIV_DEFAULT_OFF is not set
 CONFIG_PREEMPT=y
+CONFIG_SLIM_SCHED=y
+CONFIG_SCHED_CLASS_EXT=y
 CONFIG_IRQ_TIME_ACCOUNTING=y
 CONFIG_TASKSTATS=y
 CONFIG_TASK_XACCT=y
@@ -133,9 +135,6 @@ CONFIG_ANON_VMA_NAME=y
 CONFIG_USERFAULTFD=y
 CONFIG_LRU_GEN=y
 CONFIG_LRU_GEN_ENABLED=y
-CONFIG_DAMON=y
-CONFIG_DAMON_VADDR=y
-CONFIG_DAMON_SYSFS=y
 CONFIG_NET=y
 CONFIG_PACKET=y
 CONFIG_UNIX=y
@@ -629,6 +628,7 @@ CONFIG_F2FS_FS=y
 CONFIG_F2FS_FS_SECURITY=y
 CONFIG_F2FS_FS_COMPRESSION=y
 CONFIG_F2FS_UNFAIR_RWSEM=y
+CONFIG_F2FS_SEQZONE=y
 CONFIG_FS_ENCRYPTION=y
 CONFIG_FS_ENCRYPTION_INLINE_CRYPT=y
 CONFIG_FS_VERITY=y
@@ -771,4 +771,4 @@ CONFIG_PID_IN_CONTEXTIDR=y
 CONFIG_KUNIT=y
 CONFIG_KUNIT_DEBUGFS=y
 # CONFIG_KUNIT_DEFAULT_ENABLED is not set
-# CONFIG_RUNTIME_TESTING_MENU is not set
+# CONFIG_RUNTIME_TESTING_MENU is not set
\ No newline at end of file
diff --git a/fs/f2fs/Kconfig b/fs/f2fs/Kconfig
index 03ef08753..896352f83 100644
--- a/fs/f2fs/Kconfig
+++ b/fs/f2fs/Kconfig
@@ -99,6 +99,13 @@ config F2FS_FS_COMPRESSION
 	  Enable filesystem-level compression on f2fs regular files,
 	  multiple back-end compression algorithms are supported.
 
+config F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	bool "F2FS compression feature (fixed output)"
+	depends on F2FS_FS_COMPRESSION
+	default y
+	help
+	  Enable fixed-output compression.
+
 config F2FS_FS_LZO
 	bool "LZO compression support"
 	depends on F2FS_FS_COMPRESSION
@@ -150,3 +157,22 @@ config F2FS_UNFAIR_RWSEM
 	help
 	  Use unfair rw_semaphore, if system configured IO priority by block
 	  cgroup.
+
+config F2FS_APPBOOST
+       bool
+       default y
+       help
+         Enable Appboost
+
+config F2FS_FS_DEDUP
+	bool "F2FS dedup feature"
+	depends on F2FS_FS
+	default y
+	help
+	  Enable the file dedup function.
+
+config F2FS_SEQZONE
+	bool "F2FS SeqZone feature"
+	depends on F2FS_FS && FS_ENCRYPTION_INLINE_CRYPT && F2FS_FS_DEDUP
+	help
+	  Improve Random Write Speed when inlinecrypt is enabled.
diff --git a/fs/f2fs/Makefile b/fs/f2fs/Makefile
index 8a7322d22..183c96303 100644
--- a/fs/f2fs/Makefile
+++ b/fs/f2fs/Makefile
@@ -10,3 +10,6 @@ f2fs-$(CONFIG_F2FS_FS_POSIX_ACL) += acl.o
 f2fs-$(CONFIG_FS_VERITY) += verity.o
 f2fs-$(CONFIG_F2FS_FS_COMPRESSION) += compress.o
 f2fs-$(CONFIG_F2FS_IOSTAT) += iostat.o
+ifeq ($(CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT),y)
+f2fs-$(CONFIG_ARM64) += $(addprefix lz4armv8/, lz4accel.o lz4armv8.o)
+endif
diff --git a/fs/f2fs/checkpoint.c b/fs/f2fs/checkpoint.c
old mode 100644
new mode 100755
index f76778434..cad89452f
--- a/fs/f2fs/checkpoint.c
+++ b/fs/f2fs/checkpoint.c
@@ -173,9 +173,12 @@ static bool __is_bitmap_valid(struct f2fs_sb_info *sbi, block_t blkaddr,
 	return exist;
 }
 
-static bool __f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
+bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 					block_t blkaddr, int type)
 {
+	if (time_to_inject(sbi, FAULT_BLKADDR))
+		return false;
+
 	switch (type) {
 	case META_NAT:
 		break;
@@ -230,20 +233,6 @@ static bool __f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 	return true;
 }
 
-bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
-					block_t blkaddr, int type)
-{
-	if (time_to_inject(sbi, FAULT_BLKADDR_VALIDITY))
-		return false;
-	return __f2fs_is_valid_blkaddr(sbi, blkaddr, type);
-}
-
-bool f2fs_is_valid_blkaddr_raw(struct f2fs_sb_info *sbi,
-					block_t blkaddr, int type)
-{
-	return __f2fs_is_valid_blkaddr(sbi, blkaddr, type);
-}
-
 /*
  * Readahead CP/NAT/SIT/SSA/POR pages
  */
@@ -706,6 +695,25 @@ static int recover_orphan_inode(struct f2fs_sb_info *sbi, nid_t ino)
 		goto err_out;
 	}
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (is_inode_flag_set(inode, FI_REVOKE_DEDUP)) {
+		f2fs_notice(sbi, "recover orphan: ino[%u] set revoke, flags[%lu]",
+				ino, F2FS_I(inode)->flags[0]);
+		f2fs_bug_on(sbi, is_inode_flag_set(inode, FI_DOING_DEDUP));
+		err = f2fs_truncate_dedup_inode(inode, FI_REVOKE_DEDUP);
+		iput(inode);
+		return err;
+	}
+
+	if (is_inode_flag_set(inode, FI_DOING_DEDUP)) {
+		f2fs_notice(sbi, "recover orphan: ino[%u] set doing dedup, flags[%lu]",
+				ino, F2FS_I(inode)->flags[0]);
+		err = f2fs_truncate_dedup_inode(inode, FI_DOING_DEDUP);
+		iput(inode);
+		return err;
+	}
+#endif
+
 	clear_nlink(inode);
 
 	/* truncate all the data during iput */
@@ -1598,9 +1606,8 @@ static int do_checkpoint(struct f2fs_sb_info *sbi, struct cp_control *cpc)
 	 */
 	if (f2fs_sb_has_encrypt(sbi) || f2fs_sb_has_verity(sbi) ||
 		f2fs_sb_has_compression(sbi))
-		f2fs_bug_on(sbi,
-			invalidate_inode_pages2_range(META_MAPPING(sbi),
-				MAIN_BLKADDR(sbi), MAX_BLKADDR(sbi) - 1));
+		f2fs_truncate_meta_inode_pages(sbi, MAIN_BLKADDR(sbi),
+					MAX_BLKADDR(sbi) - MAIN_BLKADDR(sbi));
 
 	f2fs_release_ino_entry(sbi, false);
 
diff --git a/fs/f2fs/compress.c b/fs/f2fs/compress.c
old mode 100644
new mode 100755
index 2ca8cb607..caa7b1f67
--- a/fs/f2fs/compress.c
+++ b/fs/f2fs/compress.c
@@ -19,6 +19,10 @@
 #include "node.h"
 #include "segment.h"
 #include <trace/events/f2fs.h>
+#if defined(CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT) || defined(__ARCH_HAS_LZ4_ACCELERATOR)
+#include "lz4armv8/lz4accel.h"
+#include "f2fs_lz4.h"
+#endif
 
 static struct kmem_cache *cic_entry_slab;
 static struct kmem_cache *dic_entry_slab;
@@ -56,6 +60,9 @@ struct f2fs_compress_ops {
 	void (*destroy_decompress_ctx)(struct decompress_io_ctx *dic);
 	int (*decompress_pages)(struct decompress_io_ctx *dic);
 	bool (*is_level_valid)(int level);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	int (*inplace_margin)(int decompressed_sz);
+#endif
 };
 
 static unsigned int offset_in_cluster(struct compress_ctx *cc, pgoff_t index)
@@ -97,6 +104,17 @@ static void f2fs_set_compressed_page(struct page *page,
 	page->mapping = inode->i_mapping;
 }
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static void f2fs_restore_compressed_page(struct page *page, pgoff_t index)
+{
+	/*
+	 * restore original page index, which is changed for fscrypt iv
+	 * index by inplace io
+	 */
+	page->index = index;
+}
+#endif
+
 static void f2fs_drop_rpages(struct compress_ctx *cc, int len, bool unlock)
 {
 	int i;
@@ -156,6 +174,11 @@ void f2fs_destroy_compress_ctx(struct compress_ctx *cc, bool reuse)
 	cc->nr_rpages = 0;
 	cc->nr_cpages = 0;
 	cc->valid_nr_cpages = 0;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	memset(cc->di, 0, sizeof(cc->di));
+	cc->rofs = 0;
+	cc->cofs = 0;
+#endif
 	if (!reuse)
 		cc->cluster_idx = NULL_CLUSTER;
 }
@@ -267,43 +290,104 @@ static void lz4_destroy_compress_ctx(struct compress_ctx *cc)
 
 static int lz4_compress_pages(struct compress_ctx *cc)
 {
-	int len = -EINVAL;
-	unsigned char level = F2FS_I(cc->inode)->i_compress_level;
+	if (f2fs_compress_layout(cc->inode) == COMPRESS_FIXED_INPUT) {
+		int len = -EINVAL;
+		unsigned char level = F2FS_I(cc->inode)->i_compress_level;
 
-	if (!level)
-		len = LZ4_compress_default(cc->rbuf, cc->cbuf->cdata, cc->rlen,
+		if (!level)
+			len = LZ4_compress_default(cc->rbuf, cc->cbuf->cdata, cc->rlen,
 						cc->clen, cc->private);
+
 #ifdef CONFIG_F2FS_FS_LZ4HC
-	else
-		len = LZ4_compress_HC(cc->rbuf, cc->cbuf->cdata, cc->rlen,
+		else
+			len = LZ4_compress_HC(cc->rbuf, cc->cbuf->cdata, cc->rlen,
 					cc->clen, level, cc->private);
 #endif
-	if (len < 0)
-		return len;
-	if (!len)
-		return -EAGAIN;
+		if (len < 0)
+			return len;
+		if (!len)
+			return -EAGAIN;
+
+		cc->clen = len;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	} else {
+		int slen = (cc->nr_rpages << PAGE_SHIFT) - cc->rofs;
+		void *src = cc->rbuf + cc->rofs;
+		void *dst = (void *)cc->cbuf + cc->cofs;
+		int dlen;
+
+		dlen = LZ4_compress_destSize(src, dst, &slen, PAGE_SIZE, cc->private);
+		if (!dlen)
+			return -EAGAIN;
+
+		if (dlen != PAGE_SIZE && cc->rofs + slen != cc->nr_rpages << PAGE_SHIFT) {
+			dlen = round_up(dlen, PAGE_SIZE);
+		}
+
+		cc->rofs += slen;
+		cc->cofs += dlen;
+#endif
+	}
 
-	cc->clen = len;
 	return 0;
 }
 
 static int lz4_decompress_pages(struct decompress_io_ctx *dic)
 {
-	int ret;
+	unsigned long expected;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	bool accel = false;
+#endif
+	int ret = 0;
 
-	ret = LZ4_decompress_safe(dic->cbuf->cdata, dic->rbuf,
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT) {
+		expected = PAGE_SIZE << dic->log_cluster_size;
+		ret = LZ4_decompress_safe(dic->cbuf->cdata, dic->rbuf,
 						dic->clen, dic->rlen);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	} else {
+		uint8_t *dst = (uint8_t *)dic->rbuf + dic->rofs;
+		const uint8_t *src = (uint8_t *)dic->cbuf + dic->cofs;
+		uint8_t *dstptr = dst;
+		const uint8_t *srcptr = src;
+
+		expected = dic->rlen;
+#ifdef __ARCH_HAS_LZ4_ACCELERATOR
+		if (lz4_decompress_accel_enable() &&
+		    dic->rlen > LZ4_FAST_MARGIN &&
+		    dic->clen > LZ4_FAST_MARGIN) {
+			accel = true;
+
+			ret = lz4_decompress_asm(&dstptr, dst, dst + dic->rlen - LZ4_FAST_MARGIN,
+						  &srcptr, src + dic->clen - LZ4_FAST_MARGIN,
+						  !!dic->inplace_io[dic->current_blk]);
+			if (ret) {
+				printk_ratelimited("%sF2FS-fs (%s): lz4 decompress accel failed, ret:%d\n",
+					KERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id, ret);
+				return -EIO;
+			}
+
+			ret = __lz4_decompress_safe_partial(dstptr, srcptr, dst,
+						  dic->rlen, src, dic->clen, false);
+		}
+#endif
+		if (!accel)
+			ret = LZ4_decompress_safe_partial(srcptr, dstptr,
+						  dic->clen, dic->rlen, dic->rlen);
+#endif
+	}
+
 	if (ret < 0) {
 		printk_ratelimited("%sF2FS-fs (%s): lz4 decompress failed, ret:%d\n",
 				KERN_ERR, F2FS_I_SB(dic->inode)->sb->s_id, ret);
 		return -EIO;
 	}
 
-	if (ret != PAGE_SIZE << dic->log_cluster_size) {
+	if (ret != expected) {
 		printk_ratelimited("%sF2FS-fs (%s): lz4 invalid ret:%d, "
 					"expected:%lu\n", KERN_ERR,
 					F2FS_I_SB(dic->inode)->sb->s_id, ret,
-					PAGE_SIZE << dic->log_cluster_size);
+					expected);
 		return -EIO;
 	}
 	return 0;
@@ -318,16 +402,28 @@ static bool lz4_is_level_valid(int lvl)
 #endif
 }
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static int lz4_inplace_margin(int decompressed_sz)
+{
+	return (decompressed_sz >> 8) + 65;
+}
+#endif
+
 static const struct f2fs_compress_ops f2fs_lz4_ops = {
 	.init_compress_ctx	= lz4_init_compress_ctx,
 	.destroy_compress_ctx	= lz4_destroy_compress_ctx,
 	.compress_pages		= lz4_compress_pages,
 	.decompress_pages	= lz4_decompress_pages,
 	.is_level_valid		= lz4_is_level_valid,
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	.inplace_margin		= lz4_inplace_margin,
+#endif
 };
 #endif
 
 #ifdef CONFIG_F2FS_FS_ZSTD
+#define F2FS_ZSTD_DEFAULT_CLEVEL	1
+
 static int zstd_init_compress_ctx(struct compress_ctx *cc)
 {
 	zstd_parameters params;
@@ -336,7 +432,6 @@ static int zstd_init_compress_ctx(struct compress_ctx *cc)
 	unsigned int workspace_size;
 	unsigned char level = F2FS_I(cc->inode)->i_compress_level;
 
-	/* Need to remain this for backward compatibility */
 	if (!level)
 		level = F2FS_ZSTD_DEFAULT_CLEVEL;
 
@@ -574,22 +669,79 @@ module_param(num_compress_pages, uint, 0444);
 MODULE_PARM_DESC(num_compress_pages,
 		"Number of intermediate compress pages to preallocate");
 
-int __init f2fs_init_compress_mempool(void)
-{
-	compress_page_pool = mempool_create_page_pool(num_compress_pages, 0);
-	return compress_page_pool ? 0 : -ENOMEM;
-}
+#define MPOOL_NR 1024
+struct page_pool {
+	spinlock_t lock;
+	struct page *pages[MPOOL_NR];
+	int head;
+};
+static struct page_pool *mpool;
 
 void f2fs_destroy_compress_mempool(void)
 {
+	unsigned long flags;
+	int i;
+
 	mempool_destroy(compress_page_pool);
+
+	if (!mpool)
+		return;
+
+	spin_lock_irqsave(&mpool->lock, flags);
+	for (i = 0; i < MPOOL_NR; i++) {
+		if (!mpool->pages[i])
+			break;
+		__free_page(mpool->pages[i]);
+		mpool->pages[i] = NULL;
+	}
+	spin_unlock_irqrestore(&mpool->lock, flags);
+	kfree(mpool);
+	mpool = NULL;
+}
+
+int __init f2fs_init_compress_mempool(void)
+{
+	int i;
+
+	compress_page_pool = mempool_create_page_pool(num_compress_pages, 0);
+	if (!compress_page_pool)
+		return -ENOMEM;
+
+	mpool = kzalloc(sizeof(struct page_pool), GFP_KERNEL);
+	if (!mpool)
+		goto out;
+
+	spin_lock_init(&mpool->lock);
+	mpool->head = 0;
+	for (i = 0; i < MPOOL_NR; i++) {
+		mpool->pages[i] = alloc_page(GFP_KERNEL);
+		if (!mpool->pages[i])
+			goto out;
+	}
+
+	return 0;
+
+out:
+	f2fs_destroy_compress_mempool();
+	return -ENOMEM;
 }
 
 static struct page *f2fs_compress_alloc_page(void)
 {
-	struct page *page;
+	struct page *page = NULL;
+	unsigned long flags;
+
+	spin_lock_irqsave(&mpool->lock, flags);
+	if (mpool->head < MPOOL_NR) {
+		page = mpool->pages[mpool->head];
+		mpool->pages[mpool->head] = NULL;
+		mpool->head++;
+	}
+	spin_unlock_irqrestore(&mpool->lock, flags);
+
+	if (!page)
+		page = mempool_alloc(compress_page_pool, GFP_NOFS);
 
-	page = mempool_alloc(compress_page_pool, GFP_NOFS);
 	lock_page(page);
 
 	return page;
@@ -597,21 +749,36 @@ static struct page *f2fs_compress_alloc_page(void)
 
 static void f2fs_compress_free_page(struct page *page)
 {
+	unsigned long flags;
+
 	if (!page)
 		return;
 	detach_page_private(page);
 	page->mapping = NULL;
 	unlock_page(page);
+
+	spin_lock_irqsave(&mpool->lock, flags);
+	if (mpool->head > 0) {
+		mpool->head--;
+		mpool->pages[mpool->head] = page;
+		spin_unlock_irqrestore(&mpool->lock, flags);
+		return;
+	}
+	spin_unlock_irqrestore(&mpool->lock, flags);
 	mempool_free(page, compress_page_pool);
 }
 
 #define MAX_VMAP_RETRIES	3
 
-static void *f2fs_vmap(struct page **pages, unsigned int count)
+static void *f2fs_vmap(struct f2fs_sb_info *sbi,
+		struct page **pages, unsigned int count)
 {
 	int i;
 	void *buf = NULL;
 
+	if (time_to_inject(sbi, FAULT_COMPRESS_VMAP))
+		return NULL;
+
 	for (i = 0; i < MAX_VMAP_RETRIES; i++) {
 		buf = vm_map_ram(pages, count, -1);
 		if (buf)
@@ -621,7 +788,7 @@ static void *f2fs_vmap(struct page **pages, unsigned int count)
 	return buf;
 }
 
-static int f2fs_compress_pages(struct compress_ctx *cc)
+static int f2fs_fixed_input_compress_pages(struct compress_ctx *cc)
 {
 	struct f2fs_inode_info *fi = F2FS_I(cc->inode);
 	const struct f2fs_compress_ops *cops =
@@ -630,9 +797,6 @@ static int f2fs_compress_pages(struct compress_ctx *cc)
 	u32 chksum = 0;
 	int i, ret;
 
-	trace_f2fs_compress_pages_start(cc->inode, cc->cluster_idx,
-				cc->cluster_size, fi->i_compress_algorithm);
-
 	if (cops->init_compress_ctx) {
 		ret = cops->init_compress_ctx(cc);
 		if (ret)
@@ -657,13 +821,13 @@ static int f2fs_compress_pages(struct compress_ctx *cc)
 		}
 	}
 
-	cc->rbuf = f2fs_vmap(cc->rpages, cc->cluster_size);
+	cc->rbuf = f2fs_vmap(F2FS_I_SB(cc->inode), cc->rpages, cc->cluster_size);
 	if (!cc->rbuf) {
 		ret = -ENOMEM;
 		goto out_free_cpages;
 	}
 
-	cc->cbuf = f2fs_vmap(cc->cpages, cc->nr_cpages);
+	cc->cbuf = f2fs_vmap(F2FS_I_SB(cc->inode), cc->cpages, cc->nr_cpages);
 	if (!cc->cbuf) {
 		ret = -ENOMEM;
 		goto out_vunmap_rbuf;
@@ -729,6 +893,225 @@ static int f2fs_compress_pages(struct compress_ctx *cc)
 	if (cops->destroy_compress_ctx)
 		cops->destroy_compress_ctx(cc);
 out:
+	return ret;
+}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static int f2fs_fixed_output_compress_pages(struct compress_ctx *cc)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(cc->inode);
+	struct f2fs_inode_info *fi = F2FS_I(cc->inode);
+	const struct f2fs_compress_ops *cops =
+				f2fs_cops[fi->i_compress_algorithm];
+	unsigned int max_len, new_nr_cpages;
+	struct decompress_index *di;
+	unsigned int blkidx = 0;
+	unsigned int rbuf_size = cc->nr_rpages << PAGE_SHIFT;
+	int i, ret;
+
+	if (cops->init_compress_ctx) {
+		if (time_to_inject(sbi, FAULT_COMPRESS_INIT_CTX)) {
+			ret = -EIO;
+			goto out;
+		}
+		ret = cops->init_compress_ctx(cc);
+		if (ret)
+			goto out;
+	}
+	di = cc->di;
+
+	max_len = COMPRESS_HEADER_SIZE + cc->clen;
+	cc->nr_cpages = DIV_ROUND_UP(max_len, PAGE_SIZE);
+	cc->valid_nr_cpages = cc->nr_cpages;
+
+	if (time_to_inject(sbi, FAULT_COMPRESS_PAGE_ARRAY)) {
+		cc->cpages = NULL;
+		ret = -ENOMEM;
+		goto destroy_compress_ctx;
+	}
+
+	cc->cpages = page_array_alloc(cc->inode, cc->nr_cpages);
+	if (!cc->cpages) {
+		ret = -ENOMEM;
+		goto destroy_compress_ctx;
+	}
+
+	for (i = 0; i < cc->nr_cpages; i++) {
+		cc->cpages[i] = f2fs_compress_alloc_page();
+		if (!cc->cpages[i]) {
+			ret = -ENOMEM;
+			goto out_free_cpages;
+		}
+	}
+
+	cc->rbuf = f2fs_vmap(sbi, cc->rpages, cc->cluster_size);
+	if (!cc->rbuf) {
+		ret = -ENOMEM;
+		goto out_free_cpages;
+	}
+
+	cc->cbuf = f2fs_vmap(sbi, cc->cpages, cc->nr_cpages);
+	if (!cc->cbuf) {
+		ret = -ENOMEM;
+		goto out_vunmap_rbuf;
+	}
+
+	while (cc->rofs < rbuf_size) {
+		int prev_src_ofs = cc->rofs;
+		int prev_dst_ofs = cc->cofs;
+		int src_len, dst_len;
+		int dist0 = 0; /* distance from first page */
+		int curofs = 0;
+		int is_compress = 1;
+
+		if (prev_dst_ofs >= (cc->nr_cpages << PAGE_SHIFT)) {
+			ret = -EAGAIN;
+			goto out_vunmap_cbuf;
+		}
+
+		if (prev_src_ofs + PAGE_SIZE >= rbuf_size) {
+			/* left raw data is less than 4K, keep it uncompressed */
+			src_len = rbuf_size - prev_src_ofs;
+			dst_len = src_len;
+			memcpy((void *)cc->cbuf + prev_dst_ofs,
+			       cc->rbuf + prev_src_ofs, src_len);
+			cc->cofs = prev_dst_ofs + dst_len;
+			cc->rofs = prev_src_ofs + src_len;
+			is_compress = 0;
+			goto skip_compress;
+		}
+
+		ret = cops->compress_pages(cc);
+		if (ret)
+			goto out_vunmap_cbuf;
+
+		src_len = cc->rofs - prev_src_ofs;
+		dst_len = cc->cofs - prev_dst_ofs;
+
+		if (dst_len < PAGE_SIZE && cops->inplace_margin &&
+		    (PAGE_SIZE - dst_len) >= cops->inplace_margin(src_len)) {
+			void *last_page_start = (void *)cc->cbuf + prev_dst_ofs;
+			void *last_page_end = last_page_start + PAGE_SIZE;
+			/*
+			 * move compressed data to tail of block in advance,
+			 * so that it can be read directly to page cache
+			 * and it can trigger inplace decompression without
+			 * moving data during read
+			 */
+
+			f2fs_bug_on(sbi, prev_dst_ofs & ~PAGE_MASK);
+			memmove(last_page_end - dst_len, last_page_start, dst_len);
+			memset(last_page_start, 0, PAGE_SIZE - dst_len);
+		} else if (dst_len >= src_len) {
+			/* bad compression ratio, keep it uncompresed */
+			memcpy((void *)cc->cbuf + prev_dst_ofs,
+			       cc->rbuf + prev_src_ofs, PAGE_SIZE);
+			cc->cofs = prev_dst_ofs + PAGE_SIZE;
+			cc->rofs = prev_src_ofs + PAGE_SIZE;
+			src_len = PAGE_SIZE;
+			dst_len = PAGE_SIZE;
+			is_compress = 0;
+		}
+
+skip_compress:
+		/* update decompress index */
+		do {
+			f2fs_bug_on(sbi, (di - cc->di) >= cc->cluster_size);
+			di->is_valid = 1;
+			di->is_compress = is_compress;
+			di->blkidx = blkidx;
+
+			if (!dist0) { // first raw page
+				di->first_page = 1;
+				if (prev_src_ofs & ~PAGE_MASK)
+					di->cross_block = 1;
+				else
+					di->cross_block = 0;
+				di->ofs = prev_src_ofs & ~PAGE_MASK;
+
+				curofs = PAGE_SIZE - (prev_src_ofs & ~PAGE_MASK);
+			} else { // the following raw pages
+				di->first_page = 0;
+				di->cross_block = 0;
+				di->ofs = dist0;
+
+				curofs += PAGE_SIZE;
+			}
+
+			di++;
+			dist0++;
+		} while (curofs + PAGE_SIZE <= src_len);
+
+		blkidx++;
+	}
+	cc->clen = cc->cofs;
+
+	max_len = PAGE_SIZE * (cc->cluster_size - 1);
+
+	if (time_to_inject(sbi, FAULT_COMPRESS_LOW_RATIO)) {
+		ret = -EAGAIN;
+		goto out_vunmap_cbuf;
+	}
+
+	if (cc->clen > max_len) {
+		ret = -EAGAIN;
+		goto out_vunmap_cbuf;
+	}
+
+	new_nr_cpages = DIV_ROUND_UP(cc->clen, PAGE_SIZE);
+
+	vm_unmap_ram(cc->cbuf, cc->nr_cpages);
+	vm_unmap_ram(cc->rbuf, cc->cluster_size);
+
+	for (i = 0; i < cc->nr_cpages; i++) {
+		if (i < new_nr_cpages)
+			continue;
+		f2fs_compress_free_page(cc->cpages[i]);
+		cc->cpages[i] = NULL;
+	}
+
+	if (cops->destroy_compress_ctx)
+		cops->destroy_compress_ctx(cc);
+
+	cc->valid_nr_cpages = new_nr_cpages;
+
+	f2fs_bug_on(sbi, cc->nr_cpages >= cc->cluster_size);
+	return 0;
+
+out_vunmap_cbuf:
+	vm_unmap_ram(cc->cbuf, cc->nr_cpages);
+out_vunmap_rbuf:
+	vm_unmap_ram(cc->rbuf, cc->cluster_size);
+out_free_cpages:
+	for (i = 0; i < cc->nr_cpages; i++) {
+		if (cc->cpages[i])
+			f2fs_compress_free_page(cc->cpages[i]);
+	}
+	page_array_free(cc->inode, cc->cpages, cc->nr_cpages);
+	cc->cpages = NULL;
+destroy_compress_ctx:
+	if (cops->destroy_compress_ctx)
+		cops->destroy_compress_ctx(cc);
+out:
+	return ret;
+}
+#endif
+
+static int f2fs_compress_pages(struct compress_ctx *cc)
+{
+	struct f2fs_inode_info *fi = F2FS_I(cc->inode);
+	int ret = 0;
+
+	trace_f2fs_compress_pages_start(cc->inode, cc->cluster_idx,
+				cc->cluster_size, fi->i_compress_algorithm);
+
+	if (f2fs_compress_layout(cc->inode) == COMPRESS_FIXED_INPUT)
+		ret = f2fs_fixed_input_compress_pages(cc);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	else
+		ret = f2fs_fixed_output_compress_pages(cc);
+#endif
+
 	trace_f2fs_compress_pages_end(cc->inode, cc->cluster_idx,
 							cc->clen, ret);
 	return ret;
@@ -739,7 +1122,8 @@ static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 static void f2fs_release_decomp_mem(struct decompress_io_ctx *dic,
 		bool bypass_destroy_callback, bool pre_alloc);
 
-void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
+static int f2fs_fixed_input_decompress_cluster(struct decompress_io_ctx *dic,
+						bool in_task)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);
 	struct f2fs_inode_info *fi = F2FS_I(dic->inode);
@@ -748,12 +1132,9 @@ void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
 	bool bypass_callback = false;
 	int ret;
 
-	trace_f2fs_decompress_pages_start(dic->inode, dic->cluster_idx,
-				dic->cluster_size, fi->i_compress_algorithm);
-
 	if (dic->failed) {
 		ret = -EIO;
-		goto out_end_io;
+		goto out;
 	}
 
 	ret = f2fs_prepare_decomp_mem(dic, false);
@@ -797,7 +1178,211 @@ void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
 out_release:
 	f2fs_release_decomp_mem(dic, bypass_callback, false);
 
-out_end_io:
+out:
+	return ret;
+};
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+struct fixed_output_pcpubuf {
+	u8 data[PAGE_SIZE];
+};
+
+static struct fixed_output_pcpubuf __percpu *f2fs_pcpubuf;
+
+static int __init f2fs_init_pcpubuf(void)
+{
+	f2fs_pcpubuf = alloc_percpu(struct fixed_output_pcpubuf);
+	if (!f2fs_pcpubuf)
+		return -ENOMEM;
+	return 0;
+}
+
+static void f2fs_destroy_pcpubuf(void)
+{
+	free_percpu(f2fs_pcpubuf);
+}
+
+/* must not sleep between copy_inplace_data and put_inplace_data */
+static void *copy_inplace_data(void *src)
+{
+	void *buf = get_cpu_ptr(f2fs_pcpubuf);
+
+	memcpy(buf, src, PAGE_SIZE);
+	return buf;
+}
+
+static void put_inplace_data(void)
+{
+	put_cpu_ptr(f2fs_pcpubuf);
+}
+
+static int f2fs_fixed_output_decompress_cluster(struct decompress_io_ctx *dic,
+						bool in_task)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);
+	struct f2fs_inode_info *fi = F2FS_I(dic->inode);
+	const struct f2fs_compress_ops *cops =
+			f2fs_cops[fi->i_compress_algorithm];
+	unsigned int blkidx;
+	int i;
+	int ret = -ENOSYS;
+
+	if (dic->failed)
+		return -EIO;
+
+	for (blkidx = 0; blkidx < dic->nr_cpages; blkidx++) {
+		int end_ofs = 0;
+		bool copied = false;
+		unsigned int lstart = -1, lend = 0, count = 0, last;
+		void *unmap_addr = NULL;
+
+		if (!dic->cpages[blkidx])
+			continue;
+
+		/* get the start di and di count in current cpage
+		 * get the last di which will be decompressed in rpage
+		 */
+		for (i = 0; i < dic->cluster_size; i++) {
+			if (dic->di[i].is_valid == 0)
+				/* the left di should be all not valid */
+				break;
+			if (dic->di[i].blkidx != blkidx)
+				continue;
+			if (lstart == -1)
+				lstart = i;
+			count++;
+			if (dic->rpages[i])
+				lend = lstart + count;
+		}
+
+		/*fix coverity error: Out-of-bounds read array dic->di*/
+		if(lstart >= dic->cluster_size) {
+			f2fs_bug_on(sbi, 1);
+			return -EIO;
+		}
+
+		/* should consider last cluster is not full, di[lstart + count]
+		 * could be invalid
+		 */
+		f2fs_bug_on(sbi, lstart + count < dic->cluster_size &&
+				dic->di[lstart + count].first_page != 1);
+
+		last = lstart + count;
+		/* if next is cross_page, we need one more rpage to save decompressed data at the tail */
+		if (last < dic->cluster_size && dic->di[last].cross_block) {
+			end_ofs = PAGE_SIZE - dic->di[last].ofs;
+			count++;
+			if (dic->rpages[last])
+				lend = lstart + count;
+		}
+
+		/* only partial rpages will be decompressed, so we don't have to
+		 * care about cross_block rpage*/
+		if (lend - lstart < count) {
+			count = lend - lstart;
+			end_ofs = 0;
+		}
+
+		/* padding hole in rpages */
+		for (i = 0; i < count; i++) {
+			if (dic->rpages[lstart + i]) {
+				dic->tpages[lstart + i] = dic->rpages[lstart + i];
+				continue;
+			}
+
+			dic->tpages[lstart + i] = f2fs_compress_alloc_page();
+			if (!dic->tpages[lstart + i])
+				return -ENOMEM;
+		}
+
+		if (cops->init_decompress_ctx) {
+			ret = cops->init_decompress_ctx(dic);
+			if (ret)
+				return ret;
+		}
+
+		dic->rbuf = f2fs_vmap(sbi, dic->tpages + lstart, count);
+		if (!dic->rbuf) {
+			ret = -ENOMEM;
+			goto out_destroy;
+		}
+		dic->rofs = 0;
+
+		if (dic->di[lstart].cross_block)
+			dic->rofs = dic->di[lstart].ofs;
+
+		dic->rlen = (count << PAGE_SHIFT) - dic->rofs - end_ofs;
+
+		dic->cbuf = kmap_atomic(dic->cpages[blkidx]);
+		dic->cofs = 0;
+
+		/* calculate margin length */
+		if (dic->di[lstart].is_compress) {
+			for (i = 0; i < PAGE_SIZE; i++) {
+				if (((u8 *)dic->cbuf)[i])
+					break;
+				dic->cofs++;
+			}
+			dic->clen = PAGE_SIZE - dic->cofs;
+		} else {
+			dic->clen = PAGE_SIZE;
+		}
+
+		/*
+		 * inplace decompression can only be triggered in first-page
+		 * which crosses two compressed blocks, and there is enough
+		 * free space of first-page to store compressed data to avoid
+		 * overlapping durint compression.
+		 */
+		if (dic->inplace_io[blkidx] &&
+		    end_ofs < cops->inplace_margin(dic->rlen)) {
+			unmap_addr = dic->cbuf;
+			dic->cbuf = copy_inplace_data(unmap_addr);
+			kunmap_atomic(unmap_addr);
+			copied = true;
+		}
+
+		if (!dic->di[lstart].is_compress) {
+			memcpy(dic->rbuf + dic->rofs, dic->cbuf + dic->cofs, dic->rlen);
+			ret = 0;
+		} else {
+			dic->current_blk = blkidx;
+			ret = cops->decompress_pages(dic);
+		}
+
+		if (copied)
+			put_inplace_data();
+		else
+			kunmap_atomic(unmap_addr);
+		vm_unmap_ram(dic->rbuf, count);
+
+out_destroy:
+		if (cops->destroy_decompress_ctx)
+			cops->destroy_decompress_ctx(dic);
+
+		if (ret)
+			break;
+	}
+
+	return ret;
+}
+#endif
+
+void f2fs_decompress_cluster(struct decompress_io_ctx *dic, bool in_task)
+{
+	struct f2fs_inode_info *fi = F2FS_I(dic->inode);
+	int ret = -ENOSYS;
+
+	trace_f2fs_decompress_pages_start(dic->inode, dic->cluster_idx,
+				dic->cluster_size, fi->i_compress_algorithm);
+
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT)
+		ret = f2fs_fixed_input_decompress_cluster(dic, in_task);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	else
+		ret = f2fs_fixed_output_decompress_cluster(dic, in_task);
+#endif
+
 	trace_f2fs_decompress_pages_end(dic->inode, dic->cluster_idx,
 							dic->clen, ret);
 	f2fs_decompress_end_io(dic, ret, in_task);
@@ -1381,7 +1966,7 @@ static int f2fs_write_compressed_pages(struct compress_ctx *cc,
 			if (blkaddr == COMPRESS_ADDR)
 				fio.compr_blocks++;
 			if (__is_valid_data_blkaddr(blkaddr))
-				f2fs_invalidate_blocks(sbi, blkaddr, 1);
+				f2fs_invalidate_blocks(sbi, blkaddr);
 			f2fs_update_data_blkaddr(&dn, COMPRESS_ADDR);
 			goto unlock_continue;
 		}
@@ -1391,7 +1976,7 @@ static int f2fs_write_compressed_pages(struct compress_ctx *cc,
 
 		if (i > cc->valid_nr_cpages) {
 			if (__is_valid_data_blkaddr(blkaddr)) {
-				f2fs_invalidate_blocks(sbi, blkaddr, 1);
+				f2fs_invalidate_blocks(sbi, blkaddr);
 				f2fs_update_data_blkaddr(&dn, NEW_ADDR);
 			}
 			goto unlock_continue;
@@ -1417,6 +2002,19 @@ static int f2fs_write_compressed_pages(struct compress_ctx *cc,
 		}
 		(*submitted)++;
 unlock_continue:
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT) {
+			__le32 *addr = decompress_index_addr(inode, dn.node_page);
+			if (cc->di[i].is_valid) {
+				decompress_index_t val = serialize_decompress_index(&cc->di[i]);
+				addr[dn.ofs_in_node] = cpu_to_le32(val);
+			} else {
+				addr[dn.ofs_in_node] = cpu_to_le32(0);
+			}
+			set_page_dirty(dn.node_page);
+		}
+#endif
+
 		inode_dec_dirty_pages(cc->inode);
 		unlock_page(fio.page);
 	}
@@ -1477,8 +2075,6 @@ void f2fs_compress_write_end_io(struct bio *bio, struct page *page)
 	struct f2fs_sb_info *sbi = bio->bi_private;
 	struct compress_io_ctx *cic =
 			(struct compress_io_ctx *)page_private(page);
-	enum count_type type = WB_DATA_TYPE(page,
-				f2fs_is_compressed_page(page));
 	int i;
 
 	if (unlikely(bio->bi_status))
@@ -1486,7 +2082,7 @@ void f2fs_compress_write_end_io(struct bio *bio, struct page *page)
 
 	f2fs_compress_free_page(page);
 
-	dec_page_count(sbi, type);
+	dec_page_count(sbi, F2FS_WB_DATA);
 
 	if (atomic_dec_return(&cic->pending_pages))
 		return;
@@ -1502,14 +2098,12 @@ void f2fs_compress_write_end_io(struct bio *bio, struct page *page)
 }
 
 static int f2fs_write_raw_pages(struct compress_ctx *cc,
-					int *submitted_p,
+					int *submitted,
 					struct writeback_control *wbc,
 					enum iostat_type io_type)
 {
 	struct address_space *mapping = cc->inode->i_mapping;
-	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
-	int submitted, compr_blocks, i;
-	int ret = 0;
+	int _submitted, compr_blocks, ret, i;
 
 	compr_blocks = f2fs_compressed_blocks(cc);
 
@@ -1524,10 +2118,6 @@ static int f2fs_write_raw_pages(struct compress_ctx *cc,
 	if (compr_blocks < 0)
 		return compr_blocks;
 
-	/* overwrite compressed cluster w/ normal cluster */
-	if (compr_blocks > 0)
-		f2fs_lock_op(sbi);
-
 	for (i = 0; i < cc->cluster_size; i++) {
 		if (!cc->rpages[i])
 			continue;
@@ -1552,7 +2142,7 @@ static int f2fs_write_raw_pages(struct compress_ctx *cc,
 		if (!clear_page_dirty_for_io(cc->rpages[i]))
 			goto continue_unlock;
 
-		ret = f2fs_write_single_data_page(cc->rpages[i], &submitted,
+		ret = f2fs_write_single_data_page(cc->rpages[i], &_submitted,
 						NULL, NULL, wbc, io_type,
 						compr_blocks, false);
 		if (ret) {
@@ -1560,29 +2150,26 @@ static int f2fs_write_raw_pages(struct compress_ctx *cc,
 				unlock_page(cc->rpages[i]);
 				ret = 0;
 			} else if (ret == -EAGAIN) {
-				ret = 0;
 				/*
 				 * for quota file, just redirty left pages to
 				 * avoid deadlock caused by cluster update race
 				 * from foreground operation.
 				 */
 				if (IS_NOQUOTA(cc->inode))
-					goto out;
+					return 0;
+				ret = 0;
 				f2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);
 				goto retry_write;
 			}
-			goto out;
+			return ret;
 		}
 
-		*submitted_p += submitted;
+		*submitted += _submitted;
 	}
 
-out:
-	if (compr_blocks > 0)
-		f2fs_unlock_op(sbi);
+	f2fs_balance_fs(F2FS_M_SB(mapping), true);
 
-	f2fs_balance_fs(sbi, true);
-	return ret;
+	return 0;
 }
 
 int f2fs_write_multi_pages(struct compress_ctx *cc,
@@ -1628,6 +2215,7 @@ static inline bool allow_memalloc_for_decomp(struct f2fs_sb_info *sbi,
 static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 		bool pre_alloc)
 {
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dic->inode);
 	const struct f2fs_compress_ops *cops =
 		f2fs_cops[F2FS_I(dic->inode)->i_compress_algorithm];
 	int i;
@@ -1635,10 +2223,6 @@ static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 	if (!allow_memalloc_for_decomp(F2FS_I_SB(dic->inode), pre_alloc))
 		return 0;
 
-	dic->tpages = page_array_alloc(dic->inode, dic->cluster_size);
-	if (!dic->tpages)
-		return -ENOMEM;
-
 	for (i = 0; i < dic->cluster_size; i++) {
 		if (dic->rpages[i]) {
 			dic->tpages[i] = dic->rpages[i];
@@ -1650,11 +2234,11 @@ static int f2fs_prepare_decomp_mem(struct decompress_io_ctx *dic,
 			return -ENOMEM;
 	}
 
-	dic->rbuf = f2fs_vmap(dic->tpages, dic->cluster_size);
+	dic->rbuf = f2fs_vmap(sbi, dic->tpages, dic->cluster_size);
 	if (!dic->rbuf)
 		return -ENOMEM;
 
-	dic->cbuf = f2fs_vmap(dic->cpages, dic->nr_cpages);
+	dic->cbuf = f2fs_vmap(sbi, dic->cpages, dic->nr_cpages);
 	if (!dic->cbuf)
 		return -ENOMEM;
 
@@ -1689,6 +2273,7 @@ static void f2fs_free_dic(struct decompress_io_ctx *dic,
 struct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)
 {
 	struct decompress_io_ctx *dic;
+	struct page *page;
 	pgoff_t start_idx = start_idx_of_cluster(cc);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(cc->inode);
 	int i, ret;
@@ -1697,15 +2282,8 @@ struct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)
 	if (!dic)
 		return ERR_PTR(-ENOMEM);
 
-	dic->rpages = page_array_alloc(cc->inode, cc->cluster_size);
-	if (!dic->rpages) {
-		kmem_cache_free(dic_entry_slab, dic);
-		return ERR_PTR(-ENOMEM);
-	}
-
 	dic->magic = F2FS_COMPRESSED_PAGE_MAGIC;
 	dic->inode = cc->inode;
-	atomic_set(&dic->remaining_pages, cc->nr_cpages);
 	dic->cluster_idx = cc->cluster_idx;
 	dic->cluster_size = cc->cluster_size;
 	dic->log_cluster_size = cc->log_cluster_size;
@@ -1718,29 +2296,152 @@ struct decompress_io_ctx *f2fs_alloc_dic(struct compress_ctx *cc)
 		dic->rpages[i] = cc->rpages[i];
 	dic->nr_rpages = cc->cluster_size;
 
-	dic->cpages = page_array_alloc(dic->inode, dic->nr_cpages);
-	if (!dic->cpages) {
-		ret = -ENOMEM;
-		goto out_free;
-	}
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT) {
+		for (i = 0; i < dic->nr_cpages; i++) {
+			page = f2fs_compress_alloc_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out_free;
+			}
 
-	for (i = 0; i < dic->nr_cpages; i++) {
-		struct page *page;
+			f2fs_set_compressed_page(page, cc->inode,
+						start_idx + i + 1, dic);
+			dic->cpages[i] = page;
+		}
+		atomic_set(&dic->remaining_pages, cc->nr_cpages);
 
-		page = f2fs_compress_alloc_page();
-		if (!page) {
-			ret = -ENOMEM;
+		ret = f2fs_prepare_decomp_mem(dic, true);
+		if (ret)
 			goto out_free;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	} else {
+		int prev_blkidx = -1, blkidx;
+		int cnt = 0;
+
+		for (i = 0; i < dic->cluster_size; i++)
+			dic->di[i] = cc->di[i];
+
+		/*
+		 * One cpage could be decomrpessed into several continous
+		 * rpages[n]. If rpages[n-1] is cross_block, rpages[0] ~
+		 * rpage[n-2] have the same blkidx and are not cross_block,
+		 * and rpages[n-1] has blkidx+1 and is cross_block.
+		 *
+		 * In this case, cpage could reuse rpage[n-1] to do inplace
+		 * io, and could try to do inplace decompression later.
+		 *
+		 * If rpages[n-1] is not cross_block, inplace-io cannot be
+		 * used.
+		 */
+		for (i = 0; i < dic->cluster_size; i++) {
+			/* only care about pages will be read */
+			if (!cc->rpages[i]) {
+				/*
+				 * if there are NULL in rpages[], need alloc
+				 * previous cpage and reset all states, e.g
+				 *
+				 *           NULL          NULL
+				 * rpages: |------|-+----|------|--+---|
+				 *                  /              /
+				 *                 /       .------'
+				 *                |       /
+				 * cpages: |------|------|------|
+				 *
+				 * cpages[1] is allocated here
+				 */
+				if (prev_blkidx != -1 && !dic->cpages[prev_blkidx]) {
+					page = f2fs_compress_alloc_page();
+					if (!page) {
+						ret = -ENOMEM;
+						goto out_free;
+					}
+
+					f2fs_set_compressed_page(page, cc->inode,
+								 start_idx + prev_blkidx + 1, dic);
+					dic->cpages[prev_blkidx] = page;
+					prev_blkidx = -1;
+					cnt++;
+				}
+				continue;
+			}
+
+			blkidx = dic->di[i].blkidx;
+			if (prev_blkidx == -1) {
+				prev_blkidx = blkidx;
+				/*
+				 * if we start from not the first rpage
+				 * (first rpage must be not cross_page) in
+				 * the cluster, and the current rpage is
+				 * cross-block, we have to read the previous
+				 * cpage
+				 */
+				if (dic->di[i].cross_block) {
+					prev_blkidx--;
+					f2fs_bug_on(sbi, i == 0);
+				}
+			}
+
+			/*
+			 * it is the first page, continue to check if it can
+			 * be inplace_io
+			 */
+			if (prev_blkidx == blkidx)
+				continue;
+			/*
+			 * if cpage already allocated, skip it, e.g
+			 *
+			 *           NULL          NULL
+			 * rpages: |------|-+----|------|--+---|
+			 *                  /              /
+			 *                 /       .------'
+			 *                |       /
+			 * cpages: |------|------|------|
+			 *
+			 * cpages[1] is allocated by above alloc
+			 */
+			if (dic->cpages[prev_blkidx]) {
+				prev_blkidx = blkidx;
+				continue;
+			}
+
+			/* If di[i] is cross_blocks, di[i-1] must exist */
+			if (dic->di[i].cross_block && dic->di[i-1].is_compress) {
+				page = dic->rpages[i];
+				f2fs_bug_on(sbi, PagePrivate(page));
+				dic->inplace_io[prev_blkidx] = 1;
+				f2fs_set_compressed_page(page, cc->inode,
+						start_idx + prev_blkidx + 1, dic);
+			} else {
+				page = f2fs_compress_alloc_page();
+				if (!page) {
+					ret = -ENOMEM;
+					goto out_free;
+				}
+
+				f2fs_set_compressed_page(page, cc->inode,
+						 start_idx + prev_blkidx + 1, dic);
+			}
+			dic->cpages[prev_blkidx] = page;
+			prev_blkidx = blkidx;
+			cnt++;
 		}
 
-		f2fs_set_compressed_page(page, cc->inode,
-					start_idx + i + 1, dic);
-		dic->cpages[i] = page;
-	}
+		if (prev_blkidx != -1 && !dic->cpages[prev_blkidx]) {
+			page = f2fs_compress_alloc_page();
+			if (!page) {
+				ret = -ENOMEM;
+				goto out_free;
+			}
 
-	ret = f2fs_prepare_decomp_mem(dic, true);
-	if (ret)
-		goto out_free;
+			f2fs_set_compressed_page(page, cc->inode,
+						 start_idx + prev_blkidx + 1, dic);
+			dic->cpages[prev_blkidx] = page;
+			cnt++;
+		}
+
+		atomic_set(&dic->remaining_pages, cnt);
+#endif
+	}
 
 	return dic;
 
@@ -1754,29 +2455,36 @@ static void f2fs_free_dic(struct decompress_io_ctx *dic,
 {
 	int i;
 
-	f2fs_release_decomp_mem(dic, bypass_destroy_callback, true);
+	if (f2fs_compress_layout(dic->inode) == COMPRESS_FIXED_INPUT)
+		f2fs_release_decomp_mem(dic, bypass_destroy_callback, true);
 
-	if (dic->tpages) {
-		for (i = 0; i < dic->cluster_size; i++) {
-			if (dic->rpages[i])
-				continue;
-			if (!dic->tpages[i])
-				continue;
-			f2fs_compress_free_page(dic->tpages[i]);
-		}
-		page_array_free(dic->inode, dic->tpages, dic->cluster_size);
+	for (i = 0; i < dic->cluster_size; i++) {
+		if (dic->rpages[i])
+			continue;
+		if (!dic->tpages[i])
+			continue;
+		f2fs_compress_free_page(dic->tpages[i]);
 	}
 
-	if (dic->cpages) {
-		for (i = 0; i < dic->nr_cpages; i++) {
-			if (!dic->cpages[i])
-				continue;
-			f2fs_compress_free_page(dic->cpages[i]);
+	for (i = 0; i < dic->nr_cpages; i++) {
+		struct page *cpage;
+
+		if (!dic->cpages[i])
+			continue;
+
+		cpage = dic->cpages[i];
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (dic->inplace_io[i]) {
+			detach_page_private(cpage);
+			unlock_page(cpage);
+		} else {
+			f2fs_compress_free_page(cpage);
 		}
-		page_array_free(dic->inode, dic->cpages, dic->nr_cpages);
+#else
+		f2fs_compress_free_page(cpage);
+#endif
 	}
 
-	page_array_free(dic->inode, dic->rpages, dic->nr_rpages);
 	kmem_cache_free(dic_entry_slab, dic);
 }
 
@@ -1805,6 +2513,10 @@ static void f2fs_verify_cluster(struct work_struct *work)
 {
 	struct decompress_io_ctx *dic =
 		container_of(work, struct decompress_io_ctx, verity_work);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	pgoff_t start_idx = dic->cluster_idx << dic->log_cluster_size;
+	bool skip_unlock;
+#endif
 	int i;
 
 	/* Verify, update, and unlock the decompressed pages. */
@@ -1814,10 +2526,29 @@ static void f2fs_verify_cluster(struct work_struct *work)
 		if (!rpage)
 			continue;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		/*
+		 * if cpage is inplace io, it reuses rpage. In order to avoid
+		 * double unlock_page, keep the rpage locked here, and unlock
+		 * it when cpage is released later in f2fs_free_dic.
+		 */
+		skip_unlock = false;
+		if (f2fs_is_compressed_page(rpage)) {
+			f2fs_restore_compressed_page(rpage, start_idx + i);
+			skip_unlock = true;
+		}
+#endif
+
 		if (fsverity_verify_page(rpage))
 			SetPageUptodate(rpage);
 		else
 			ClearPageUptodate(rpage);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (skip_unlock)
+			continue;
+#endif
+
 		unlock_page(rpage);
 	}
 
@@ -1831,6 +2562,10 @@ static void f2fs_verify_cluster(struct work_struct *work)
 void f2fs_decompress_end_io(struct decompress_io_ctx *dic, bool failed,
 				bool in_task)
 {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	pgoff_t start_idx = dic->cluster_idx << dic->log_cluster_size;
+	bool skip_unlock;
+#endif
 	int i;
 
 	if (!failed && dic->need_verity) {
@@ -1852,10 +2587,29 @@ void f2fs_decompress_end_io(struct decompress_io_ctx *dic, bool failed,
 		if (!rpage)
 			continue;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		/*
+		 * if cpage is inplace io, it reuses rpage. In order to avoid
+		 * double unlock_page, keep the rpage locked here, and unlock
+		 * it when cpage is released later in f2fs_free_dic.
+		 */
+		skip_unlock = false;
+		if (f2fs_is_compressed_page(rpage)) {
+			f2fs_restore_compressed_page(rpage, start_idx + i);
+			skip_unlock = true;
+		}
+#endif
+
 		if (failed)
 			ClearPageUptodate(rpage);
 		else
 			SetPageUptodate(rpage);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (skip_unlock)
+			continue;
+#endif
+
 		unlock_page(rpage);
 	}
 
@@ -1916,12 +2670,11 @@ struct address_space *COMPRESS_MAPPING(struct f2fs_sb_info *sbi)
 	return sbi->compress_inode->i_mapping;
 }
 
-void f2fs_invalidate_compress_pages_range(struct f2fs_sb_info *sbi,
-				block_t blkaddr, unsigned int len)
+void f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi, block_t blkaddr)
 {
 	if (!sbi->compress_inode)
 		return;
-	invalidate_mapping_pages(COMPRESS_MAPPING(sbi), blkaddr, blkaddr + len - 1);
+	invalidate_mapping_pages(COMPRESS_MAPPING(sbi), blkaddr, blkaddr);
 }
 
 void f2fs_cache_compressed_page(struct f2fs_sb_info *sbi, struct page *page,
@@ -2085,10 +2838,17 @@ void f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi)
 
 int __init f2fs_init_compress_cache(void)
 {
+	int err;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	err = f2fs_init_pcpubuf();
+	if (err)
+		goto out;
+#endif
 	cic_entry_slab = f2fs_kmem_cache_create("f2fs_cic_entry",
 					sizeof(struct compress_io_ctx));
 	if (!cic_entry_slab)
-		return -ENOMEM;
+		goto free_pcpubuf;
 	dic_entry_slab = f2fs_kmem_cache_create("f2fs_dic_entry",
 					sizeof(struct decompress_io_ctx));
 	if (!dic_entry_slab)
@@ -2096,6 +2856,11 @@ int __init f2fs_init_compress_cache(void)
 	return 0;
 free_cic:
 	kmem_cache_destroy(cic_entry_slab);
+free_pcpubuf:
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	f2fs_destroy_pcpubuf();
+#endif
+out:
 	return -ENOMEM;
 }
 
@@ -2103,4 +2868,7 @@ void f2fs_destroy_compress_cache(void)
 {
 	kmem_cache_destroy(dic_entry_slab);
 	kmem_cache_destroy(cic_entry_slab);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	f2fs_destroy_pcpubuf();
+#endif
 }
diff --git a/fs/f2fs/data.c b/fs/f2fs/data.c
old mode 100644
new mode 100755
index 2ebcb4f1b..158ac9e37
--- a/fs/f2fs/data.c
+++ b/fs/f2fs/data.c
@@ -49,7 +49,7 @@ void f2fs_destroy_bioset(void)
 	bioset_exit(&f2fs_bioset);
 }
 
-bool f2fs_is_cp_guaranteed(struct page *page)
+static bool __is_cp_guaranteed(struct page *page)
 {
 	struct address_space *mapping = page->mapping;
 	struct inode *inode;
@@ -66,6 +66,8 @@ bool f2fs_is_cp_guaranteed(struct page *page)
 			S_ISDIR(inode->i_mode))
 		return true;
 
+	if (f2fs_is_compressed_page(page))
+		return false;
 	if ((S_ISREG(inode->i_mode) && IS_NOQUOTA(inode)) ||
 			page_private_gcing(page))
 		return true;
@@ -98,14 +100,21 @@ enum bio_post_read_step {
 #endif
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 	STEP_DECOMPRESS	= BIT(1),
+	STEP_DECOMPRESS_FIXED_OUTPUT = BIT(3),
 #else
 	STEP_DECOMPRESS	= 0,	/* compile out the decompression-related code */
+	STEP_DECOMPRESS_FIXED_OUTPUT = 0,
 #endif
 #ifdef CONFIG_FS_VERITY
 	STEP_VERITY	= BIT(2),
 #else
 	STEP_VERITY	= 0,	/* compile out the verity-related code */
 #endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	STEP_READ_ACCOUNT = BIT(4),
+#else
+	STEP_READ_ACCOUNT = 0,
+#endif
 };
 
 struct bio_post_read_ctx {
@@ -120,6 +129,9 @@ struct bio_post_read_ctx {
 	 */
 	bool decompression_attempted;
 	block_t fs_blkaddr;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inode;
+#endif
 };
 
 /*
@@ -142,6 +154,15 @@ static void f2fs_finish_read_bio(struct bio *bio, bool in_task)
 	struct bvec_iter_all iter_all;
 	struct bio_post_read_ctx *ctx = bio->bi_private;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (ctx && (ctx->enabled_steps & STEP_READ_ACCOUNT))
+		inode_dec_read_io(ctx->inode);
+#endif
+
+	/*
+	 * Update and unlock the bio's pagecache pages, and put the
+	 * decompression context for any compressed pages.
+	 */
 	bio_for_each_segment_all(bv, bio, iter_all) {
 		struct page *page = bv->bv_page;
 
@@ -171,7 +192,8 @@ static void f2fs_verify_bio(struct work_struct *work)
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
 	struct bio *bio = ctx->bio;
-	bool may_have_compressed_pages = (ctx->enabled_steps & STEP_DECOMPRESS);
+	bool may_have_compressed_pages = (ctx->enabled_steps &
+			(STEP_DECOMPRESS | STEP_DECOMPRESS_FIXED_OUTPUT));
 
 	/*
 	 * fsverity_verify_bio() may call readahead() again, and while verity
@@ -268,18 +290,88 @@ static void f2fs_handle_step_decompress(struct bio_post_read_ctx *ctx,
 		ctx->enabled_steps &= ~STEP_VERITY;
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+/*
+ * This function copy from fscrypt_decrypt_pagecache_blocks.
+ *
+ * The inode is a FI_DEDUPED inode. It should be consistent with
+ * the issuer to avoid geting incorrect crypt info during dedup or revoke.
+ */
+static int f2fs_fscrypt_decrypt_pagecache_blocks(struct page *page, unsigned int len,
+				     unsigned int offs, struct inode *inode)
+{
+	const unsigned int blockbits = inode->i_blkbits;
+	const unsigned int blocksize = 1 << blockbits;
+	u64 lblk_num = ((u64)page->index << (PAGE_SHIFT - blockbits)) +
+		       (offs >> blockbits);
+	unsigned int i;
+	int err = 0;
+
+	if (WARN_ON_ONCE(!PageLocked(page))) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	if (WARN_ON_ONCE(len <= 0 || !IS_ALIGNED(len | offs, blocksize))) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	for (i = offs; i < offs + len; lblk_num++) {
+		err = fscrypt_encrypt_block_inplace(inode, page,
+				blocksize, i, lblk_num, GFP_NOFS);
+		i += blocksize;
+		if (err)
+			goto out;
+	}
+out:
+	return err;
+}
+
+/*
+ * copy from fscrypt_decrypt_bio, but may use inner inode
+ */
+static bool f2fs_fscrypt_decrypt_bio(struct bio *bio, struct inode *inode)
+{
+	struct bio_vec *bv;
+	struct bvec_iter_all iter_all;
+
+	bio_for_each_segment_all(bv, bio, iter_all) {
+		struct page *page = bv->bv_page;
+		int ret = f2fs_fscrypt_decrypt_pagecache_blocks(page, bv->bv_len,
+							   bv->bv_offset, inode);
+		if (ret) {
+			SetPageError(page);
+			return false;
+		}
+	}
+	return true;
+}
+#endif
+
 static void f2fs_post_read_work(struct work_struct *work)
 {
 	struct bio_post_read_ctx *ctx =
 		container_of(work, struct bio_post_read_ctx, work);
 	struct bio *bio = ctx->bio;
+	bool ret = true;
 
-	if ((ctx->enabled_steps & STEP_DECRYPT) && !fscrypt_decrypt_bio(bio)) {
-		f2fs_finish_read_bio(bio, true);
-		return;
+	if (ctx->enabled_steps & STEP_DECRYPT) {
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if ((ctx->enabled_steps & STEP_READ_ACCOUNT) && ctx->inode)
+			ret = f2fs_fscrypt_decrypt_bio(ctx->bio, ctx->inode);
+		else
+			ret = fscrypt_decrypt_bio(ctx->bio);
+#else
+		ret = fscrypt_decrypt_bio(ctx->bio);
+#endif
+		if(!ret) {
+			f2fs_finish_read_bio(bio, true);
+			return;
+		}
 	}
 
-	if (ctx->enabled_steps & STEP_DECOMPRESS)
+	if (ctx->enabled_steps & (STEP_DECOMPRESS | STEP_DECOMPRESS_FIXED_OUTPUT))
 		f2fs_handle_step_decompress(ctx, true);
 
 	f2fs_verify_and_finish_bio(bio, true);
@@ -304,11 +396,14 @@ static void f2fs_read_end_io(struct bio *bio)
 
 	if (ctx) {
 		unsigned int enabled_steps = ctx->enabled_steps &
-					(STEP_DECRYPT | STEP_DECOMPRESS);
+					(STEP_DECRYPT | STEP_DECOMPRESS |
+					 STEP_DECOMPRESS_FIXED_OUTPUT);
 
 		/*
 		 * If we have only decompression step between decompression and
 		 * decrypt, we don't need post processing for this.
+		 * fixed-output should not decompress in irq, since it needs to
+		 * do allocating and mapping.
 		 */
 		if (enabled_steps == STEP_DECOMPRESS &&
 				!f2fs_low_mem_mode(sbi)) {
@@ -337,7 +432,7 @@ static void f2fs_write_end_io(struct bio *bio)
 
 	bio_for_each_segment_all(bvec, bio, iter_all) {
 		struct page *page = bvec->bv_page;
-		enum count_type type = WB_DATA_TYPE(page, false);
+		enum count_type type = WB_DATA_TYPE(page);
 
 		if (page_private_dummy(page)) {
 			clear_page_private_dummy(page);
@@ -491,30 +586,52 @@ static void f2fs_set_bio_crypt_ctx(struct bio *bio, const struct inode *inode,
 				  const struct f2fs_io_info *fio,
 				  gfp_t gfp_mask)
 {
+	struct inode *inner = NULL, *outer = (struct inode*)inode;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(outer, FI_SNAPSHOT_PREPARED))
+		inner = get_inner_inode(outer);
+#endif
 	/*
 	 * The f2fs garbage collector sets ->encrypted_page when it wants to
 	 * read/write raw data without encryption.
 	 */
 	if (!fio || !fio->encrypted_page)
-		fscrypt_set_bio_crypt_ctx(bio, inode, first_idx, gfp_mask);
-	else if (fscrypt_inode_should_skip_dm_default_key(inode))
+		fscrypt_set_bio_crypt_ctx(bio, inner ? :inode, first_idx, gfp_mask);
+	else if (fscrypt_inode_should_skip_dm_default_key(inner ? :inode))
 		bio_set_skip_dm_default_key(bio);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner)
+		put_inner_inode(inner);
+#endif
 }
 
 static bool f2fs_crypt_mergeable_bio(struct bio *bio, const struct inode *inode,
 				     pgoff_t next_idx,
 				     const struct f2fs_io_info *fio)
 {
+	bool ret;
+	struct inode *inner = NULL, *outer = (struct inode*)inode;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(outer, FI_SNAPSHOT_PREPARED))
+		inner = get_inner_inode(outer);
+#endif
 	/*
 	 * The f2fs garbage collector sets ->encrypted_page when it wants to
 	 * read/write raw data without encryption.
 	 */
 	if (fio && fio->encrypted_page)
-		return !bio_has_crypt_ctx(bio) &&
+		ret = !bio_has_crypt_ctx(bio) &&
 			(bio_should_skip_dm_default_key(bio) ==
-			 fscrypt_inode_should_skip_dm_default_key(inode));
-
-	return fscrypt_mergeable_bio(bio, inode, next_idx);
+			 fscrypt_inode_should_skip_dm_default_key(inner ? :inode));
+	else
+		ret = fscrypt_mergeable_bio(bio, inner ? :inode, next_idx);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner)
+		put_inner_inode(inner);
+#endif
+	return ret;
 }
 
 void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, struct bio *bio,
@@ -639,6 +756,11 @@ int f2fs_init_write_merge_io(struct f2fs_sb_info *sbi)
 		int n = (i == META) ? 1 : NR_TEMP_TYPE;
 		int j;
 
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_sb_has_seqzone(sbi) && i == DATA)
+			n = 2 * F2FS_NR_CPUS + 1; /* 8 hot 8 warm 1 cold */
+#endif
+
 		sbi->write_io[i] = f2fs_kmalloc(sbi,
 				array_size(n, sizeof(struct f2fs_bio_info)),
 				GFP_KERNEL);
@@ -693,9 +815,15 @@ static void __submit_merged_write_cond(struct f2fs_sb_info *sbi,
 {
 	enum temp_type temp;
 	bool ret = true;
+	int sum = NR_TEMP_TYPE;
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi) && type == DATA)
+		sum = 2 * F2FS_NR_CPUS + 1;
+#endif
 
-	for (temp = HOT; temp < NR_TEMP_TYPE; temp++) {
-		if (!force)	{
+	for (temp = HOT; temp < sum; temp++) {
+		if (!force) {
 			enum page_type btype = PAGE_TYPE_OF_BIO(type);
 			struct f2fs_bio_info *io = sbi->write_io[btype] + temp;
 
@@ -731,6 +859,16 @@ void f2fs_flush_merged_writes(struct f2fs_sb_info *sbi)
 	f2fs_submit_merged_write(sbi, META);
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+static noinline u32 get_seqzone_index(struct inode *inode, pgoff_t index);
+
+void f2fs_seqzone_fio_check(struct f2fs_io_info *fio)
+{
+	f2fs_bug_on(fio->sbi, !f2fs_seqzone_file(fio->page->mapping->host));
+	f2fs_bug_on(fio->sbi, fio->encrypted_page);
+}
+#endif
+
 /*
  * Fill the locked page with data located in the block address.
  * A caller needs to unlock the page on failure.
@@ -752,10 +890,19 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 
 	/* Allocate a new bio */
 	bio = __bio_alloc(fio, 1);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		f2fs_bug_on(fio->sbi, fio->seqzone_index == SEQZONE_ADDR);
+	}
 
+	f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
+			       fio->use_seqzone ? fio->seqzone_index : fio->page->index,
+			       fio, GFP_NOIO);
+#else
 	f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
 			       fio->page->index, fio, GFP_NOIO);
-
+#endif
 	if (bio_add_page(bio, page, PAGE_SIZE, 0) < PAGE_SIZE) {
 		bio_put(bio);
 		return -EFAULT;
@@ -765,7 +912,7 @@ int f2fs_submit_page_bio(struct f2fs_io_info *fio)
 		wbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);
 
 	inc_page_count(fio->sbi, is_read_io(fio->op) ?
-			__read_io_type(page) : WB_DATA_TYPE(fio->page, false));
+			__read_io_type(page) : WB_DATA_TYPE(fio->page));
 
 	if (is_read_io(bio_op(bio)))
 		f2fs_submit_read_bio(fio->sbi, bio, fio->type);
@@ -817,7 +964,14 @@ static bool io_is_mergeable(struct f2fs_sb_info *sbi, struct bio *bio,
 static void add_bio_entry(struct f2fs_sb_info *sbi, struct bio *bio,
 				struct page *page, enum temp_type temp)
 {
+	/* ipu bio add to #0 bio cache */
+#ifdef CONFIG_F2FS_SEQZONE
+	struct f2fs_bio_info *io = f2fs_sb_has_seqzone(sbi) ?
+		sbi->write_io[DATA] + temp * F2FS_NR_CPUS :
+		sbi->write_io[DATA] + temp;
+#else
 	struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+#endif
 	struct bio_entry *be;
 
 	be = f2fs_kmem_cache_alloc(bio_entry_slab, GFP_NOFS, true, NULL);
@@ -846,8 +1000,21 @@ static int add_ipu_page(struct f2fs_io_info *fio, struct bio **bio,
 	bool found = false;
 	int ret = -EAGAIN;
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		f2fs_bug_on(sbi, fio->seqzone_index == SEQZONE_ADDR);
+	}
+#endif
+
 	for (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {
+#ifdef CONFIG_F2FS_SEQZONE
+		struct f2fs_bio_info *io = f2fs_sb_has_seqzone(sbi) ?
+			sbi->write_io[DATA] + temp * F2FS_NR_CPUS :
+			sbi->write_io[DATA] + temp;
+#else
 		struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
+#endif
 		struct list_head *head = &io->bio_list;
 		struct bio_entry *be;
 
@@ -861,15 +1028,26 @@ static int add_ipu_page(struct f2fs_io_info *fio, struct bio **bio,
 			f2fs_bug_on(sbi, !page_is_mergeable(sbi, *bio,
 							    *fio->last_block,
 							    fio->new_blkaddr));
+#ifdef CONFIG_F2FS_SEQZONE
 			if (f2fs_crypt_mergeable_bio(*bio,
 					fio->page->mapping->host,
+					fio->use_seqzone ? fio->seqzone_index :
 					fio->page->index, fio) &&
 			    bio_add_page(*bio, page, PAGE_SIZE, 0) ==
 					PAGE_SIZE) {
 				ret = 0;
 				break;
 			}
-
+#else
+			if (f2fs_crypt_mergeable_bio(*bio,
+					fio->page->mapping->host,
+					fio->page->index, fio) &&
+			    bio_add_page(*bio, page, PAGE_SIZE, 0) ==
+					PAGE_SIZE) {
+				ret = 0;
+				break;
+			}
+#endif
 			/* page can't be merged into bio; submit the bio */
 			del_bio_entry(be);
 			f2fs_submit_write_bio(sbi, *bio, DATA);
@@ -892,10 +1070,15 @@ void f2fs_submit_merged_ipu_write(struct f2fs_sb_info *sbi,
 	enum temp_type temp;
 	bool found = false;
 	struct bio *target = bio ? *bio : NULL;
+	int types = NR_TEMP_TYPE;
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi))
+		types = F2FS_NR_CPUS * 2 + 1;
+#endif
 
 	f2fs_bug_on(sbi, !target && !page);
 
-	for (temp = HOT; temp < NR_TEMP_TYPE && !found; temp++) {
+	for (temp = HOT; temp < types && !found; temp++) {
 		struct f2fs_bio_info *io = sbi->write_io[DATA] + temp;
 		struct list_head *head = &io->bio_list;
 		struct bio_entry *be;
@@ -964,9 +1147,19 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 alloc_new:
 	if (!bio) {
 		bio = __bio_alloc(fio, BIO_MAX_VECS);
+#ifdef CONFIG_F2FS_SEQZONE
+		if (fio->use_seqzone) {
+			f2fs_seqzone_fio_check(fio);
+			f2fs_bug_on(fio->sbi, fio->seqzone_index == SEQZONE_ADDR);
+		}
+
+		f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
+				       fio->use_seqzone ? fio->seqzone_index : fio->page->index,
+				       fio, GFP_NOIO);
+#else
 		f2fs_set_bio_crypt_ctx(bio, fio->page->mapping->host,
 				       fio->page->index, fio, GFP_NOIO);
-
+#endif
 		add_bio_entry(fio->sbi, bio, page, fio->temp);
 	} else {
 		if (add_ipu_page(fio, &bio, page))
@@ -976,7 +1169,7 @@ int f2fs_merge_page_bio(struct f2fs_io_info *fio)
 	if (fio->io_wbc)
 		wbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);
 
-	inc_page_count(fio->sbi, WB_DATA_TYPE(page, false));
+	inc_page_count(fio->sbi, WB_DATA_TYPE(page));
 
 	*fio->last_block = fio->new_blkaddr;
 	*fio->bio = bio;
@@ -1008,10 +1201,17 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 {
 	struct f2fs_sb_info *sbi = fio->sbi;
 	enum page_type btype = PAGE_TYPE_OF_BIO(fio->type);
-	struct f2fs_bio_info *io = sbi->write_io[btype] + fio->temp;
+	struct f2fs_bio_info *io;
 	struct page *bio_page;
-	enum count_type type;
+	int real_temp = fio->temp;
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		real_temp = fio->real_temp;
+	}
+#endif
+	io = sbi->write_io[btype] + real_temp;
 	f2fs_bug_on(sbi, is_read_io(fio->op));
 
 	f2fs_down_write(&io->io_rwsem);
@@ -1049,22 +1249,35 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	/* set submitted = true as a return value */
 	fio->submitted = 1;
 
-	type = WB_DATA_TYPE(bio_page, fio->compressed_page);
-	inc_page_count(sbi, type);
-
+	inc_page_count(sbi, WB_DATA_TYPE(bio_page));
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (io->bio) {
+		if (!io_is_mergeable(sbi, io->bio, io, fio, io->last_block_in_bio,
+			      fio->new_blkaddr))
+			__submit_merged_bio(io);
+		else if (!f2fs_crypt_mergeable_bio(io->bio, fio->page->mapping->host,
+				       bio_page->index, fio)) {
+			if ((fio->encrypted_page)
+			|| bio_first_page_all(io->bio)->mapping->host != fio->page->mapping->host
+			|| !fio->use_seqzone)
+				__submit_merged_bio(io);
+		}
+	}
+#else
 	if (io->bio &&
 	    (!io_is_mergeable(sbi, io->bio, io, fio, io->last_block_in_bio,
 			      fio->new_blkaddr) ||
 	     !f2fs_crypt_mergeable_bio(io->bio, fio->page->mapping->host,
 				       bio_page->index, fio)))
 		__submit_merged_bio(io);
+#endif
 alloc_new:
 	if (io->bio == NULL) {
 		if (F2FS_IO_ALIGNED(sbi) &&
 				(fio->type == DATA || fio->type == NODE) &&
 				fio->new_blkaddr & F2FS_IO_SIZE_MASK(sbi)) {
-			dec_page_count(sbi, WB_DATA_TYPE(bio_page,
-						fio->compressed_page));
+			dec_page_count(sbi, WB_DATA_TYPE(bio_page));
 			fio->retry = 1;
 			goto skip;
 		}
@@ -1079,6 +1292,12 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 		goto alloc_new;
 	}
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone)
+		fio->seqzone_index = bio_first_page_all(io->bio)->index
+					+ io->bio->bi_iter.bi_size / PAGE_SIZE - 1;
+#endif
+
 	if (fio->io_wbc)
 		wbc_account_cgroup_owner(fio->io_wbc, fio->page, PAGE_SIZE);
 
@@ -1107,6 +1326,46 @@ void f2fs_submit_page_write(struct f2fs_io_info *fio)
 	f2fs_up_write(&io->io_rwsem);
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+static void bio_set_post_read_account(struct inode *inode, struct bio *bio)
+{
+	struct bio_post_read_ctx *ctx = get_post_read_ctx(bio);
+
+	ctx->enabled_steps |= STEP_READ_ACCOUNT;
+	ctx->inode = inode;
+}
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+static noinline u32 get_seqzone_index(struct inode *inode, pgoff_t index)
+{
+	struct dnode_of_data dn;
+	struct f2fs_node *rn;
+	__le32 *addr_array;
+	int base = 0;
+	int err;
+	u32 ret;
+	int addrs;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	if (!f2fs_seqzone_file(inode))
+		return 0;
+	set_new_dnode(&dn, inode, NULL ,NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+	if (err) {
+		f2fs_info(sbi, "get_seqzone_index get dnode failed!");
+		return 0;
+	}
+	rn = F2FS_NODE(dn.node_page);
+	addr_array = blkaddr_in_node(rn);
+	if (IS_INODE(dn.node_page) && f2fs_has_extra_attr(dn.inode))
+		base = get_extra_isize(dn.inode);
+	addrs = ADDRS_PER_PAGE(dn.node_page, dn.inode);
+	ret = le32_to_cpu(addr_array[base + dn.ofs_in_node + addrs]);
+	f2fs_put_dnode(&dn);
+	return ret;
+}
+#endif
+
 static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 				      unsigned nr_pages, blk_opf_t op_flag,
 				      pgoff_t first_idx, bool for_write)
@@ -1117,14 +1376,25 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	unsigned int post_read_steps = 0;
 	sector_t sector;
 	struct block_device *bdev = f2fs_target_device(sbi, blkaddr, &sector);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	bool use_seqzone = false;
+	u32 seqzone_index = 0;
+#endif
 	bio = bio_alloc_bioset(bdev, bio_max_segs(nr_pages),
 			       REQ_OP_READ | op_flag,
 			       for_write ? GFP_NOIO : GFP_KERNEL, &f2fs_bioset);
 	if (!bio)
 		return ERR_PTR(-ENOMEM);
 	bio->bi_iter.bi_sector = sector;
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode)) {
+		seqzone_index = get_seqzone_index(inode, first_idx);
+		use_seqzone = true;
+	}
+	f2fs_set_bio_crypt_ctx(bio, inode, (use_seqzone && seqzone_index) ? seqzone_index : first_idx, NULL, GFP_NOFS);
+#else
 	f2fs_set_bio_crypt_ctx(bio, inode, first_idx, NULL, GFP_NOFS);
+#endif
 	bio->bi_end_io = f2fs_read_end_io;
 
 	if (fscrypt_inode_uses_fs_layer_crypto(inode))
@@ -1140,7 +1410,12 @@ static struct bio *f2fs_grab_read_bio(struct inode *inode, block_t blkaddr,
 	 * responsible for enabling STEP_DECOMPRESS if it's actually needed.
 	 */
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (post_read_steps || f2fs_compressed_file(inode) ||
+			f2fs_inode_support_dedup(sbi, inode)) {
+#else
 	if (post_read_steps || f2fs_compressed_file(inode)) {
+#endif
 		/* Due to the mempool, this never fails. */
 		ctx = mempool_alloc(bio_post_read_ctx_pool, GFP_NOFS);
 		ctx->bio = bio;
@@ -1181,7 +1456,7 @@ static int f2fs_submit_page_read(struct inode *inode, struct page *page,
 	return 0;
 }
 
-static void __set_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
+void __set_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr)
 {
 	__le32 *addr = get_dnode_addr(dn->inode, dn->node_page);
 
@@ -1486,12 +1761,12 @@ static int __allocate_data_block(struct dnode_of_data *dn, int seg_type)
 	set_summary(&sum, dn->nid, dn->ofs_in_node, ni.version);
 	old_blkaddr = dn->data_blkaddr;
 	err = f2fs_allocate_data_block(sbi, NULL, old_blkaddr,
-				&dn->data_blkaddr, &sum, seg_type, NULL);
+				&dn->data_blkaddr, &sum, seg_type, NULL, 0);
 	if (err)
 		return err;
 
 	if (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO)
-		f2fs_invalidate_internal_cache(sbi, old_blkaddr, 1);
+		f2fs_invalidate_internal_cache(sbi, old_blkaddr);
 
 	f2fs_update_data_blkaddr(dn, dn->data_blkaddr);
 	return 0;
@@ -1557,6 +1832,11 @@ static bool f2fs_map_blocks_cached(struct inode *inode,
 	pgoff_t pgoff = (pgoff_t)map->m_lblk;
 	struct extent_info ei = {};
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode) && flag == F2FS_GET_BLOCK_DIO)
+		return false;
+#endif
+
 	if (!f2fs_lookup_read_extent_cache(inode, pgoff, &ei))
 		return false;
 
@@ -1603,9 +1883,19 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 	unsigned int start_pgofs;
 	int bidx = 0;
 	bool is_hole;
+#ifdef CONFIG_F2FS_SEQZONE
+	bool dio_ipu = false;
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *outer = NULL;
+#endif
 
 	if (!maxblocks)
 		return 0;
+#ifdef CONFIG_F2FS_SEQZONE
+	else if (maxblocks > 2 * DEF_MIN_FSYNC_BLOCKS)
+		dio_ipu = true;
+#endif
 
 	if (!map->m_may_create && f2fs_map_blocks_cached(inode, map, flag))
 		goto out;
@@ -1614,6 +1904,27 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 	map->m_multidev_dio =
 		f2fs_allow_multi_device_dio(F2FS_I_SB(inode), flag);
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	/*
+	 * For generic read, flag is F2FS_GET_BLOCK_DEFAULT,
+	 * caller have decided to use inner or outer inode.
+	 * Here, we just accept. Otherwise, it may cause inode
+	 * inconsistency.
+	 */
+	if (flag != F2FS_GET_BLOCK_DEFAULT &&
+			f2fs_is_outer_inode(inode)) {
+		/* if create != 0 && not snapshot prepared , the inode should not be deduped inode */
+		if (map->m_may_create && !is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED)) {
+			f2fs_err(sbi, "inode[%lu] dedup addr error", inode->i_ino);
+			f2fs_bug_on(sbi, 1);
+		}
+		inner = get_inner_inode(inode);
+		if (inner) {
+			outer = inode;
+			inode = inner;
+		}
+	}
+#endif
 	map->m_len = 0;
 	map->m_flags = 0;
 
@@ -1652,8 +1963,14 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 	}
 
 	/* use out-place-update for direct IO under LFS mode */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (map->m_may_create &&
+	    (is_hole || (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO) ||
+		(f2fs_seqzone_file(inode) && flag == F2FS_GET_BLOCK_DIO))) {
+#else
 	if (map->m_may_create &&
 	    (is_hole || (f2fs_lfs_mode(sbi) && flag == F2FS_GET_BLOCK_DIO))) {
+#endif
 		if (unlikely(f2fs_cp_error(sbi))) {
 			err = -EIO;
 			goto sync_out;
@@ -1668,7 +1985,25 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 			break;
 		case F2FS_GET_BLOCK_PRE_DIO:
 		case F2FS_GET_BLOCK_DIO:
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(inode)) {
+				dn.seqzone_index = map->m_seqblk + map->m_len;
+
+				if (!is_hole && dio_ipu) {
+					f2fs_wait_on_page_writeback(dn.node_page, NODE, true, true);
+					err = __set_seqzone_index(&dn);
+					if (set_page_dirty(dn.node_page))
+						dn.node_changed = true;
+				} else {
+					dn.use_seqzone = true;
+					err = __allocate_data_block(&dn, map->m_seg_type);
+				}
+			} else {
+				err = __allocate_data_block(&dn, map->m_seg_type);
+			}
+#else
 			err = __allocate_data_block(&dn, map->m_seg_type);
+#endif
 			if (err)
 				goto sync_out;
 			if (flag == F2FS_GET_BLOCK_PRE_DIO)
@@ -1737,6 +2072,13 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 			flag == F2FS_GET_BLOCK_PRE_DIO) {
 		if (map->m_multidev_dio && map->m_bdev != FDEV(bidx).bdev)
 			goto sync_out;
+#ifdef CONFIG_F2FS_SEQZONE
+		if (flag == F2FS_GET_BLOCK_DIO && !map->m_may_create &&
+			f2fs_seqzone_file(inode)
+			&& seqzone_index(inode, dn.node_page, dn.ofs_in_node) != map->m_seqblk + ofs)
+			goto sync_out;
+
+#endif
 		ofs++;
 		map->m_len++;
 	} else {
@@ -1832,6 +2174,13 @@ int f2fs_map_blocks(struct inode *inode, struct f2fs_map_blocks *map, int flag)
 		f2fs_balance_fs(sbi, dn.node_changed);
 	}
 out:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		trace_f2fs_dedup_map_blocks(outer, inner);
+		inode = outer;
+		put_inner_inode(inner);
+	}
+#endif
 	trace_f2fs_map_blocks(inode, map, flag, err);
 	return err;
 }
@@ -2114,7 +2463,10 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	sector_t last_block_in_file;
 	sector_t block_nr;
 	int ret = 0;
-
+#ifdef CONFIG_F2FS_SEQZONE
+	bool use_seqzone = false;
+	u32 seqzone_index = 0;
+#endif
 	block_in_file = (sector_t)page_index(page);
 	last_block = block_in_file + nr_pages;
 	last_block_in_file = bytes_to_blks(inode,
@@ -2179,9 +2531,20 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 	 * This page will go to BIO.  Do we need to send this
 	 * BIO off first?
 	 */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode)) {
+		seqzone_index = get_seqzone_index(inode, page->index);
+		use_seqzone = true;
+	}
+	if (bio && (!page_is_mergeable(F2FS_I_SB(inode), bio,
+				       *last_block_in_bio, block_nr) ||
+		    !f2fs_crypt_mergeable_bio(bio, inode, (use_seqzone && seqzone_index) ?
+					      seqzone_index : page->index, NULL))) {
+#else
 	if (bio && (!page_is_mergeable(F2FS_I_SB(inode), bio,
 				       *last_block_in_bio, block_nr) ||
 		    !f2fs_crypt_mergeable_bio(bio, inode, page->index, NULL))) {
+#endif
 submit_and_realloc:
 		f2fs_submit_read_bio(F2FS_I_SB(inode), bio, DATA);
 		bio = NULL;
@@ -2195,6 +2558,18 @@ static int f2fs_read_single_page(struct inode *inode, struct page *page,
 			bio = NULL;
 			goto out;
 		}
+#ifdef CONFIG_F2FS_FS_DEDUP
+		/*
+		 * If read and dedup process concurrently,
+		 * apps may read unexpected data, such as truncated data.
+		 * Let dedup wait all bio of the file complete before
+		 * doing truncate.
+		 */
+		if (f2fs_inode_support_dedup(F2FS_I_SB(inode), inode)) {
+			bio_set_post_read_account(inode, bio);
+			inode_inc_read_io(inode);
+		}
+#endif
 	}
 
 	/*
@@ -2313,6 +2688,17 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		goto out_put_dnode;
 	}
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT) {
+		__le32 *addr = decompress_index_addr(inode, dn.node_page);
+
+		for (i = 0; i < cc->cluster_size; i++) {
+			decompress_index_t val = le32_to_cpu(addr[dn.ofs_in_node + i]);
+			deserialize_decompress_index(val, &cc->di[i]);
+		}
+	}
+#endif
+
 	dic = f2fs_alloc_dic(cc);
 	if (IS_ERR(dic)) {
 		ret = PTR_ERR(dic);
@@ -2324,6 +2710,14 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 		block_t blkaddr;
 		struct bio_post_read_ctx *ctx;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (!page) {
+			f2fs_bug_on(sbi, f2fs_compress_layout(inode) !=
+							COMPRESS_FIXED_OUTPUT);
+			continue;
+		}
+#endif
+
 		blkaddr = from_dnode ? data_blkaddr(dn.inode, dn.node_page,
 					dn.ofs_in_node + i + 1) :
 					ei.blk + i;
@@ -2362,6 +2756,10 @@ int f2fs_read_multi_pages(struct compress_ctx *cc, struct bio **bio_ret,
 
 		ctx = get_post_read_ctx(bio);
 		ctx->enabled_steps |= STEP_DECOMPRESS;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT)
+			ctx->enabled_steps |= STEP_DECOMPRESS_FIXED_OUTPUT;
+#endif
 		refcount_inc(&dic->refcnt);
 
 		inc_page_count(sbi, F2FS_RD_DATA);
@@ -2416,6 +2814,9 @@ static int f2fs_mpage_readpages(struct inode *inode,
 	unsigned nr_pages = rac ? readahead_count(rac) : 1;
 	unsigned max_nr_pages = nr_pages;
 	int ret = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *outer = NULL, *prev = NULL;
+#endif
 
 	map.m_pblk = 0;
 	map.m_lblk = 0;
@@ -2470,10 +2871,40 @@ static int f2fs_mpage_readpages(struct inode *inode,
 			goto next_page;
 		}
 read_single_page:
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+		inode_inc_read_io(inode);
+		inner = get_inner_inode(inode);
+		if (inner) {
+			trace_f2fs_dedup_map_readpage(inode, inner);
+			outer = inode;
+			inode = inner;
+		}
+
+		/*
+		 * If the inode do revoke or dedup during read,
+		 * we should clear previous map to avoid get wrong
+		 * physic block.
+		 */
+		if (!prev)
+			prev = inode;
+		if (unlikely(inode != prev)) {
+			memset(&map, 0, sizeof(map));
+			map.m_seg_type = NO_CHECK_TYPE;
+			prev = inode;
+		}
+
 #endif
 
 		ret = f2fs_read_single_page(inode, page, max_nr_pages, &map,
 					&bio, &last_block_in_bio, rac);
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (inner) {
+			inode = outer;
+			put_inner_inode(inner);
+		}
+		inode_dec_read_io(inode);
+#endif
 		if (ret) {
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 set_error_page:
@@ -2502,6 +2933,7 @@ static int f2fs_mpage_readpages(struct inode *inode,
 	}
 	if (bio)
 		f2fs_submit_read_bio(F2FS_I_SB(inode), bio, DATA);
+	f2fs_update_atime(inode, true);
 	return ret;
 }
 
@@ -2553,6 +2985,9 @@ int f2fs_encrypt_one_page(struct f2fs_io_info *fio)
 
 	page = fio->compressed_page ? fio->compressed_page : fio->page;
 
+	/* wait for GCed page writeback via META_MAPPING */
+	f2fs_wait_on_block_writeback(inode, fio->old_blkaddr);
+
 	if (fscrypt_inode_uses_inline_crypto(inode))
 		return 0;
 
@@ -2649,6 +3084,16 @@ bool f2fs_should_update_outplace(struct inode *inode, struct f2fs_io_info *fio)
 		return true;
 	if (f2fs_used_in_atomic_write(inode))
 		return true;
+	/* rewrite low ratio compress data w/ OPU to avoid fragmentation */
+	if (f2fs_compressed_file(inode) &&
+		F2FS_OPTION(sbi).compress_mode == COMPR_MODE_USER &&
+		is_inode_flag_set(inode, FI_ENABLE_COMPRESS))
+		return true;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode) &&
+		is_inode_flag_set(inode, FI_REVOKE_DEDUP))
+		return true;
+#endif
 
 	/* swap file is migrating in aligned write mode */
 	if (is_inode_flag_set(inode, FI_ALIGNED_WRITE))
@@ -2697,9 +3142,15 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 	else
 		set_new_dnode(&dn, inode, NULL, NULL, 0);
 
+#ifdef CONFIG_F2FS_SEQZONE
+	if (need_inplace_update(fio) && !f2fs_seqzone_file(inode) &&
+	    f2fs_lookup_read_extent_cache_block(inode, page->index,
+						&fio->old_blkaddr)) {
+#else
 	if (need_inplace_update(fio) &&
 	    f2fs_lookup_read_extent_cache_block(inode, page->index,
 						&fio->old_blkaddr)) {
+#endif
 		if (!f2fs_is_valid_blkaddr(fio->sbi, fio->old_blkaddr,
 						DATA_GENERIC_ENHANCE)) {
 			f2fs_handle_error(fio->sbi,
@@ -2752,6 +3203,13 @@ int f2fs_do_write_data_page(struct f2fs_io_info *fio)
 		if (err)
 			goto out_writepage;
 
+#ifdef CONFIG_F2FS_SEQZONE
+		if (fio->use_seqzone) {
+			f2fs_seqzone_fio_check(fio);
+			f2fs_bug_on(fio->sbi, fio->seqzone_index != SEQZONE_ADDR);
+			fio->seqzone_index = dn.seqzone_index;
+		}
+#endif
 		set_page_writeback(page);
 		f2fs_put_dnode(&dn);
 		if (fio->need_lock == LOCK_REQ)
@@ -2841,6 +3299,11 @@ int f2fs_write_single_data_page(struct page *page, int *submitted,
 		.io_wbc = wbc,
 		.bio = bio,
 		.last_block = last_block,
+#ifdef CONFIG_F2FS_SEQZONE
+		.use_seqzone = f2fs_seqzone_file(inode),
+		.seqzone_index = f2fs_seqzone_file(inode) ?
+						SEQZONE_ADDR : NEW_ADDR,
+#endif
 	};
 
 	trace_f2fs_writepage(page, DATA);
@@ -2912,7 +3375,6 @@ int f2fs_write_single_data_page(struct page *page, int *submitted,
 	if (err == -EAGAIN) {
 		err = f2fs_do_write_data_page(&fio);
 		if (err == -EAGAIN) {
-			f2fs_bug_on(sbi, compr_blocks);
 			fio.need_lock = LOCK_REQ;
 			err = f2fs_do_write_data_page(&fio);
 		}
@@ -3008,7 +3470,8 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 {
 	int ret = 0;
 	int done = 0, retry = 0;
-	struct page *pages[F2FS_ONSTACK_PAGES];
+	struct page *pages_local[F2FS_ONSTACK_PAGES];
+	struct page **pages = pages_local;
 	struct f2fs_sb_info *sbi = F2FS_M_SB(mapping);
 	struct bio *bio = NULL;
 	sector_t last_block;
@@ -3030,6 +3493,7 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	};
 #endif
 	int nr_pages;
+	unsigned int max_pages = F2FS_ONSTACK_PAGES;
 	pgoff_t index;
 	pgoff_t end;		/* Inclusive */
 	pgoff_t done_index;
@@ -3039,6 +3503,15 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	int submitted = 0;
 	int i;
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (f2fs_compressed_file(inode) &&
+		1 << cc.log_cluster_size > F2FS_ONSTACK_PAGES) {
+		pages = f2fs_kzalloc(sbi, sizeof(struct page *) <<
+				cc.log_cluster_size, __GFP_NOFAIL | GFP_NOFS);
+		max_pages = 1 << cc.log_cluster_size;
+	}
+#endif
+
 	if (get_dirty_pages(mapping->host) <=
 				SM_I(F2FS_M_SB(mapping))->min_hot_blocks)
 		set_inode_flag(mapping->host, FI_HOT_DATA);
@@ -3064,8 +3537,11 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 		tag_pages_for_writeback(mapping, index, end);
 	done_index = index;
 	while (!done && !retry && (index <= end)) {
+		/* fix coverity error: Dereferencing a pointer that might be NULL pages */
+		if (!pages)
+			break;
 		nr_pages = find_get_pages_range_tag(mapping, &index, end,
-				tag, F2FS_ONSTACK_PAGES, pages);
+				tag, max_pages, pages);
 		if (nr_pages == 0)
 			break;
 
@@ -3238,6 +3714,10 @@ static int f2fs_write_cache_pages(struct address_space *mapping,
 	if (bio)
 		f2fs_submit_merged_ipu_write(sbi, &bio, NULL);
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (pages != pages_local)
+		kfree(pages);
+#endif
 	return ret;
 }
 
@@ -3338,10 +3818,22 @@ static int f2fs_write_data_pages(struct address_space *mapping,
 			    struct writeback_control *wbc)
 {
 	struct inode *inode = mapping->host;
+	struct inode *inner = NULL;
+	int ret = 0;
 
-	return __f2fs_write_data_pages(mapping, wbc,
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		inner = get_inner_inode(inode);
+#endif
+	ret = __f2fs_write_data_pages(inner ? inner->i_mapping : mapping, wbc,
 			F2FS_I(inode)->cp_task == current ?
 			FS_CP_DATA_IO : FS_DATA_IO);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner)
+		put_inner_inode(inner);
+#endif
+	return ret;
 }
 
 void f2fs_write_failed(struct inode *inode, loff_t to)
@@ -4035,6 +4527,13 @@ static int f2fs_swap_activate(struct swap_info_struct *sis, struct file *file,
 			"Swapfile not supported in LFS mode");
 		return -EINVAL;
 	}
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode)) {
+		f2fs_err(F2FS_I_SB(inode),
+			"Swapfile not supported in dedup mode");
+		return -EOPNOTSUPP;
+	}
+#endif
 
 	ret = f2fs_convert_inline_inode(inode);
 	if (ret)
@@ -4166,27 +4665,42 @@ static int f2fs_iomap_begin(struct inode *inode, loff_t offset, loff_t length,
 	struct f2fs_map_blocks map = {};
 	pgoff_t next_pgofs = 0;
 	int err;
+#ifdef CONFIG_F2FS_SEQZONE
+	struct f2fs_inode_info* fi = F2FS_I(inode);
+#endif
 
 	map.m_lblk = bytes_to_blks(inode, offset);
 	map.m_len = bytes_to_blks(inode, offset + length - 1) - map.m_lblk + 1;
 	map.m_next_pgofs = &next_pgofs;
 	map.m_seg_type = f2fs_rw_hint_to_seg_type(inode->i_write_hint);
-	if (flags & IOMAP_WRITE)
+	if (flags & IOMAP_WRITE) {
 		map.m_may_create = true;
-
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_seqzone_file(inode))
+			map.m_seqblk = fi->dio_cur_seqindex;
+	} else if (f2fs_seqzone_file(inode)) {
+		map.m_seqblk = get_seqzone_index(inode, map.m_lblk);
+#endif
+	}
 	err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DIO);
 	if (err)
 		return err;
 
 	iomap->offset = blks_to_bytes(inode, map.m_lblk);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		iomap->android_kabi_reserved1 = (u64)map.m_seqblk;
+#endif
 	/*
 	 * When inline encryption is enabled, sometimes I/O to an encrypted file
 	 * has to be broken up to guarantee DUN contiguity.  Handle this by
 	 * limiting the length of the mapping returned.
 	 */
 	map.m_len = fscrypt_limit_io_blocks(inode, map.m_lblk, map.m_len);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if ((flags & IOMAP_WRITE) && f2fs_seqzone_file(inode))
+		fi->dio_cur_seqindex += map.m_len;
+#endif
 	/*
 	 * We should never see delalloc or compressed extents here based on
 	 * prior flushing and checks.
diff --git a/fs/f2fs/extent_cache.c b/fs/f2fs/extent_cache.c
old mode 100644
new mode 100755
index fdd612fc2..eb1c05abc
--- a/fs/f2fs/extent_cache.c
+++ b/fs/f2fs/extent_cache.c
@@ -74,14 +74,40 @@ static void __set_extent_info(struct extent_info *ei,
 	}
 }
 
+static bool __may_read_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (!test_opt(sbi, READ_EXTENT_CACHE))
+		return false;
+	if (is_inode_flag_set(inode, FI_NO_EXTENT))
+		return false;
+	if (is_inode_flag_set(inode, FI_COMPRESSED_FILE) &&
+			 !f2fs_sb_has_readonly(sbi))
+		return false;
+	return S_ISREG(inode->i_mode);
+}
+
+static bool __may_age_extent_tree(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (!test_opt(sbi, AGE_EXTENT_CACHE))
+		return false;
+	if (is_inode_flag_set(inode, FI_COMPRESSED_FILE))
+		return false;
+	if (file_is_cold(inode))
+		return false;
+
+	return S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode);
+}
+
 static bool __init_may_extent_tree(struct inode *inode, enum extent_type type)
 {
 	if (type == EX_READ)
-		return test_opt(F2FS_I_SB(inode), READ_EXTENT_CACHE) &&
-			S_ISREG(inode->i_mode);
-	if (type == EX_BLOCK_AGE)
-		return test_opt(F2FS_I_SB(inode), AGE_EXTENT_CACHE) &&
-			(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode));
+		return __may_read_extent_tree(inode);
+	else if (type == EX_BLOCK_AGE)
+		return __may_age_extent_tree(inode);
 	return false;
 }
 
@@ -94,22 +120,7 @@ static bool __may_extent_tree(struct inode *inode, enum extent_type type)
 	if (list_empty(&F2FS_I_SB(inode)->s_list))
 		return false;
 
-	if (!__init_may_extent_tree(inode, type))
-		return false;
-
-	if (type == EX_READ) {
-		if (is_inode_flag_set(inode, FI_NO_EXTENT))
-			return false;
-		if (is_inode_flag_set(inode, FI_COMPRESSED_FILE) &&
-				 !f2fs_sb_has_readonly(F2FS_I_SB(inode)))
-			return false;
-	} else if (type == EX_BLOCK_AGE) {
-		if (is_inode_flag_set(inode, FI_COMPRESSED_FILE))
-			return false;
-		if (file_is_cold(inode))
-			return false;
-	}
-	return true;
+	return __init_may_extent_tree(inode, type);
 }
 
 static void __try_update_largest_extent(struct extent_tree *et,
diff --git a/fs/f2fs/f2fs.h b/fs/f2fs/f2fs.h
old mode 100644
new mode 100755
index aa2f0ea6b..3f90ff82b
--- a/fs/f2fs/f2fs.h
+++ b/fs/f2fs/f2fs.h
@@ -60,18 +60,52 @@ enum {
 	FAULT_SLAB_ALLOC,
 	FAULT_DQUOT_INIT,
 	FAULT_LOCK_OP,
-	FAULT_BLKADDR_VALIDITY,
-	FAULT_BLKADDR_CONSISTENCE,
+	FAULT_BLKADDR,
+#ifdef CONFIG_F2FS_APPBOOST
+	FAULT_READ_ERROR,
+	FAULT_WRITE_ERROR,
+	FAULT_PAGE_ERROR,
+	FAULT_FSYNC_ERROR,
+	FAULT_FLUSH_ERROR,
+	FAULT_WRITE_TAIL_ERROR,
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	FAULT_DEDUP_WRITEBACK,
+	FAULT_DEDUP_TRUNCATE,
+	FAULT_DEDUP_FILL_INODE,
+	FAULT_DEDUP_SAME_FILE,
+	FAULT_DEDUP_PARAM_CHECK,
+	FAULT_DEDUP_CRYPT_POLICY,
+	FAULT_DEDUP_REVOKE,
+	FAULT_DEDUP_ORPHAN_INODE,
+	FAULT_DEDUP_INIT_INNER,
+	FAULT_DEDUP_HOLE,
+	FAULT_DEDUP_CLONE,
+	FAULT_DEDUP_OPEN,
+	FAULT_DEDUP_SETXATTR,
+#endif
+	FAULT_COMPRESS_REDIRTY,
+	FAULT_COMPRESS_WRITEBACK,
+	FAULT_COMPRESS_RESERVE_NOSPC,
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	FAULT_COMPRESS_VMAP,
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	FAULT_COMPRESS_INIT_CTX,
+	FAULT_COMPRESS_PAGE_ARRAY,
+	FAULT_COMPRESS_LOW_RATIO,
+	FAULT_COMPRESS_GET_DNODE,
+#endif
+#endif
 	FAULT_MAX,
 };
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
-#define F2FS_ALL_FAULT_TYPE		(GENMASK(FAULT_MAX - 1, 0))
+#define F2FS_ALL_FAULT_TYPE		(GENMASK_ULL(FAULT_MAX - 1, 0))
 
 struct f2fs_fault_info {
 	atomic_t inject_ops;
 	int inject_rate;
-	unsigned int inject_type;
+	unsigned long long inject_type;
 };
 
 extern const char *f2fs_fault_name[FAULT_MAX];
@@ -115,6 +149,8 @@ extern const char *f2fs_fault_name[FAULT_MAX];
 #define F2FS_MOUNT_COMPRESS_CACHE	0x40000000
 #define F2FS_MOUNT_AGE_EXTENT_CACHE	0x80000000
 
+#define F2FS_MOUNT_FILE_DEDUP		0x80000000
+
 #define F2FS_OPTION(sbi)	((sbi)->mount_opt)
 #define clear_opt(sbi, option)	(F2FS_OPTION(sbi).opt &= ~F2FS_MOUNT_##option)
 #define set_opt(sbi, option)	(F2FS_OPTION(sbi).opt |= F2FS_MOUNT_##option)
@@ -146,6 +182,9 @@ struct f2fs_rwsem {
 #endif
 };
 
+#define OPLUS_FEAT_COMPR	0x1
+#define OPLUS_FEAT_DEDUP	0x2
+
 struct f2fs_mount_info {
 	unsigned int opt;
 	int write_io_size_bits;		/* Write IO size bits */
@@ -187,6 +226,9 @@ struct f2fs_mount_info {
 	unsigned char compress_ext_cnt;		/* extension count */
 	unsigned char nocompress_ext_cnt;		/* nocompress extension count */
 	int compress_mode;			/* compression mode */
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	unsigned char compress_layout;		/* compression layout */
+#endif
 	unsigned char extensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN];	/* extensions */
 	unsigned char noextensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN]; /* extensions */
 };
@@ -206,6 +248,15 @@ struct f2fs_mount_info {
 #define F2FS_FEATURE_CASEFOLD		0x1000
 #define F2FS_FEATURE_COMPRESSION	0x2000
 #define F2FS_FEATURE_RO			0x4000
+#ifdef CONFIG_F2FS_SEQZONE
+#define F2FS_FEATURE_SEQZONE		0x20000000
+#define F2FS_NR_CPUS			(8)
+enum {
+	ENABLE_SEQZONE_EXCEPT_COLD = 1,
+	ENABLE_SEQZONE_HOT_FILES,
+};
+#endif
+#define F2FS_FEATURE_DEDUP		0x80000000
 
 #define __F2FS_HAS_FEATURE(raw_super, mask)				\
 	((raw_super->feature & cpu_to_le32(mask)) != 0)
@@ -380,6 +431,12 @@ enum {
 	MAX_DPOLICY,
 };
 
+enum {
+	DPOLICY_IO_AWARE_DISABLE,	/* force to not be aware of IO */
+	DPOLICY_IO_AWARE_ENABLE,	/* force to be aware of IO */
+	DPOLICY_IO_AWARE_MAX,
+};
+
 struct discard_policy {
 	int type;			/* type of discard */
 	unsigned int min_interval;	/* used for candidates exist */
@@ -412,6 +469,7 @@ struct discard_cmd_control {
 	unsigned int discard_urgent_util;	/* utilization which issue discard proactively */
 	unsigned int discard_granularity;	/* discard granularity */
 	unsigned int max_ordered_discard;	/* maximum discard granularity issued by lba order */
+	unsigned int discard_io_aware;		/* io_aware policy */
 	unsigned int undiscard_blks;		/* # of undiscard blocks */
 	unsigned int next_pos;			/* next discard position */
 	atomic_t issued_discard;		/* # of issued discard */
@@ -693,6 +751,9 @@ struct f2fs_map_blocks {
 	struct block_device *m_bdev;	/* for multi-device dio */
 	block_t m_pblk;
 	block_t m_lblk;
+#ifdef CONFIG_F2FS_SEQZONE
+	block_t m_seqblk;
+#endif
 	unsigned int m_len;
 	unsigned int m_flags;
 	pgoff_t *m_next_pgofs;		/* point next possible non-hole pgofs */
@@ -800,7 +861,26 @@ enum {
 	FI_ATOMIC_COMMITTED,	/* indicate atomic commit completed except disk sync */
 	FI_ATOMIC_DIRTIED,	/* indicate atomic file is dirtied */
 	FI_ATOMIC_REPLACE,	/* indicate atomic replace */
-	FI_OPENED_FILE,         /* indicate file has been opened */
+#ifdef CONFIG_F2FS_FS_DEDUP
+	/* use for dedup */
+	FI_DEDUPED,
+	FI_INNER_INODE,
+	FI_UPDATED,
+	FI_REVOKE_DEDUP,
+	FI_DOING_DEDUP,
+	FI_META_UN_MODIFY,
+	FI_DATA_UN_MODIFY,
+#ifdef CONFIG_F2FS_APPBOOST
+	FI_MERGED_FILE,
+#endif
+	/* use for snapshot */
+	FI_SNAPSHOT_PREPARED,
+	FI_SNAPSHOTED,
+#ifdef CONFIG_F2FS_SEQZONE
+	FI_SEQZONE,		/* enable seqzone */
+#endif
+#endif
+    FI_OPENED_FILE,         /* indicate file has been opened */
 	FI_MAX,			/* max flag, never be used */
 };
 
@@ -865,6 +945,20 @@ struct f2fs_inode_info {
 
 	unsigned int atomic_write_cnt;
 	loff_t original_i_size;		/* original i_size before atomic write */
+#ifdef CONFIG_F2FS_APPBOOST
+	struct fi_merge_manage *i_boostfile; /* boostfile merge info */
+	atomic_t appboost_abort;
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner_inode;
+	atomic_t inflight_read_io;
+	wait_queue_head_t dedup_wq;
+	unsigned long long dedup_cp_ver;
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+	unsigned long long dio_cur_seqindex;
+#endif
 };
 
 static inline void get_read_extent_info(struct extent_info *ext,
@@ -976,6 +1070,10 @@ struct dnode_of_data {
 	char cur_level;			/* level of hole node page */
 	char max_level;			/* level of current page located */
 	block_t	data_blkaddr;		/* block address of the node block */
+#ifdef CONFIG_F2FS_SEQZONE
+	block_t seqzone_index;
+	bool use_seqzone;
+#endif
 };
 
 static inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,
@@ -1003,7 +1101,11 @@ static inline void set_new_dnode(struct dnode_of_data *dn, struct inode *inode,
  */
 #define	NR_CURSEG_DATA_TYPE	(3)
 #define NR_CURSEG_NODE_TYPE	(3)
+#ifdef CONFIG_F2FS_SEQZONE
+#define NR_CURSEG_INMEM_TYPE	(18)
+#else
 #define NR_CURSEG_INMEM_TYPE	(2)
+#endif
 #define NR_CURSEG_RO_TYPE	(2)
 #define NR_CURSEG_PERSIST_TYPE	(NR_CURSEG_DATA_TYPE + NR_CURSEG_NODE_TYPE)
 #define NR_CURSEG_TYPE		(NR_CURSEG_INMEM_TYPE + NR_CURSEG_PERSIST_TYPE)
@@ -1019,6 +1121,24 @@ enum {
 	CURSEG_COLD_DATA_PINNED = NR_PERSISTENT_LOG,
 				/* pinned file that needs consecutive block address */
 	CURSEG_ALL_DATA_ATGC,	/* SSR alloctor in hot/warm/cold data area */
+#ifdef CONFIG_F2FS_SEQZONE
+	CURSEG_HOT_DATA_0,
+	CURSEG_HOT_DATA_1,
+	CURSEG_HOT_DATA_2,
+	CURSEG_HOT_DATA_3,
+	CURSEG_HOT_DATA_4,
+	CURSEG_HOT_DATA_5,
+	CURSEG_HOT_DATA_6,
+	CURSEG_HOT_DATA_7,
+	CURSEG_WARM_DATA_0,
+	CURSEG_WARM_DATA_1,
+	CURSEG_WARM_DATA_2,
+	CURSEG_WARM_DATA_3,
+	CURSEG_WARM_DATA_4,
+	CURSEG_WARM_DATA_5,
+	CURSEG_WARM_DATA_6,
+	CURSEG_WARM_DATA_7,
+#endif
 	NO_CHECK_TYPE,		/* number of persistent & inmem log */
 };
 
@@ -1084,8 +1204,7 @@ struct f2fs_sm_info {
  * f2fs monitors the number of several block types such as on-writeback,
  * dirty dentry blocks, dirty node blocks, and dirty meta blocks.
  */
-#define WB_DATA_TYPE(p, f)			\
-	(f || f2fs_is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)
+#define WB_DATA_TYPE(p)	(__is_cp_guaranteed(p) ? F2FS_WB_CP_DATA : F2FS_WB_DATA)
 enum count_type {
 	F2FS_DIRTY_DENTS,
 	F2FS_DIRTY_DATA,
@@ -1150,6 +1269,7 @@ enum cp_reason_type {
 	CP_FASTBOOT_MODE,
 	CP_SPEC_LOG_NUM,
 	CP_RECOVER_DIR,
+	CP_DEDUPED,
 	CP_XATTR_DIR,
 };
 
@@ -1199,6 +1319,11 @@ struct f2fs_io_info {
 	blk_opf_t op_flags;	/* req_flag_bits */
 	block_t new_blkaddr;	/* new block address to be written */
 	block_t old_blkaddr;	/* old block address before Cow */
+#ifdef CONFIG_F2FS_SEQZONE
+	bool use_seqzone;
+	u32 seqzone_index;
+	int real_temp;
+#endif
 	struct page *page;	/* page to be written */
 	struct page *encrypted_page;	/* encrypted page */
 	struct page *compressed_page;	/* compressed page */
@@ -1428,9 +1553,24 @@ enum compress_algorithm_type {
 
 enum compress_flag {
 	COMPRESS_CHKSUM,
+	COMPRESS_LAYOUT,
+	COMPRESS_ATIME = 3,
+	COMPRESS_RESERVED = 4,
+	COMPRESS_LEVEL = 8,
 	COMPRESS_MAX_FLAG,
 };
 
+#define COMPRESS_CHKSUM_MASK	0x1
+#define COMPRESS_LAYOUT_MASK	0x6
+#define COMPRESS_ATIME_MASK	0x8
+#define COMPRESS_LEVEL_MASK	0xf0
+
+enum compress_layout_type {
+	COMPRESS_FIXED_INPUT,
+	COMPRESS_FIXED_OUTPUT,
+	COMPRESS_LAYOUT_MAX,
+};
+
 #define	COMPRESS_WATERMARK			20
 #define	COMPRESS_PERCENT			20
 
@@ -1446,10 +1586,130 @@ struct compress_data {
 
 #define F2FS_COMPRESSED_PAGE_MAGIC	0xF5F2C000
 
-#define F2FS_ZSTD_DEFAULT_CLEVEL	1
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+/*
+ compress metadata layout at inode/dnode
+
+ f2fs_inode.i_addr[] or direct_node.addr[] space management
+
+ original layout
+ +--------------------+
+ |                    |
+ |                    |
+ |     addrs[]        +--> total block address array size, get via addrs_per_inode() / addrs_per_block()
+ |                    |
+ |                    |
+ +--------------------+
+
+ compression fixed-output layout
+ +--------------------+
+ |     addrs[]        |--> block address array size, get via addrs_per_inode() / addrs_per_block()
+ |                    |
+ +--------------------+--> decompress_index array start blkaddr, get via decompress_index_addr()
+ | decompress_index[] |
+ |                    |--> di array size, the size is the same as blkaddr array size
+ +--------------------+
+
+ [compress fixed-output layout]
+
+  |-- page 0--|-- page 1--|-- page 2--|-- page 3--|-- page 4--|
+  |           |          /           /
+  |          /          /          /
+  |         /          /         /
+  |        /         /        /
+  |       /        /       /
+  |      /       /      /
+  |-- blk 0 --|-- blk 1 --|-- blk 2 --|
+  ^      ^       ^      ^
+  |      |       |      |
+ [0]   [ofs1]   [1]   [ofs2]
+
+  page 0
+  first_page = 1
+  cross_block = 0
+  blkidx = 0
+  ofs = 0
+              page 1
+              first_page = 1
+              cross_block = 1
+              blkidx = 1
+              ofs = [ofs1]
+                          page 2
+                          first_page = 0
+                          cross_block = 0
+                          blkidx = 1
+                          ofs = 1 (distance from first page)
+                                      page 3
+                                      first_page = 1
+                                      cross_block = 1
+                                      blkidx = 2
+                                      ofs = [ofs2]
+ */
+struct decompress_index {
+	unsigned int is_valid:1;	/* indicate entry is valid or not */
+	unsigned int is_compress:1;	/* indicate page is compressed */
+	unsigned int first_page:1;	/* first page in compressed block */
+	unsigned int cross_block:1;	/* cross two compressed blocks */
+	/*
+	 * indicate compressed/raw block index that page locates, if it
+	 * is cross block, its blkidx equals to second(last) compressed
+	 * block's index
+	 */
+	unsigned int blkidx:8;
+	/*
+	 * when .first_page is one, @ofs indicates:
+	 * offset in raw page, which is start offset of compressed blocks
+	 * when .first_page is zero, @ofs indicates:
+	 * distance from first page in compressed block
+	 */
+	unsigned int ofs:12;
+	unsigned int padding:8;
+} __packed;
+
+typedef u32 decompress_index_t;
+
+#define DI_VALID_OFFS		0
+#define DI_COMPRESSED_OFFS	1
+#define DI_FIRST_PAGE_OFFS	2
+#define DI_CROSS_BLOCK_OFFS	3
+#define DI_BLKID_OFFS		4
+#define DI_OFS_OFFS		12
+
+static decompress_index_t inline
+serialize_decompress_index(struct decompress_index *di)
+{
+	decompress_index_t val = 0;
+
+	val |= di->is_valid << DI_VALID_OFFS;
+	val |= di->is_compress << DI_COMPRESSED_OFFS;
+	val |= di->first_page << DI_FIRST_PAGE_OFFS;
+	val |= di->cross_block << DI_CROSS_BLOCK_OFFS;
+	val |= di->blkidx << DI_BLKID_OFFS;
+	val |= di->ofs << DI_OFS_OFFS;
 
-#define	COMPRESS_LEVEL_OFFSET	8
+	return val;
+}
+
+static void inline deserialize_decompress_index(decompress_index_t val,
+						struct decompress_index *di)
+{
+	di->is_valid = (val & (1 << DI_VALID_OFFS)) >> DI_VALID_OFFS;
+	di->is_compress = (val & (1 << DI_COMPRESSED_OFFS)) >> DI_COMPRESSED_OFFS;
+	di->first_page = (val & (1 << DI_FIRST_PAGE_OFFS)) >> DI_FIRST_PAGE_OFFS;
+	di->cross_block = (val & (1 << DI_CROSS_BLOCK_OFFS)) >> DI_CROSS_BLOCK_OFFS;
+	di->blkidx = (val & (0xff << DI_BLKID_OFFS)) >> DI_BLKID_OFFS;
+	di->ofs = (val & (0xfff << DI_OFS_OFFS)) >> DI_OFS_OFFS;
+}
+
+#define MAX_BLKS_PER_CLUSTER		(1 << 3) // MAX_COMPRESS_LOG_SIZE
 
+#define CLEAR_IFLAG_IF_SET(inode, flag)                                        \
+	if (F2FS_I(inode)->i_flags & flag) {                                   \
+		F2FS_I(inode)->i_flags &= ~flag;                               \
+		f2fs_mark_inode_dirty_sync(inode, true);                       \
+	}
+#endif
+#define F2FS_ZSTD_DEFAULT_CLEVEL	1
 /* compress context */
 struct compress_ctx {
 	struct inode *inode;		/* inode the context belong to */
@@ -1465,6 +1725,11 @@ struct compress_ctx {
 	struct compress_data *cbuf;	/* virtual mapped address on cpages */
 	size_t rlen;			/* valid data length in rbuf */
 	size_t clen;			/* valid data length in cbuf */
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	struct decompress_index di[MAX_BLKS_PER_CLUSTER];
+	unsigned int rofs;		/* offset in rbuf */
+	unsigned int cofs;		/* offset in cbuf */
+#endif
 	void *private;			/* payload buffer for specified compression algorithm */
 	void *private2;			/* extra payload buffer */
 };
@@ -1485,16 +1750,19 @@ struct decompress_io_ctx {
 	pgoff_t cluster_idx;		/* cluster index number */
 	unsigned int cluster_size;	/* page count in cluster */
 	unsigned int log_cluster_size;	/* log of cluster size */
-	struct page **rpages;		/* pages store raw data in cluster */
+	struct page *rpages[MAX_BLKS_PER_CLUSTER];		/* pages store raw data in cluster */
 	unsigned int nr_rpages;		/* total page number in rpages */
-	struct page **cpages;		/* pages store compressed data in cluster */
+	struct page *cpages[MAX_BLKS_PER_CLUSTER];		/* pages store compressed data in cluster */
 	unsigned int nr_cpages;		/* total page number in cpages */
-	struct page **tpages;		/* temp pages to pad holes in cluster */
+	struct page *tpages[MAX_BLKS_PER_CLUSTER];		/* temp pages to pad holes in cluster */
 	void *rbuf;			/* virtual mapped address on rpages */
 	struct compress_data *cbuf;	/* virtual mapped address on cpages */
 	size_t rlen;			/* valid data length in rbuf */
 	size_t clen;			/* valid data length in cbuf */
-
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	unsigned int rofs;
+	unsigned int cofs;
+#endif
 	/*
 	 * The number of compressed pages remaining to be read in this cluster.
 	 * This is initially nr_cpages.  It is decremented by 1 each time a page
@@ -1526,6 +1794,12 @@ struct decompress_io_ctx {
 	void *private2;			/* extra payload buffer */
 	struct work_struct verity_work;	/* work to verify the decompressed pages */
 	struct work_struct free_work;	/* work for late free this structure itself */
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	int current_blk;
+	struct decompress_index di[MAX_BLKS_PER_CLUSTER];
+	char inplace_io[MAX_BLKS_PER_CLUSTER];
+#endif
 };
 
 #define NULL_CLUSTER			((unsigned int)(~0))
@@ -1794,6 +2068,17 @@ struct f2fs_sb_info {
 	spinlock_t iostat_lat_lock;
 	struct iostat_lat_info *iostat_io_lat;
 #endif
+
+	unsigned int oplus_feats;
+
+#ifdef CONFIG_F2FS_APPBOOST
+	int appboost;
+	unsigned int appboost_max_blocks;
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+	int seq_zone;
+#endif
 };
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
@@ -2978,6 +3263,25 @@ static inline void f2fs_change_bit(unsigned int nr, char *addr)
 #define F2FS_PROJINHERIT_FL		0x20000000 /* Create with parents projid */
 #define F2FS_CASEFOLD_FL		0x40000000 /* Casefolded file */
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+#define F2FS_DEDUPED_FL			0x00000001
+#define F2FS_INNER_FL			0x00000002
+#define F2FS_UPDATED_FL			0x00000004
+#define F2FS_REVOKE_FL			0x00000008
+#define F2FS_DOING_DEDUP_FL		0x00000010
+
+#define F2FS_META_UN_MODIFY_FL		0x00000100
+#define F2FS_DATA_UN_MODIFY_FL		0x00000200
+#ifdef CONFIG_F2FS_APPBOOST
+#define F2FS_MERGED_FILE_FL		0x00000400
+#endif
+#define F2FS_SNAPSHOT_PREPARED_FL	0x00000800
+#define F2FS_SNAPSHOTED_FL		0x00001000
+#ifdef CONFIG_F2FS_SEQZONE
+#define F2FS_SEQZONE_FL			0x00002000 /* Seqzone file */
+#endif
+#endif
+
 /* Flags that should be inherited by new inodes from their parent. */
 #define F2FS_FL_INHERITED (F2FS_SYNC_FL | F2FS_NODUMP_FL | F2FS_NOATIME_FL | \
 			   F2FS_DIRSYNC_FL | F2FS_PROJINHERIT_FL | \
@@ -3014,6 +3318,22 @@ static inline void __mark_inode_dirty_flag(struct inode *inode,
 	case FI_DATA_EXIST:
 	case FI_PIN_FILE:
 	case FI_COMPRESS_RELEASED:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	case FI_DEDUPED:
+	case FI_INNER_INODE:
+	case FI_REVOKE_DEDUP:
+	case FI_DOING_DEDUP:
+	case FI_META_UN_MODIFY:
+	case FI_DATA_UN_MODIFY:
+#ifdef CONFIG_F2FS_APPBOOST
+	case FI_MERGED_FILE:
+#endif
+	case FI_SNAPSHOT_PREPARED:
+	case FI_SNAPSHOTED:
+#ifdef CONFIG_F2FS_SEQZONE
+	case FI_SEQZONE:
+#endif
+#endif
 		f2fs_mark_inode_dirty_sync(inode, true);
 	}
 }
@@ -3179,6 +3499,13 @@ static inline int f2fs_compressed_file(struct inode *inode)
 		is_inode_flag_set(inode, FI_COMPRESSED_FILE);
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+static inline int f2fs_seqzone_file(struct inode *inode)
+{
+	return S_ISREG(inode->i_mode) &&
+		is_inode_flag_set(inode, FI_SEQZONE);
+}
+#endif
 static inline bool f2fs_need_compress_data(struct inode *inode)
 {
 	int compress_mode = F2FS_OPTION(F2FS_I_SB(inode)).compress_mode;
@@ -3195,23 +3522,89 @@ static inline bool f2fs_need_compress_data(struct inode *inode)
 	return false;
 }
 
+static inline int f2fs_compress_layout(struct inode *inode)
+{
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	__u16 flag = F2FS_I(inode)->i_compress_flag;
+
+	f2fs_bug_on(F2FS_I_SB(inode), !S_ISREG(inode->i_mode));
+	return (flag & COMPRESS_LAYOUT_MASK) >> COMPRESS_LAYOUT;
+#else
+	return COMPRESS_FIXED_INPUT;
+#endif
+}
+
 static inline unsigned int addrs_per_inode(struct inode *inode)
 {
 	unsigned int addrs = CUR_ADDRS_PER_INODE(inode) -
 				get_inline_xattr_addrs(inode);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		return addrs / 2;
+#endif
 	if (!f2fs_compressed_file(inode))
 		return addrs;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compress_layout(inode) == COMPRESS_FIXED_INPUT)
+		return ALIGN_DOWN(addrs, F2FS_I(inode)->i_cluster_size);
+	return ALIGN_DOWN(addrs, F2FS_I(inode)->i_cluster_size * 2) / 2;
+#else
 	return ALIGN_DOWN(addrs, F2FS_I(inode)->i_cluster_size);
+#endif
 }
 
 static inline unsigned int addrs_per_block(struct inode *inode)
 {
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		return DEF_ADDRS_PER_BLOCK / 2;
+#endif
 	if (!f2fs_compressed_file(inode))
 		return DEF_ADDRS_PER_BLOCK;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compress_layout(inode) == COMPRESS_FIXED_INPUT)
+		return ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, F2FS_I(inode)->i_cluster_size);
+	return ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, F2FS_I(inode)->i_cluster_size * 2) / 2;
+#else
 	return ALIGN_DOWN(DEF_ADDRS_PER_BLOCK, F2FS_I(inode)->i_cluster_size);
+#endif
+}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+static inline void *decompress_index_addr(struct inode *inode, struct page *page)
+{
+	struct f2fs_node *rn = F2FS_NODE(page);
+	__le32 *addr_array = blkaddr_in_node(rn);
+	int addrs = ADDRS_PER_PAGE(page, inode);
+	int base = 0;
+
+	if (IS_INODE(page) && f2fs_has_extra_attr(inode))
+		base = get_extra_isize(inode);
+
+	return addr_array + base + addrs;
+}
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+static inline int __set_seqzone_index(struct dnode_of_data *dn)
+{
+	struct f2fs_node *rn = F2FS_NODE(dn->node_page);
+	__le32 *addr_array;
+	int base = 0;
+	int addrs = ADDRS_PER_PAGE(dn->node_page, dn->inode);
+
+	if (IS_INODE(dn->node_page) && f2fs_has_extra_attr(dn->inode))
+		base = get_extra_isize(dn->inode);
+
+	addr_array = blkaddr_in_node(rn);
+	if (f2fs_seqzone_file(dn->inode))
+		addr_array[base + dn->ofs_in_node + addrs] = cpu_to_le32(dn->seqzone_index);
+
+	return 0;
 }
+#endif
 
+extern void __set_data_blkaddr(struct dnode_of_data *dn, block_t blkaddr);
 static inline void *inline_xattr_addr(struct inode *inode, struct page *page)
 {
 	struct f2fs_inode *ri = F2FS_INODE(page);
@@ -3275,6 +3668,155 @@ static inline int f2fs_has_inline_dentry(struct inode *inode)
 	return is_inode_flag_set(inode, FI_INLINE_DENTRY);
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+static inline void get_dedup_flags_info(struct inode *inode, struct f2fs_inode *ri)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	u32 i_dedup_flags = le32_to_cpu(ri->i_dedup_flags);
+	if (i_dedup_flags & F2FS_DEDUPED_FL)
+		set_bit(FI_DEDUPED, fi->flags);
+	if (i_dedup_flags & F2FS_INNER_FL)
+		set_bit(FI_INNER_INODE, fi->flags);
+	if (i_dedup_flags & F2FS_UPDATED_FL)
+		set_bit(FI_UPDATED, fi->flags);
+	if (i_dedup_flags & F2FS_REVOKE_FL)
+		set_bit(FI_REVOKE_DEDUP, fi->flags);
+	if (i_dedup_flags & F2FS_DOING_DEDUP_FL)
+		set_bit(FI_DOING_DEDUP, fi->flags);
+	if (i_dedup_flags & F2FS_META_UN_MODIFY_FL)
+		set_bit(FI_META_UN_MODIFY, fi->flags);
+	if (i_dedup_flags & F2FS_DATA_UN_MODIFY_FL)
+		set_bit(FI_DATA_UN_MODIFY, fi->flags);
+#ifdef CONFIG_F2FS_APPBOOST
+	if (i_dedup_flags & F2FS_MERGED_FILE_FL)
+		set_bit(FI_MERGED_FILE, fi->flags);
+#endif
+	if (i_dedup_flags & F2FS_SNAPSHOT_PREPARED_FL)
+		set_bit(FI_SNAPSHOT_PREPARED, fi->flags);
+	if (i_dedup_flags & F2FS_SNAPSHOTED_FL)
+		set_bit(FI_SNAPSHOTED, fi->flags);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (i_dedup_flags & F2FS_SEQZONE_FL)
+		set_bit(FI_SEQZONE, fi->flags);
+#endif
+}
+
+static inline void set_raw_dedup_flags(struct inode *inode, struct f2fs_inode *ri)
+{
+	u32 i_dedup_flags = 0;
+
+	if (is_inode_flag_set(inode, FI_DEDUPED))
+		i_dedup_flags |= F2FS_DEDUPED_FL;
+	if (is_inode_flag_set(inode, FI_INNER_INODE))
+		i_dedup_flags |= F2FS_INNER_FL;
+	if (is_inode_flag_set(inode, FI_UPDATED))
+		i_dedup_flags |= F2FS_UPDATED_FL;
+	if (is_inode_flag_set(inode, FI_REVOKE_DEDUP))
+		i_dedup_flags |= F2FS_REVOKE_FL;
+	if (is_inode_flag_set(inode, FI_DOING_DEDUP))
+		i_dedup_flags |= F2FS_DOING_DEDUP_FL;
+	if (is_inode_flag_set(inode, FI_META_UN_MODIFY))
+		i_dedup_flags |= F2FS_META_UN_MODIFY_FL;
+	if (is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+		i_dedup_flags |= F2FS_DATA_UN_MODIFY_FL;
+#ifdef CONFIG_F2FS_APPBOOST
+	if (is_inode_flag_set(inode, FI_MERGED_FILE))
+		i_dedup_flags |= F2FS_MERGED_FILE_FL;
+#endif
+	if (is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		i_dedup_flags |= F2FS_SNAPSHOT_PREPARED_FL;
+	if (is_inode_flag_set(inode, FI_SNAPSHOTED))
+		i_dedup_flags |= F2FS_SNAPSHOTED_FL;
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (is_inode_flag_set(inode, FI_SEQZONE))
+		i_dedup_flags |= F2FS_SEQZONE_FL;
+#endif
+	ri->i_dedup_flags = cpu_to_le32(i_dedup_flags);
+}
+
+static inline int f2fs_is_deduped_inode(struct inode *inode)
+{
+	return is_inode_flag_set(inode, FI_DEDUPED);
+}
+
+static inline int f2fs_is_inner_inode(struct inode *inode)
+{
+	if (!f2fs_is_deduped_inode(inode))
+		return false;
+	return is_inode_flag_set(inode, FI_INNER_INODE);
+}
+
+static inline int f2fs_is_outer_inode(struct inode *inode)
+{
+	if (!f2fs_is_deduped_inode(inode))
+		return false;
+	return !f2fs_is_inner_inode(inode);
+}
+
+static inline struct inode *get_inner_inode(struct inode *inode)
+{
+	struct inode *inner = NULL;
+
+	f2fs_down_read(&F2FS_I(inode)->i_sem);
+	if (f2fs_is_outer_inode(inode))
+		inner = igrab(F2FS_I(inode)->inner_inode);
+	f2fs_up_read(&F2FS_I(inode)->i_sem);
+	return inner;
+}
+
+static inline void put_inner_inode(struct inode *inode)
+{
+	if (inode) {
+		f2fs_bug_on(F2FS_I_SB(inode), !f2fs_is_inner_inode(inode));
+		iput(inode);
+	}
+}
+
+static inline void mark_file_modified(struct inode *inode)
+{
+	if (!is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+		return;
+	clear_bit(FI_DATA_UN_MODIFY, F2FS_I(inode)->flags);
+	f2fs_mark_inode_dirty_sync(inode, true);
+}
+
+static inline void inode_dec_read_io(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+
+	if (atomic_read(&fi->inflight_read_io) == 0) {
+		f2fs_err(F2FS_I_SB(inode),
+			"F2FS-fs: inode [%lu] inflight_read_io refcount may leak",
+			inode->i_ino);
+		wake_up_all(&fi->dedup_wq);
+		return;
+	}
+
+	if (atomic_dec_and_test(&fi->inflight_read_io) &&
+			wq_has_sleeper(&fi->dedup_wq))
+		wake_up_all(&fi->dedup_wq);
+}
+
+static inline void inode_inc_read_io(struct inode *inode)
+{
+	atomic_inc(&F2FS_I(inode)->inflight_read_io);
+}
+
+#ifdef CONFIG_F2FS_CHECK_FS
+#define f2fs_dedup_info(sbi, fmt, ...) \
+	f2fs_printk(sbi, KERN_INFO fmt, ##__VA_ARGS__)
+#else
+#define f2fs_dedup_info(sbi, fmt, ...) do {} while(0)
+#endif
+
+#else
+static inline int f2fs_is_deduped_inode(struct inode *inode)
+{
+	return 0;
+}
+#endif
+
 static inline int is_file(struct inode *inode, int type)
 {
 	return F2FS_I(inode)->i_advise & type;
@@ -3414,7 +3956,11 @@ static inline __le32 *get_dnode_addr(struct inode *inode,
 
 #define F2FS_TOTAL_EXTRA_ATTR_SIZE			\
 	(offsetof(struct f2fs_inode, i_extra_end) -	\
-	offsetof(struct f2fs_inode, i_extra_isize))	\
+	offsetof(struct f2fs_inode, i_extra_isize))
+#define F2FS_LEGACY_EXTRA_ATTR_SIZE			\
+	(offsetof(struct f2fs_inode, i_compress_flag) -	\
+	offsetof(struct f2fs_inode, i_extra_isize) +	\
+	sizeof(__le16))
 
 #define F2FS_OLD_ATTRIBUTE_SIZE	(offsetof(struct f2fs_inode, i_addr))
 #define F2FS_FITS_IN_INODE(f2fs_inode, extra_isize, field)		\
@@ -3431,15 +3977,17 @@ bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 static inline void verify_blkaddr(struct f2fs_sb_info *sbi,
 					block_t blkaddr, int type)
 {
-	if (!f2fs_is_valid_blkaddr(sbi, blkaddr, type))
+	if (!f2fs_is_valid_blkaddr(sbi, blkaddr, type)) {
 		f2fs_err(sbi, "invalid blkaddr: %u, type: %d, run fsck to fix.",
 			 blkaddr, type);
+		f2fs_bug_on(sbi, 1);
+	}
 }
 
 static inline bool __is_valid_data_blkaddr(block_t blkaddr)
 {
 	if (blkaddr == NEW_ADDR || blkaddr == NULL_ADDR ||
-			blkaddr == COMPRESS_ADDR)
+			blkaddr == COMPRESS_ADDR || blkaddr == DEDUP_ADDR)
 		return false;
 	return true;
 }
@@ -3466,6 +4014,21 @@ long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
 long f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
 int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid);
 int f2fs_pin_file_control(struct inode *inode, bool inc);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+int f2fs_reserve_compress_blocks(struct inode *inode, unsigned int *ret_rsvd_blks);
+int f2fs_decompress_inode(struct inode *inode);
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+int f2fs_filemap_write_and_wait_range(struct inode *inode);
+int f2fs_revoke_deduped_inode(struct inode *dedup, const char *revoke_source);
+void f2fs_drop_deduped_link(struct inode *inode);
+bool f2fs_inode_support_dedup(struct f2fs_sb_info *sbi,
+			struct inode *inode);
+int f2fs_set_inode_addr(struct inode* inode, block_t addr);
+int create_page_info_slab(void);
+void destroy_page_info_slab(void);
+#endif
 
 /*
  * inode.c
@@ -3473,6 +4036,8 @@ int f2fs_pin_file_control(struct inode *inode, bool inc);
 void f2fs_set_inode_flags(struct inode *inode);
 bool f2fs_inode_chksum_verify(struct f2fs_sb_info *sbi, struct page *page);
 void f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page);
+int f2fs_inode_chksum_get(struct f2fs_sb_info *sbi, struct inode *inode,
+			  u32 *chksum);
 struct inode *f2fs_iget(struct super_block *sb, unsigned long ino);
 struct inode *f2fs_iget_retry(struct super_block *sb, unsigned long ino);
 int f2fs_try_to_free_nats(struct f2fs_sb_info *sbi, int nr_shrink);
@@ -3490,6 +4055,7 @@ int f2fs_update_extension_list(struct f2fs_sb_info *sbi, const char *name,
 struct dentry *f2fs_get_parent(struct dentry *child);
 int f2fs_get_tmpfile(struct user_namespace *mnt_userns, struct inode *dir,
 		     struct inode **new_inode);
+void f2fs_update_atime(struct inode *inode, bool oneshot);
 
 /*
  * dir.c
@@ -3635,8 +4201,7 @@ int f2fs_issue_flush(struct f2fs_sb_info *sbi, nid_t ino);
 int f2fs_create_flush_cmd_control(struct f2fs_sb_info *sbi);
 int f2fs_flush_device_cache(struct f2fs_sb_info *sbi);
 void f2fs_destroy_flush_cmd_control(struct f2fs_sb_info *sbi, bool free);
-void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr,
-						unsigned int len);
+void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr);
 bool f2fs_is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);
 int f2fs_start_discard_thread(struct f2fs_sb_info *sbi);
 void f2fs_drop_discard_cmd(struct f2fs_sb_info *sbi);
@@ -3682,7 +4247,7 @@ void f2fs_replace_block(struct f2fs_sb_info *sbi, struct dnode_of_data *dn,
 int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 			block_t old_blkaddr, block_t *new_blkaddr,
 			struct f2fs_summary *sum, int type,
-			struct f2fs_io_info *fio);
+			struct f2fs_io_info *fio, bool use_seqzone);
 void f2fs_update_device_state(struct f2fs_sb_info *sbi, nid_t ino,
 					block_t blkaddr, unsigned int blkcnt);
 void f2fs_wait_on_page_writeback(struct page *page,
@@ -3729,8 +4294,6 @@ struct page *f2fs_get_meta_page_retry(struct f2fs_sb_info *sbi, pgoff_t index);
 struct page *f2fs_get_tmp_page(struct f2fs_sb_info *sbi, pgoff_t index);
 bool f2fs_is_valid_blkaddr(struct f2fs_sb_info *sbi,
 					block_t blkaddr, int type);
-bool f2fs_is_valid_blkaddr_raw(struct f2fs_sb_info *sbi,
-					block_t blkaddr, int type);
 int f2fs_ra_meta_pages(struct f2fs_sb_info *sbi, block_t start, int nrpages,
 			int type, bool sync);
 void f2fs_ra_meta_pages_cond(struct f2fs_sb_info *sbi, pgoff_t index,
@@ -3745,6 +4308,9 @@ void f2fs_set_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,
 					unsigned int devidx, int type);
 bool f2fs_is_dirty_device(struct f2fs_sb_info *sbi, nid_t ino,
 					unsigned int devidx, int type);
+#ifdef CONFIG_F2FS_FS_DEDUP
+int f2fs_truncate_dedup_inode(struct inode *inode, unsigned int flag);
+#endif
 int f2fs_acquire_orphan_inode(struct f2fs_sb_info *sbi);
 void f2fs_release_orphan_inode(struct f2fs_sb_info *sbi);
 void f2fs_add_orphan_inode(struct inode *inode);
@@ -3771,7 +4337,6 @@ void f2fs_init_ckpt_req_control(struct f2fs_sb_info *sbi);
  */
 int __init f2fs_init_bioset(void);
 void f2fs_destroy_bioset(void);
-bool f2fs_is_cp_guaranteed(struct page *page);
 int f2fs_init_bio_entry_cache(void);
 void f2fs_destroy_bio_entry_cache(void);
 void f2fs_submit_read_bio(struct f2fs_sb_info *sbi, struct bio *bio,
@@ -3826,6 +4391,13 @@ void f2fs_destroy_post_read_processing(void);
 int f2fs_init_post_read_wq(struct f2fs_sb_info *sbi);
 void f2fs_destroy_post_read_wq(struct f2fs_sb_info *sbi);
 extern const struct iomap_ops f2fs_iomap_ops;
+#ifdef CONFIG_F2FS_SEQZONE
+void f2fs_seqzone_fio_check(struct f2fs_io_info *fio);
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+extern const char *seqzone_switch;
+#endif
 
 /*
  * gc.c
@@ -3850,7 +4422,9 @@ int f2fs_recover_fsync_data(struct f2fs_sb_info *sbi, bool check_only);
 bool f2fs_space_for_roll_forward(struct f2fs_sb_info *sbi);
 int __init f2fs_create_recovery_cache(void);
 void f2fs_destroy_recovery_cache(void);
-
+#ifdef CONFIG_F2FS_APPBOOST
+void f2fs_boostfile_free(struct inode *inode);
+#endif
 /*
  * debug.c
  */
@@ -4185,6 +4759,7 @@ unsigned int f2fs_shrink_age_extent_tree(struct f2fs_sb_info *sbi,
  */
 #define MIN_RA_MUL	2
 #define MAX_RA_MUL	256
+#define COMPR_RA_MUL	32
 
 int __init f2fs_init_sysfs(void);
 void f2fs_exit_sysfs(void);
@@ -4288,8 +4863,7 @@ void f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi);
 int __init f2fs_init_compress_cache(void);
 void f2fs_destroy_compress_cache(void);
 struct address_space *COMPRESS_MAPPING(struct f2fs_sb_info *sbi);
-void f2fs_invalidate_compress_pages_range(struct f2fs_sb_info *sbi,
-					block_t blkaddr, unsigned int len);
+void f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi, block_t blkaddr);
 void f2fs_cache_compressed_page(struct f2fs_sb_info *sbi, struct page *page,
 						nid_t ino, block_t blkaddr);
 bool f2fs_load_compressed_page(struct f2fs_sb_info *sbi, struct page *page,
@@ -4344,8 +4918,8 @@ static inline int f2fs_init_page_array_cache(struct f2fs_sb_info *sbi) { return
 static inline void f2fs_destroy_page_array_cache(struct f2fs_sb_info *sbi) { }
 static inline int __init f2fs_init_compress_cache(void) { return 0; }
 static inline void f2fs_destroy_compress_cache(void) { }
-static inline void f2fs_invalidate_compress_pages_range(struct f2fs_sb_info *sbi,
-				block_t blkaddr, unsigned int len) { }
+static inline void f2fs_invalidate_compress_page(struct f2fs_sb_info *sbi,
+				block_t blkaddr) { }
 static inline void f2fs_cache_compressed_page(struct f2fs_sb_info *sbi,
 				struct page *page, nid_t ino, block_t blkaddr) { }
 static inline bool f2fs_load_compressed_page(struct f2fs_sb_info *sbi,
@@ -4365,11 +4939,19 @@ static inline void f2fs_update_read_extent_tree_range_compressed(
 				unsigned int llen, unsigned int c_len) { }
 #endif
 
+extern bool may_compress;
+extern bool may_set_compr_fl;
+
 static inline int set_compress_context(struct inode *inode)
 {
 #ifdef CONFIG_F2FS_FS_COMPRESSION
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		return -EBUSY;
+#endif
+	if (!may_compress)
+		return -EOPNOTSUPP;
 	F2FS_I(inode)->i_compress_algorithm =
 			F2FS_OPTION(sbi).compress_algorithm;
 	F2FS_I(inode)->i_log_cluster_size =
@@ -4377,6 +4959,11 @@ static inline int set_compress_context(struct inode *inode)
 	F2FS_I(inode)->i_compress_flag =
 			F2FS_OPTION(sbi).compress_chksum ?
 				BIT(COMPRESS_CHKSUM) : 0;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (F2FS_OPTION(sbi).compress_layout == COMPRESS_FIXED_OUTPUT)
+		F2FS_I(inode)->i_compress_flag |= COMPRESS_FIXED_OUTPUT <<
+				COMPRESS_LAYOUT;
+#endif
 	F2FS_I(inode)->i_cluster_size =
 			BIT(F2FS_I(inode)->i_log_cluster_size);
 	if ((F2FS_I(inode)->i_compress_algorithm == COMPRESS_LZ4 ||
@@ -4438,8 +5025,26 @@ F2FS_FEATURE_FUNCS(lost_found, LOST_FOUND);
 F2FS_FEATURE_FUNCS(verity, VERITY);
 F2FS_FEATURE_FUNCS(sb_chksum, SB_CHKSUM);
 F2FS_FEATURE_FUNCS(casefold, CASEFOLD);
+#ifndef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
 F2FS_FEATURE_FUNCS(compression, COMPRESSION);
+#else
+static inline bool f2fs_sb_has_compression(struct f2fs_sb_info *sbi)
+{
+	return F2FS_HAS_FEATURE(sbi, F2FS_FEATURE_COMPRESSION) &&
+		(sbi->oplus_feats & OPLUS_FEAT_COMPR);
+}
+#endif
 F2FS_FEATURE_FUNCS(readonly, RO);
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_FEATURE_FUNCS(seqzone, SEQZONE);
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+static inline bool f2fs_sb_has_dedup(struct f2fs_sb_info *sbi)
+{
+	return F2FS_HAS_FEATURE(sbi, F2FS_FEATURE_DEDUP) &&
+		(sbi->oplus_feats & OPLUS_FEAT_DEDUP);
+}
+#endif
 
 #ifdef CONFIG_BLK_DEV_ZONED
 static inline bool f2fs_blkz_is_seq(struct f2fs_sb_info *sbi, int devi,
@@ -4555,7 +5160,7 @@ static inline bool f2fs_need_verity(const struct inode *inode, pgoff_t idx)
 
 #ifdef CONFIG_F2FS_FAULT_INJECTION
 extern int f2fs_build_fault_attr(struct f2fs_sb_info *sbi, unsigned long rate,
-							unsigned long type);
+							unsigned long long type);
 #else
 static inline int f2fs_build_fault_attr(struct f2fs_sb_info *sbi,
 					unsigned long rate, unsigned long type)
@@ -4611,34 +5216,17 @@ static inline bool f2fs_is_readonly(struct f2fs_sb_info *sbi)
 static inline void f2fs_truncate_meta_inode_pages(struct f2fs_sb_info *sbi,
 					block_t blkaddr, unsigned int cnt)
 {
-	bool need_submit = false;
-	int i = 0;
-
-	do {
-		struct page *page;
-
-		page = find_get_page(META_MAPPING(sbi), blkaddr + i);
-		if (page) {
-			if (PageWriteback(page))
-				need_submit = true;
-			f2fs_put_page(page, 0);
-		}
-	} while (++i < cnt && !need_submit);
-
-	if (need_submit)
-		f2fs_submit_merged_write_cond(sbi, sbi->meta_inode,
-							NULL, 0, DATA);
-
+	f2fs_submit_merged_write_cond(sbi, sbi->meta_inode, NULL, 0, DATA);
 	truncate_inode_pages_range(META_MAPPING(sbi),
 			F2FS_BLK_TO_BYTES((loff_t)blkaddr),
 			F2FS_BLK_END_BYTES((loff_t)(blkaddr + cnt - 1)));
 }
 
 static inline void f2fs_invalidate_internal_cache(struct f2fs_sb_info *sbi,
-						block_t blkaddr, unsigned int len)
+								block_t blkaddr)
 {
-	f2fs_truncate_meta_inode_pages(sbi, blkaddr, len);
-	f2fs_invalidate_compress_pages_range(sbi, blkaddr, len);
+	f2fs_truncate_meta_inode_pages(sbi, blkaddr, 1);
+	f2fs_invalidate_compress_page(sbi, blkaddr);
 }
 
 #define EFSBADCRC	EBADMSG		/* Bad CRC detected */
diff --git a/fs/f2fs/f2fs_lz4.h b/fs/f2fs/f2fs_lz4.h
new file mode 100644
index 000000000..5a7ccb110
--- /dev/null
+++ b/fs/f2fs/f2fs_lz4.h
@@ -0,0 +1,381 @@
+#ifndef _F2FS_LZ4_H_
+#define _F2FS_LZ4_H_
+#include <asm/unaligned.h>
+
+#define FORCE_INLINE __always_inline
+
+/*-************************************
+ *	Basic Types
+ **************************************/
+#include <linux/types.h>
+
+typedef uint8_t BYTE;
+typedef uint16_t U16;
+typedef uint32_t U32;
+typedef int32_t S32;
+typedef uint64_t U64;
+typedef uintptr_t uptrval;
+
+/*-************************************
+ *	Architecture specifics
+ **************************************/
+#if defined(CONFIG_64BIT)
+#define LZ4_ARCH64 1
+#else
+#define LZ4_ARCH64 0
+#endif
+
+#if defined(__LITTLE_ENDIAN)
+#define LZ4_LITTLE_ENDIAN 1
+#else
+#define LZ4_LITTLE_ENDIAN 0
+#endif
+
+/*-************************************
+ *	Constants
+ **************************************/
+#define MINMATCH 4
+
+#define WILDCOPYLENGTH 8
+#define LASTLITERALS 5
+#define MFLIMIT (WILDCOPYLENGTH + MINMATCH)
+
+/*
+ * ensure it's possible to write 2 x wildcopyLength
+ * without overflowing output buffer
+ */
+#define MATCH_SAFEGUARD_DISTANCE  ((2 * WILDCOPYLENGTH) - MINMATCH)
+
+#define HASH_UNIT sizeof(size_t)
+
+#define ML_BITS	4
+#define ML_MASK	((1U << ML_BITS) - 1)
+#define RUN_BITS (8 - ML_BITS)
+#define RUN_MASK ((1U << RUN_BITS) - 1)
+
+/*-************************************
+ *	Reading and writing into memory
+ **************************************/
+static FORCE_INLINE U16 LZ4_readLE16(const void *memPtr)
+{
+	return get_unaligned_le16(memPtr);
+}
+
+static FORCE_INLINE void LZ4_copy8(void *dst, const void *src)
+{
+#if LZ4_ARCH64
+	U64 a = get_unaligned((const U64 *)src);
+
+	put_unaligned(a, (U64 *)dst);
+#else
+	U32 a = get_unaligned((const U32 *)src);
+	U32 b = get_unaligned((const U32 *)src + 1);
+
+	put_unaligned(a, (U32 *)dst);
+	put_unaligned(b, (U32 *)dst + 1);
+#endif
+}
+
+/*
+ * customized variant of memcpy,
+ * which can overwrite up to 7 bytes beyond dstEnd
+ */
+static FORCE_INLINE void LZ4_wildCopy(void *dstPtr,
+	const void *srcPtr, void *dstEnd)
+{
+	BYTE *d = (BYTE *)dstPtr;
+	const BYTE *s = (const BYTE *)srcPtr;
+	BYTE *const e = (BYTE *)dstEnd;
+
+	do {
+		LZ4_copy8(d, s);
+		d += 8;
+		s += 8;
+	} while (d < e);
+}
+
+#define DEBUGLOG(l, ...) {}	/* disabled */
+#define LZ4_STATIC_ASSERT(c)    BUILD_BUG_ON(!(c))
+
+/*
+ * no public solution to solve our requirement yet.
+ * see: <required buffer size for LZ4_decompress_safe_partial>
+ *      https://groups.google.com/forum/#!topic/lz4c/_3kkz5N6n00
+ */
+static FORCE_INLINE int __lz4_decompress_safe_partial(
+	uint8_t         *dst_ptr,
+	const uint8_t   *src_ptr,
+	BYTE            *dst,
+	int             outputSize,
+	const void      *src,
+	int             inputSize,
+	bool            trusted)
+{
+	/* Local Variables */
+	const BYTE *ip = (const BYTE *) src_ptr;
+	const BYTE *const iend = src + inputSize;
+
+	BYTE *op = (BYTE *) dst_ptr;
+	BYTE *const oend = dst + outputSize;
+	BYTE *cpy;
+
+	static const unsigned int inc32table[] = { 0, 1, 2, 1, 0, 4, 4, 4 };
+	static const int dec64table[] = { 0, 0, 0, -1, -4, 1, 2, 3 };
+
+	/* Set up the "end" pointers for the shortcut. */
+	const BYTE *const shortiend = iend - 14 /*maxLL*/ - 2 /*offset*/;
+	const BYTE *const shortoend = oend - 14 /*maxLL*/ - 18 /*maxML*/;
+
+	DEBUGLOG(5, "%s (srcSize:%i, dstSize:%i)", __func__,
+		 inputSize, outputSize);
+
+	/* Empty output buffer */
+	if (unlikely(!outputSize))
+		return ((inputSize == 1) && (*ip == 0)) ? 0 : -EINVAL;
+
+	if (unlikely(!inputSize))
+		return -EINVAL;
+
+	/* Main Loop : decode sequences */
+	while (1) {
+		size_t length;
+		const BYTE *match;
+		size_t offset;
+
+		/* get literal length */
+		unsigned int const token = *ip++;
+		length = token >> ML_BITS;
+
+		/* ip < iend before the increment */
+		if (ip > iend) {
+			pr_err("ip[0x%px] > iend[0x%px] op[0x%px] > oend[0x%px] src[0x%px] dst[0x%px] src_ptr[0x%px] dst_ptr[0x%px] (srcSize:%i, dstSize:%i) length:%zd",
+				ip, iend, op, oend, src, dst, src_ptr, dst_ptr, inputSize, outputSize, length);
+			return -ENOMEM;
+		}
+
+		/*
+		 * A two-stage shortcut for the most common case:
+		 * 1) If the literal length is 0..14, and there is enough
+		 * space, enter the shortcut and copy 16 bytes on behalf
+		 * of the literals (in the fast mode, only 8 bytes can be
+		 * safely copied this way).
+		 * 2) Further if the match length is 4..18, copy 18 bytes
+		 * in a similar manner; but we ensure that there's enough
+		 * space in the output for those 18 bytes earlier, upon
+		 * entering the shortcut (in other words, there is a
+		 * combined check for both stages).
+		 */
+		if (length != RUN_MASK &&
+		    /*
+		     * strictly "less than" on input, to re-enter
+		     * the loop with at least one byte
+		     */
+		    likely((ip < shortiend) & (op <= shortoend))) {
+
+			/* Copy the literals */
+			memcpy(op, ip, 16);
+			op += length;
+			ip += length;
+
+			/*
+			 * The second stage:
+			 * prepare for match copying, decode full info.
+			 * If it doesn't work out, the info won't be wasted.
+			 */
+			length = token & ML_MASK; /* match length */
+			offset = LZ4_readLE16(ip);
+			ip += 2;
+			match = op - offset;
+			if (match > op) { /* check overflow */
+				goto _output_error;
+			}
+
+			/* Do not deal with overlapping matches. */
+			if ((length != ML_MASK) &&
+			    (offset >= 8) && (match >= dst)) {
+				/* Copy the match. */
+				LZ4_copy8(op + 0, match + 0);
+				LZ4_copy8(op + 8, match + 8);
+				memcpy(op + 16, match + 16, 2);
+				op += length + MINMATCH;
+				/* Both stages worked, load the next token. */
+				continue;
+			}
+
+			/*
+			 * The second stage didn't work out, but the info
+			 * is ready. Propel it right to the point of match
+			 * copying.
+			 */
+			goto _copy_match;
+		}
+
+		/* decode literal length */
+		if (length == RUN_MASK) {
+			unsigned int s;
+
+			if (unlikely(!trusted && ip >= iend - RUN_MASK)) {
+				/* overflow detection */
+				goto _output_error;
+			}
+
+			do {
+				s = *ip++;
+				length += s;
+			} while (likely(ip < iend - RUN_MASK) & (s == 255));
+
+			if (!trusted) {
+				if (unlikely((uptrval)(op) + length < (uptrval)op))
+					/* overflow detection */
+					goto _output_error;
+				if (unlikely((uptrval)(ip) + length < (uptrval)ip))
+					/* overflow detection */
+					goto _output_error;
+			}
+		}
+
+		/* copy literals */
+		cpy = op + length;
+		LZ4_STATIC_ASSERT(MFLIMIT >= WILDCOPYLENGTH);
+
+		if ((cpy > oend - MFLIMIT)
+			|| (ip + length > iend - (2 + 1 + LASTLITERALS))) {
+			if (cpy > oend) {
+				/*
+				 * Partial decoding :
+				 * stop in the middle of literal segment
+				 */
+				cpy = oend;
+				length = oend - op;
+			}
+
+			if (!trusted && ip + length > iend) {
+				/*
+				 * Error :
+				 * read attempt beyond
+				 * end of input buffer
+				 */
+				goto _output_error;
+			}
+
+			memcpy(op, ip, length);
+			ip += length;
+			op += length;
+
+			/* Necessarily EOF, due to parsing restrictions */
+			if (cpy == oend)
+				break;
+		} else {
+			/* may overwrite up to WILDCOPYLENGTH beyond cpy */
+			LZ4_wildCopy(op, ip, cpy);
+			ip += length;
+			op = cpy;
+		}
+
+		/* get offset */
+		offset = LZ4_readLE16(ip);
+		ip += 2;
+		match = op - offset;
+
+		/* get matchlength */
+		length = token & ML_MASK;
+
+_copy_match:
+		if (length == ML_MASK) {
+			unsigned int s;
+
+			do {
+				s = *ip++;
+
+				if (!trusted && ip > iend - LASTLITERALS)
+					goto _output_error;
+
+				length += s;
+			} while (s == 255);
+
+			if (unlikely(!trusted &&
+				     (uptrval)(op) + length < (uptrval)op)) {
+				/* overflow detection */
+				goto _output_error;
+			}
+		}
+
+		length += MINMATCH;
+
+		/* copy match within block */
+		cpy = op + length;
+
+		/*
+		 * partialDecoding :
+		 * may not respect endBlock parsing restrictions
+		 */
+		if (op > oend) {
+			goto _output_error;
+		}
+		if (cpy > oend - MATCH_SAFEGUARD_DISTANCE) {
+			size_t const mlen = min(length, (size_t)(oend - op));
+			const BYTE * const matchEnd = match + mlen;
+			BYTE * const copyEnd = op + mlen;
+
+			if (matchEnd > op) {
+				/* overlap copy */
+				while (op < copyEnd)
+					*op++ = *match++;
+			} else {
+				memcpy(op, match, mlen);
+			}
+			op = copyEnd;
+			if (op == oend)
+				break;
+			continue;
+		}
+
+		if (unlikely(offset < 8)) {
+			op[0] = match[0];
+			op[1] = match[1];
+			op[2] = match[2];
+			op[3] = match[3];
+			match += inc32table[offset];
+			memcpy(op + 4, match, 4);
+			match -= dec64table[offset];
+		} else {
+			LZ4_copy8(op, match);
+			match += 8;
+		}
+
+		op += 8;
+		if (unlikely(cpy > oend - MATCH_SAFEGUARD_DISTANCE)) {
+			BYTE * const oCopyLimit = oend - (WILDCOPYLENGTH - 1);
+
+			if (!trusted && cpy > oend - LASTLITERALS) {
+				/*
+				 * Error : last LASTLITERALS bytes
+				 * must be literals (uncompressed)
+				 */
+				goto _output_error;
+			}
+
+			if (op < oCopyLimit) {
+				LZ4_wildCopy(op, match, oCopyLimit);
+				match += oCopyLimit - op;
+				op = oCopyLimit;
+			}
+			while (op < cpy)
+				*op++ = *match++;
+		} else {
+			LZ4_copy8(op, match);
+			if (length > 16)
+				LZ4_wildCopy(op + 8, match + 8, cpy);
+		}
+		op = cpy; /* wildcopy correction */
+	}
+
+	/* end of decoding */
+	/* Nb of output bytes decoded */
+	return (int) (((BYTE *)op) - dst);
+
+	/* Overflow error detected */
+_output_error:
+	return -ERANGE;
+}
+#endif
diff --git a/fs/f2fs/file.c b/fs/f2fs/file.c
old mode 100644
new mode 100755
index 062e5e5cc..58c547704
--- a/fs/f2fs/file.c
+++ b/fs/f2fs/file.c
@@ -25,6 +25,7 @@
 #include <linux/fileattr.h>
 #include <linux/fadvise.h>
 #include <linux/iomap.h>
+#include <linux/random.h>
 
 #include "f2fs.h"
 #include "node.h"
@@ -36,1609 +37,3407 @@
 #include <trace/events/f2fs.h>
 #include <uapi/linux/f2fs.h>
 
-static vm_fault_t f2fs_filemap_fault(struct vm_fault *vmf)
-{
-	struct inode *inode = file_inode(vmf->vma->vm_file);
-	vm_fault_t ret;
+#ifdef CONFIG_F2FS_APPBOOST
+#include <linux/version.h>
+#include <linux/delay.h>
+#include "../crypto/fscrypt_private.h"
+#endif
 
-	ret = filemap_fault(vmf);
-	if (ret & VM_FAULT_LOCKED)
-		f2fs_update_iostat(F2FS_I_SB(inode), inode,
-					APP_MAPPED_READ_IO, F2FS_BLKSIZE);
 
-	trace_f2fs_filemap_fault(inode, vmf->pgoff, (unsigned long)ret);
+#ifdef CONFIG_F2FS_FS_DEDUP
+#define DEDUP_COMPARE_PAGES	10
+
+#define DEDUP_META_UN_MODIFY_FL		0x1
+#define DEDUP_DATA_UN_MODIFY_FL		0x2
+#define DEDUP_SET_MODIFY_CHECK		0x4
+#define DEDUP_GET_MODIFY_CHECK		0x8
+#define DEDUP_CLEAR_MODIFY_CHECK	0x10
+#define DEDUP_CLONE_META		0x20
+#define DEDUP_CLONE_DATA		0x40
+#define DEDUP_SYNC_DATA			0x80
+#define DEDUP_FOR_SNAPSHOT		0x100
+#define DEDUP_LOOP_MOD			10000
+#define DEDUP_MIN_SIZE			65536
+#define OUTER_INODE			1
+#define INNER_INODE			2
+#define NORMAL_INODE			3
+
+struct page_list {
+	struct list_head list;
+	struct page *page;
+};
+static struct kmem_cache *page_info_slab;
+
+bool may_compress = false;
+bool may_set_compr_fl = false;
+
+#define LOG_PAGE_INTO_LIST(head, page)	do {			\
+	struct page_list *tmp;					\
+	tmp = f2fs_kmem_cache_alloc_nofail(page_info_slab, GFP_NOFS);	\
+	if (tmp) {						\
+		tmp->page = page;				\
+		INIT_LIST_HEAD(&tmp->list);			\
+		list_add_tail(&tmp->list, &head);		\
+	}							\
+} while (0)
+
+#define FREE_FIRST_PAGE_IN_LIST(head)	do {			\
+	struct page_list *tmp;					\
+	tmp = list_first_entry(&head, struct page_list, list);	\
+	f2fs_put_page(tmp->page, 0);				\
+	list_del(&tmp->list);					\
+	kmem_cache_free(page_info_slab, tmp);			\
+} while (0)
+
+int create_page_info_slab(void)
+{
+	page_info_slab = f2fs_kmem_cache_create("f2fs_page_info_entry",
+				sizeof(struct page_list));
+	if (!page_info_slab)
+		return -ENOMEM;
 
-	return ret;
+	return 0;
 }
 
-static vm_fault_t f2fs_vm_page_mkwrite(struct vm_fault *vmf)
+void destroy_page_info_slab(void)
 {
-	struct page *page = vmf->page;
-	struct inode *inode = file_inode(vmf->vma->vm_file);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	bool need_alloc = true;
-	int err = 0;
+	if (!page_info_slab)
+		return;
 
-	if (unlikely(IS_IMMUTABLE(inode)))
-		return VM_FAULT_SIGBUS;
+	kmem_cache_destroy(page_info_slab);
+}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
-		return VM_FAULT_SIGBUS;
+/*
+ * need lock_op and acquire_orphan by caller
+ */
+void f2fs_drop_deduped_link(struct inode *inner)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inner);
 
-	if (unlikely(f2fs_cp_error(sbi))) {
-		err = -EIO;
-		goto err;
-	}
+	f2fs_down_write(&F2FS_I(inner)->i_sem);
+	f2fs_i_links_write(inner, false);
+	f2fs_up_write(&F2FS_I(inner)->i_sem);
 
-	if (!f2fs_is_checkpoint_ready(sbi)) {
-		err = -ENOSPC;
-		goto err;
-	}
+	if (inner->i_nlink == 0)
+		f2fs_add_orphan_inode(inner);
+	else
+		f2fs_release_orphan_inode(sbi);
+}
 
-	err = f2fs_convert_inline_inode(inode);
-	if (err)
-		goto err;
+int f2fs_set_inode_addr(struct inode* inode, block_t addr)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	struct page *node_page;
+	struct f2fs_inode *ri;
+	int count = 1;
+	bool need_update;
+	block_t blkaddr;
+	int i, end_offset;
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	if (f2fs_compressed_file(inode)) {
-		int ret = f2fs_is_compressed_cluster(inode, page->index);
+	if (time_to_inject(sbi, FAULT_DEDUP_FILL_INODE))
+		return -EIO;
 
-		if (ret < 0) {
-			err = ret;
-			goto err;
-		} else if (ret) {
-			need_alloc = false;
-		}
+repeat:
+	node_page = f2fs_get_node_page(sbi, inode->i_ino);
+	if (PTR_ERR(node_page) == -ENOMEM) {
+		if (!(count++ % DEDUP_LOOP_MOD))
+			f2fs_err(sbi,
+				"%s: try to get node page %d", __func__, count);
+
+		cond_resched();
+		goto repeat;
+	} else if (IS_ERR(node_page)) {
+		f2fs_err(sbi, "%s: get node page fail", __func__);
+		return PTR_ERR(node_page);
 	}
-#endif
-	/* should do out of any locked page */
-	if (need_alloc)
-		f2fs_balance_fs(sbi, true);
 
-	sb_start_pagefault(inode->i_sb);
+	f2fs_wait_on_page_writeback(node_page, NODE, true, true);
+	ri = F2FS_INODE(node_page);
 
-	f2fs_bug_on(sbi, f2fs_has_inline_data(inode));
+	end_offset = ADDRS_PER_PAGE(node_page, inode);
+	set_new_dnode(&dn, inode, NULL, node_page, inode->i_ino);
+	dn.ofs_in_node = 0;
 
-	file_update_time(vmf->vma->vm_file);
-	filemap_invalidate_lock_shared(inode->i_mapping);
-	lock_page(page);
-	if (unlikely(page->mapping != inode->i_mapping ||
-			page_offset(page) > i_size_read(inode) ||
-			!PageUptodate(page))) {
-		unlock_page(page);
-		err = -EFAULT;
-		goto out_sem;
-	}
+	for (; dn.ofs_in_node < end_offset; dn.ofs_in_node++) {
+		blkaddr = data_blkaddr(inode, node_page, dn.ofs_in_node);
 
-	if (need_alloc) {
-		/* block allocation */
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = f2fs_get_block_locked(&dn, page->index);
+		if (__is_valid_data_blkaddr(blkaddr) &&
+			f2fs_is_valid_blkaddr(sbi, blkaddr, DATA_GENERIC_ENHANCE))
+			f2fs_err(sbi, "%s: inode[%lu] leak data addr[%d:%u]",
+				__func__, inode->i_ino, dn.ofs_in_node, blkaddr);
+		else {
+			__set_data_blkaddr(&dn, addr);
+			need_update = true;
+		}
 	}
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	if (!need_alloc) {
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
-		f2fs_put_dnode(&dn);
-	}
-#endif
-	if (err) {
-		unlock_page(page);
-		goto out_sem;
+	for (i = 0; i < DEF_NIDS_PER_INODE; i++) {
+		if (ri->i_nid[i])
+			f2fs_err(sbi, "%s: inode[%lu] leak node addr[%d:%u]",
+				__func__, inode->i_ino, i, ri->i_nid[i]);
+		else {
+			ri->i_nid[i] = cpu_to_le32(0);
+			need_update = true;
+		}
 	}
 
-	f2fs_wait_on_page_writeback(page, DATA, false, true);
+	if (need_update)
+		set_page_dirty(node_page);
+	f2fs_put_page(node_page, 1);
 
-	/* wait for GCed page writeback via META_MAPPING */
-	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
+	return 0;
+}
 
-	/*
-	 * check to see if the page is mapped already (no holes)
-	 */
-	if (PageMappedToDisk(page))
-		goto out_sem;
+static int __revoke_deduped_inode_begin(struct inode *dedup)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	int err;
 
-	/* page is wholly or partially inside EOF */
-	if (((loff_t)(page->index + 1) << PAGE_SHIFT) >
-						i_size_read(inode)) {
-		loff_t offset;
+	f2fs_lock_op(sbi);
 
-		offset = i_size_read(inode) & ~PAGE_MASK;
-		zero_user_segment(page, offset, PAGE_SIZE);
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		err = -ENOSPC;
+	else
+		err = f2fs_acquire_orphan_inode(sbi);	/* for layer inode */
+	if (err) {
+		f2fs_unlock_op(sbi);
+		f2fs_err(sbi, "revoke inode[%lu] begin fail, ret:%d",
+			dedup->i_ino, err);
+		return err;
 	}
-	set_page_dirty(page);
-	if (!PageUptodate(page))
-		SetPageUptodate(page);
 
-	f2fs_update_iostat(sbi, inode, APP_MAPPED_IO, F2FS_BLKSIZE);
-	f2fs_update_time(sbi, REQ_TIME);
+	f2fs_add_orphan_inode(dedup);
 
-	trace_f2fs_vm_page_mkwrite(page, DATA);
-out_sem:
-	filemap_invalidate_unlock_shared(inode->i_mapping);
+	set_inode_flag(dedup, FI_REVOKE_DEDUP);
 
-	sb_end_pagefault(inode->i_sb);
-err:
-	return block_page_mkwrite_return(err);
+	f2fs_unlock_op(sbi);
+
+	return 0;
 }
 
-static const struct vm_operations_struct f2fs_file_vm_ops = {
-	.fault		= f2fs_filemap_fault,
-	.map_pages	= filemap_map_pages,
-	.page_mkwrite	= f2fs_vm_page_mkwrite,
-};
+/*
+ * For kernel version < 5.4.0, we depend on inode lock in direct read IO
+ */
+static void dedup_wait_dio(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 
-static int get_parent_ino(struct inode *inode, nid_t *pino)
+	f2fs_down_write(&fi->i_gc_rwsem[READ]);
+	inode_dio_wait(inode);
+	f2fs_up_write(&fi->i_gc_rwsem[READ]);
+}
+static void prepare_free_inner_inode(struct inode *inode, struct inode *inner)
 {
-	struct dentry *dentry;
+	struct f2fs_inode_info *fi = F2FS_I(inner);
+
+	fi->i_flags &= ~F2FS_IMMUTABLE_FL;
+	f2fs_set_inode_flags(inner);
+	f2fs_mark_inode_dirty_sync(inner, true);
 
 	/*
-	 * Make sure to get the non-deleted alias.  The alias associated with
-	 * the open file descriptor being fsync()'ed may be deleted already.
+	 * Before free inner inode, we should wait all reader of
+	 * the inner complete to avoid UAF or read unexpected data.
 	 */
-	dentry = d_find_alias(inode);
-	if (!dentry)
-		return 0;
+	wait_event(fi->dedup_wq,
+			atomic_read(&fi->inflight_read_io) == 0);
 
-	*pino = parent_ino(dentry);
-	dput(dentry);
-	return 1;
+	dedup_wait_dio(inode);
 }
 
-static inline enum cp_reason_type need_do_checkpoint(struct inode *inode)
+static int __revoke_deduped_inode_end(struct inode *dedup)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	enum cp_reason_type cp_reason = CP_NO_NEEDED;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	struct f2fs_inode_info *fi = F2FS_I(dedup);
+	struct inode *inner = NULL;
+	int err;
 
-	if (!S_ISREG(inode->i_mode))
-		cp_reason = CP_NON_REGULAR;
-	else if (f2fs_compressed_file(inode))
-		cp_reason = CP_COMPRESSED;
-	else if (inode->i_nlink != 1)
-		cp_reason = CP_HARDLINK;
-	else if (is_sbi_flag_set(sbi, SBI_NEED_CP))
-		cp_reason = CP_SB_NEED_CP;
-	else if (file_wrong_pino(inode))
-		cp_reason = CP_WRONG_PINO;
-	else if (!f2fs_space_for_roll_forward(sbi))
-		cp_reason = CP_NO_SPC_ROLL;
-	else if (!f2fs_is_checkpointed_node(sbi, F2FS_I(inode)->i_pino))
-		cp_reason = CP_NODE_NEED_CP;
-	else if (test_opt(sbi, FASTBOOT))
-		cp_reason = CP_FASTBOOT_MODE;
-	else if (F2FS_OPTION(sbi).active_logs == 2)
-		cp_reason = CP_SPEC_LOG_NUM;
-	else if (F2FS_OPTION(sbi).fsync_mode == FSYNC_MODE_STRICT &&
-		f2fs_need_dentry_mark(sbi, inode->i_ino) &&
-		f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
-							TRANS_DIR_INO))
-		cp_reason = CP_RECOVER_DIR;
-	else if (f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
-							XATTR_DIR_INO))
-		cp_reason = CP_XATTR_DIR;
+	f2fs_lock_op(sbi);
 
-	return cp_reason;
-}
+	f2fs_remove_orphan_inode(sbi, dedup->i_ino);
 
-static bool need_inode_page_update(struct f2fs_sb_info *sbi, nid_t ino)
-{
-	struct page *i = find_get_page(NODE_MAPPING(sbi), ino);
-	bool ret = false;
-	/* But we need to avoid that there are some inode updates */
-	if ((i && PageDirty(i)) || f2fs_need_inode_block_update(sbi, ino))
-		ret = true;
-	f2fs_put_page(i, 0);
-	return ret;
-}
+	f2fs_down_write(&fi->i_sem);
+	clear_inode_flag(dedup, FI_REVOKE_DEDUP);
+	clear_inode_flag(dedup, FI_DEDUPED);
+	clear_inode_flag(dedup, FI_META_UN_MODIFY);
+	clear_inode_flag(dedup, FI_DATA_UN_MODIFY);
 
-static void try_to_fix_pino(struct inode *inode)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	nid_t pino;
+	/*
+	 * other reader flow:
+	 * 1) lock inode
+	 * 2) judge whether inner_inode is NULL
+	 * 3) if no, then __iget inner inode
+	 */
+	inner = fi->inner_inode;
+	fi->inner_inode = NULL;
+	fi->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_up_write(&fi->i_sem);
 
-	f2fs_down_write(&fi->i_sem);
-	if (file_wrong_pino(inode) && inode->i_nlink == 1 &&
-			get_parent_ino(inode, &pino)) {
-		f2fs_i_pino_write(inode, pino);
-		file_got_pino(inode);
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		err = -ENOSPC;
+	else
+		err = f2fs_acquire_orphan_inode(sbi);	/* for inner inode */
+	if (err) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);	/* delete inner inode */
+		f2fs_warn(sbi,
+			"%s: orphan failed (ino=%lx), run fsck to fix.",
+			__func__, inner->i_ino);
+	} else {
+		f2fs_drop_deduped_link(inner);
 	}
-	f2fs_up_write(&fi->i_sem);
+	f2fs_unlock_op(sbi);
+
+	trace_f2fs_dedup_revoke_inode(dedup, inner);
+
+	if (inner->i_nlink == 0)
+		prepare_free_inner_inode(dedup, inner);
+
+	iput(inner);
+	return err;
 }
 
-static int f2fs_do_sync_file(struct file *file, loff_t start, loff_t end,
-						int datasync, bool atomic)
+bool f2fs_is_hole_blkaddr(struct inode *inode, pgoff_t pgofs)
 {
-	struct inode *inode = file->f_mapping->host;
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	nid_t ino = inode->i_ino;
-	int ret = 0;
-	enum cp_reason_type cp_reason = 0;
-	struct writeback_control wbc = {
-		.sync_mode = WB_SYNC_ALL,
-		.nr_to_write = LONG_MAX,
-		.for_reclaim = 0,
-	};
-	unsigned int seq_id = 0;
+	struct dnode_of_data dn;
+	block_t blkaddr;
+	int err = 0;
 
-	if (unlikely(f2fs_readonly(inode->i_sb)))
-		return 0;
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_HOLE))
+		return true;
 
-	trace_f2fs_sync_file_enter(inode);
+	if (f2fs_has_inline_data(inode) ||
+		f2fs_has_inline_dentry(inode))
+		return false;
 
-	if (S_ISDIR(inode->i_mode))
-		goto go_write;
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, pgofs, LOOKUP_NODE);
+	if (err && err != -ENOENT)
+		return false;
 
-	/* if fdatasync is triggered, let's do in-place-update */
-	if (datasync || get_dirty_pages(inode) <= SM_I(sbi)->min_fsync_blocks)
-		set_inode_flag(inode, FI_NEED_IPU);
-	ret = file_write_and_wait_range(file, start, end);
-	clear_inode_flag(inode, FI_NEED_IPU);
+	/* direct node does not exists */
+	if (err == -ENOENT)
+		return true;
 
-	if (ret || is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
-		trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
-		return ret;
-	}
+	blkaddr = f2fs_data_blkaddr(&dn);
+	f2fs_put_dnode(&dn);
 
-	/* if the inode is dirty, let's recover all the time */
-	if (!f2fs_skip_inode_update(inode, datasync)) {
-		f2fs_write_inode(inode, NULL);
-		goto go_write;
-	}
+	if (__is_valid_data_blkaddr(blkaddr) &&
+		!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
+			blkaddr, DATA_GENERIC))
+		return false;
 
-	/*
-	 * if there is no written data, don't waste time to write recovery info.
-	 */
-	if (!is_inode_flag_set(inode, FI_APPEND_WRITE) &&
-			!f2fs_exist_written_data(sbi, ino, APPEND_INO)) {
+	if (blkaddr != NULL_ADDR)
+		return false;
 
-		/* it may call write_inode just prior to fsync */
-		if (need_inode_page_update(sbi, ino))
-			goto go_write;
+	return true;
+}
 
-		if (is_inode_flag_set(inode, FI_UPDATE_WRITE) ||
-				f2fs_exist_written_data(sbi, ino, UPDATE_INO))
-			goto flush_out;
-		goto out;
-	} else {
-		/*
-		 * for OPU case, during fsync(), node can be persisted before
-		 * data when lower device doesn't support write barrier, result
-		 * in data corruption after SPO.
-		 * So for strict fsync mode, force to use atomic write semantics
-		 * to keep write order in between data/node and last node to
-		 * avoid potential data corruption.
-		 */
-		if (F2FS_OPTION(sbi).fsync_mode ==
-				FSYNC_MODE_STRICT && !atomic)
-			atomic = true;
-	}
-go_write:
-	/*
-	 * Both of fdatasync() and fsync() are able to be recovered from
-	 * sudden-power-off.
-	 */
-	f2fs_down_read(&F2FS_I(inode)->i_sem);
-	cp_reason = need_do_checkpoint(inode);
-	f2fs_up_read(&F2FS_I(inode)->i_sem);
+static int revoke_deduped_blocks(struct inode *dedup, pgoff_t page_idx, int len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	struct address_space *mapping = dedup->i_mapping;
+	pgoff_t redirty_idx = page_idx;
+	int i, page_len = 0, ret = 0;
+	struct dnode_of_data dn;
+	filler_t *filler = NULL;
+	struct page *page;
+	LIST_HEAD(pages);
 
-	if (cp_reason) {
-		/* all the dirty node pages should be flushed for POR */
-		ret = f2fs_sync_fs(inode->i_sb, 1);
+	DEFINE_READAHEAD(ractl, NULL, NULL, mapping, page_idx);
+	page_cache_ra_unbounded(&ractl, len, 0);
 
-		/*
-		 * We've secured consistency through sync_fs. Following pino
-		 * will be used only for fsynced inodes after checkpoint.
-		 */
-		try_to_fix_pino(inode);
-		clear_inode_flag(inode, FI_APPEND_WRITE);
-		clear_inode_flag(inode, FI_UPDATE_WRITE);
-		goto out;
+	/* readahead pages in file */
+	for (i = 0; i < len; i++, page_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_REVOKE)) {
+			ret = -EIO;
+			goto out;
+		}
+		page = read_cache_page(mapping, page_idx, filler, NULL);
+		if (IS_ERR(page)) {
+			ret = PTR_ERR(page);
+			goto out;
+		}
+		page_len++;
+		LOG_PAGE_INTO_LIST(pages, page);
 	}
-sync_nodes:
-	atomic_inc(&sbi->wb_sync_req[NODE]);
-	ret = f2fs_fsync_node_pages(sbi, inode, &wbc, atomic, &seq_id);
-	atomic_dec(&sbi->wb_sync_req[NODE]);
-	if (ret)
-		goto out;
 
-	/* if cp_error was enabled, we should avoid infinite loop */
-	if (unlikely(f2fs_cp_error(sbi))) {
-		ret = -EIO;
-		goto out;
-	}
+	/* rewrite pages above */
+	for (i = 0; i < page_len; i++, redirty_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_REVOKE)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = find_lock_page(mapping, redirty_idx);
+		if (!page) {
+			ret = -ENOMEM;
+			break;
+		}
 
-	if (f2fs_need_inode_block_update(sbi, ino)) {
-		f2fs_mark_inode_dirty_sync(inode, true);
-		f2fs_write_inode(inode, NULL);
-		goto sync_nodes;
-	}
+		if (!f2fs_is_hole_blkaddr(F2FS_I(dedup)->inner_inode, redirty_idx)) {
+			set_new_dnode(&dn, dedup, NULL, NULL, 0);
+			if (time_to_inject(sbi, FAULT_DEDUP_REVOKE))
+				ret = -ENOSPC;
+			else
+				ret = f2fs_get_block_locked(&dn, redirty_idx);
+			f2fs_put_dnode(&dn);
+			f2fs_bug_on(sbi, !PageUptodate(page));
+			if (!ret)
+				set_page_dirty(page);
+		}
 
-	/*
-	 * If it's atomic_write, it's just fine to keep write ordering. So
-	 * here we don't need to wait for node write completion, since we use
-	 * node chain which serializes node blocks. If one of node writes are
-	 * reordered, we can see simply broken chain, resulting in stopping
-	 * roll-forward recovery. It means we'll recover all or none node blocks
-	 * given fsync mark.
-	 */
-	if (!atomic) {
-		ret = f2fs_wait_on_node_pages_writeback(sbi, seq_id);
+		f2fs_put_page(page, 1);
 		if (ret)
-			goto out;
+			break;
 	}
 
-	/* once recovery info is written, don't need to tack this */
-	f2fs_remove_ino_entry(sbi, ino, APPEND_INO);
-	clear_inode_flag(inode, FI_APPEND_WRITE);
-flush_out:
-	if ((!atomic && F2FS_OPTION(sbi).fsync_mode != FSYNC_MODE_NOBARRIER) ||
-	    (atomic && !test_opt(sbi, NOBARRIER) && f2fs_sb_has_blkzoned(sbi)))
-		ret = f2fs_issue_flush(sbi, inode->i_ino);
-	if (!ret) {
-		f2fs_remove_ino_entry(sbi, ino, UPDATE_INO);
-		clear_inode_flag(inode, FI_UPDATE_WRITE);
-		f2fs_remove_ino_entry(sbi, ino, FLUSH_INO);
-	}
-	f2fs_update_time(sbi, REQ_TIME);
 out:
-	trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
+	while (!list_empty(&pages))
+		FREE_FIRST_PAGE_IN_LIST(pages);
+
 	return ret;
 }
 
-int f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
+static int __revoke_deduped_data(struct inode *dedup)
 {
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(file_inode(file)))))
-		return -EIO;
-	return f2fs_do_sync_file(file, start, end, datasync, false);
-}
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
+	pgoff_t page_idx = 0, last_idx;
+	int blk_per_seg = sbi->blocks_per_seg;
+	int count;
+	int ret1 = 0;
+	int ret2 = 0;
 
-static bool __found_offset(struct address_space *mapping,
-		struct dnode_of_data *dn, pgoff_t index, int whence)
-{
-	block_t blkaddr = f2fs_data_blkaddr(dn);
-	struct inode *inode = mapping->host;
-	bool compressed_cluster = false;
+	f2fs_set_inode_addr(dedup, NULL_ADDR);
+	last_idx = DIV_ROUND_UP(i_size_read(dedup), PAGE_SIZE);
 
-	if (f2fs_compressed_file(inode)) {
-		block_t first_blkaddr = data_blkaddr(dn->inode, dn->node_page,
-		    ALIGN_DOWN(dn->ofs_in_node, F2FS_I(inode)->i_cluster_size));
+	count = last_idx - page_idx;
+	while (count) {
+		int len = min(blk_per_seg, count);
+		ret1 = revoke_deduped_blocks(dedup, page_idx, len);
+		if (ret1 < 0)
+			break;
 
-		compressed_cluster = first_blkaddr == COMPRESS_ADDR;
-	}
+		filemap_fdatawrite(dedup->i_mapping);
 
-	switch (whence) {
-	case SEEK_DATA:
-		if (__is_valid_data_blkaddr(blkaddr))
-			return true;
-		if (blkaddr == NEW_ADDR &&
-		    xa_get_mark(&mapping->i_pages, index, PAGECACHE_TAG_DIRTY))
-			return true;
-		if (compressed_cluster)
-			return true;
-		break;
-	case SEEK_HOLE:
-		if (compressed_cluster)
-			return false;
-		if (blkaddr == NULL_ADDR)
-			return true;
-		break;
+		count -= len;
+		page_idx += len;
 	}
-	return false;
+
+	ret2 = f2fs_filemap_write_and_wait_range(dedup);
+	if (ret1 || ret2)
+		f2fs_warn(sbi, "%s: The deduped inode[%lu] revoked fail(errno=%d,%d).",
+				__func__, dedup->i_ino, ret1, ret2);
+
+	return ret1 ? : ret2;
 }
 
-static loff_t f2fs_seek_block(struct file *file, loff_t offset, int whence)
+static void _revoke_error_handle(struct inode *inode)
 {
-	struct inode *inode = file->f_mapping->host;
-	loff_t maxbytes = inode->i_sb->s_maxbytes;
-	struct dnode_of_data dn;
-	pgoff_t pgofs, end_offset;
-	loff_t data_ofs = offset;
-	loff_t isize;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	f2fs_lock_op(sbi);
+	f2fs_truncate_dedup_inode(inode, FI_REVOKE_DEDUP);
+	f2fs_remove_orphan_inode(sbi, inode->i_ino);
+	F2FS_I(inode)->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_unlock_op(sbi);
+	trace_f2fs_dedup_revoke_fail(inode, F2FS_I(inode)->inner_inode);
+}
+
+/*
+ * need inode_lock by caller
+ */
+int f2fs_revoke_deduped_inode(struct inode *dedup, const char *revoke_source)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup);
 	int err = 0;
+	struct inode *inner_inode = NULL;
+	nid_t inner_ino = 0;
 
-	inode_lock(inode);
+	if (unlikely(f2fs_cp_error(sbi)))
+		return -EIO;
 
-	isize = i_size_read(inode);
-	if (offset >= isize)
-		goto fail;
+	if (!f2fs_is_outer_inode(dedup))
+		return -EINVAL;
 
-	/* handle inline data case */
-	if (f2fs_has_inline_data(inode)) {
-		if (whence == SEEK_HOLE) {
-			data_ofs = isize;
-			goto found;
-		} else if (whence == SEEK_DATA) {
-			data_ofs = offset;
-			goto found;
-		}
-	}
+	if (is_inode_flag_set(dedup, FI_SNAPSHOTED))
+		return -EOPNOTSUPP;
 
-	pgofs = (pgoff_t)(offset >> PAGE_SHIFT);
+	if (is_inode_flag_set(dedup, FI_SNAPSHOT_PREPARED))
+		return 0;
 
-	for (; data_ofs < isize; data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = f2fs_get_dnode_of_data(&dn, pgofs, LOOKUP_NODE);
-		if (err && err != -ENOENT) {
-			goto fail;
-		} else if (err == -ENOENT) {
-			/* direct node does not exists */
-			if (whence == SEEK_DATA) {
-				pgofs = f2fs_get_next_page_offset(&dn, pgofs);
-				continue;
-			} else {
-				goto found;
-			}
-		}
+	err = f2fs_dquot_initialize(dedup);
+	if (err)
+		return err;
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+	f2fs_balance_fs(sbi, true);
 
-		/* find data/hole in dnode block */
-		for (; dn.ofs_in_node < end_offset;
-				dn.ofs_in_node++, pgofs++,
-				data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
-			block_t blkaddr;
+	inner_inode = F2FS_I(dedup)->inner_inode;
+	if (inner_inode)
+		inner_ino = inner_inode->i_ino;
 
-			blkaddr = f2fs_data_blkaddr(&dn);
+	err = __revoke_deduped_inode_begin(dedup);
+	if (err)
+		goto ret;
 
-			if (__is_valid_data_blkaddr(blkaddr) &&
-				!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
-					blkaddr, DATA_GENERIC_ENHANCE)) {
-				f2fs_put_dnode(&dn);
-				goto fail;
-			}
+	err = __revoke_deduped_data(dedup);
+	if (err) {
+		_revoke_error_handle(dedup);
+		goto ret;
+	}
 
-			if (__found_offset(file->f_mapping, &dn,
-							pgofs, whence)) {
-				f2fs_put_dnode(&dn);
-				goto found;
-			}
-		}
-		f2fs_put_dnode(&dn);
+	err = __revoke_deduped_inode_end(dedup);
+
+ret:
+	return err;
+}
+
+static void f2fs_set_modify_check(struct inode *inode,
+		struct f2fs_modify_check_info *info)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (info->flag & DEDUP_META_UN_MODIFY_FL) {
+		if (is_inode_flag_set(inode, FI_META_UN_MODIFY))
+			f2fs_err(sbi,
+				"inode[%lu] had set meta unmodified flag",
+				inode->i_ino);
+		else
+			set_inode_flag(inode, FI_META_UN_MODIFY);
 	}
 
-	if (whence == SEEK_DATA)
-		goto fail;
-found:
-	if (whence == SEEK_HOLE && data_ofs > isize)
-		data_ofs = isize;
-	inode_unlock(inode);
-	return vfs_setpos(file, data_ofs, maxbytes);
-fail:
-	inode_unlock(inode);
-	return -ENXIO;
+	if (info->flag & DEDUP_DATA_UN_MODIFY_FL) {
+		if (is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+			f2fs_err(sbi,
+				"inode[%lu] had set data unmodified flag",
+				inode->i_ino);
+		else
+			set_inode_flag(inode, FI_DATA_UN_MODIFY);
+	}
 }
 
-static loff_t f2fs_llseek(struct file *file, loff_t offset, int whence)
+static void f2fs_get_modify_check(struct inode *inode,
+		struct f2fs_modify_check_info *info)
 {
-	struct inode *inode = file->f_mapping->host;
-	loff_t maxbytes = inode->i_sb->s_maxbytes;
+	memset(&(info->flag), 0, sizeof(info->flag));
 
-	if (f2fs_compressed_file(inode))
-		maxbytes = max_file_blocks(inode) << F2FS_BLKSIZE_BITS;
+	if (is_inode_flag_set(inode, FI_META_UN_MODIFY))
+		info->flag = info->flag | DEDUP_META_UN_MODIFY_FL;
 
-	switch (whence) {
-	case SEEK_SET:
-	case SEEK_CUR:
-	case SEEK_END:
-		return generic_file_llseek_size(file, offset, whence,
-						maxbytes, i_size_read(inode));
-	case SEEK_DATA:
-	case SEEK_HOLE:
-		if (offset < 0)
-			return -ENXIO;
-		return f2fs_seek_block(file, offset, whence);
+	if (is_inode_flag_set(inode, FI_DATA_UN_MODIFY))
+		info->flag = info->flag | DEDUP_DATA_UN_MODIFY_FL;
+}
+
+static void f2fs_clear_modify_check(struct inode *inode,
+		struct f2fs_modify_check_info *info)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (info->flag & DEDUP_META_UN_MODIFY_FL) {
+		if (!is_inode_flag_set(inode, FI_META_UN_MODIFY)) {
+			f2fs_err(sbi,
+				"inode[%lu] had clear unmodified meta flag",
+				inode->i_ino);
+		}
+
+		clear_inode_flag(inode, FI_META_UN_MODIFY);
 	}
 
-	return -EINVAL;
+	if (info->flag & DEDUP_DATA_UN_MODIFY_FL) {
+		if (!is_inode_flag_set(inode, FI_DATA_UN_MODIFY)) {
+			f2fs_err(sbi,
+				"inode[%lu] had clear unmodified data flag",
+				inode->i_ino);
+		}
+
+		clear_inode_flag(inode, FI_DATA_UN_MODIFY);
+	}
+
+	f2fs_mark_inode_dirty_sync(inode, true);
 }
 
-static int f2fs_file_mmap(struct file *file, struct vm_area_struct *vma)
+bool f2fs_inode_support_dedup(struct f2fs_sb_info *sbi,
+		struct inode *inode)
 {
-	struct inode *inode = file_inode(file);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri;
 
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
+	if (!f2fs_sb_has_dedup(sbi))
+		return false;
 
-	if (!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	if (!f2fs_has_extra_attr(inode))
+		return false;
 
-	file_accessed(file);
-	vma->vm_ops = &f2fs_file_vm_ops;
+	if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_inner_ino))
+		return false;
 
-	f2fs_down_read(&F2FS_I(inode)->i_sem);
-	set_inode_flag(inode, FI_MMAP_FILE);
-	f2fs_up_read(&F2FS_I(inode)->i_sem);
+	if (f2fs_compressed_file(inode))
+		return false;
 
-	return 0;
+	return true;
 }
 
-static int finish_preallocate_blocks(struct inode *inode)
+static int f2fs_inode_param_check(struct f2fs_sb_info *sbi,
+		struct inode *inode, int type)
 {
-	int ret;
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	inode_lock(inode);
-	if (is_inode_flag_set(inode, FI_OPENED_FILE)) {
-		inode_unlock(inode);
-		return 0;
+	if (type == OUTER_INODE &&
+		!is_inode_flag_set(inode, FI_SNAPSHOTED) &&
+		!is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED) &&
+					inode->i_size < DEDUP_MIN_SIZE) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] size < %d bytes.",
+			inode->i_ino, DEDUP_MIN_SIZE);
+		return -EINVAL;
 	}
 
-	if (!file_should_truncate(inode)) {
-		set_inode_flag(inode, FI_OPENED_FILE);
-		inode_unlock(inode);
-		return 0;
+	if (type == OUTER_INODE &&
+		!is_inode_flag_set(inode, FI_DATA_UN_MODIFY)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] has been modified.",
+			inode->i_ino);
+		return -EINVAL;
 	}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
-
-	truncate_setsize(inode, i_size_read(inode));
-	ret = f2fs_truncate(inode);
-
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	if (IS_VERITY(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] enable verity.",
+			inode->i_ino);
+		return -EACCES;
+	}
 
-	if (!ret)
-		set_inode_flag(inode, FI_OPENED_FILE);
+	if (f2fs_is_atomic_file(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] is atomic file.",
+			inode->i_ino);
+		return -EACCES;
+	}
 
-	inode_unlock(inode);
-	if (ret)
-		return ret;
+	if (f2fs_is_pinned_file(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] is pinned file.",
+			inode->i_ino);
+		return -EACCES;
+	}
 
-	file_dont_truncate(inode);
+	if (type != INNER_INODE && IS_IMMUTABLE(inode)) {
+		f2fs_err(sbi, "dedup fails, inode[%lu] is immutable.",
+			inode->i_ino);
+		return -EACCES;
+	}
 	return 0;
 }
 
-static int f2fs_file_open(struct inode *inode, struct file *filp)
+static int f2fs_dedup_param_check(struct f2fs_sb_info *sbi,
+		struct inode *inode1, int type1,
+		struct inode *inode2, int type2)
 {
-	int err = fscrypt_file_open(inode, filp);
+	int ret;
 
-	if (err)
-		return err;
+	if (time_to_inject(sbi, FAULT_DEDUP_PARAM_CHECK))
+		return -EINVAL;
 
-	if (!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	if (inode1->i_sb != inode2->i_sb || inode1 == inode2) {
+		f2fs_err(sbi, "%s: input inode[%lu] and [%lu] are illegal.",
+			__func__, inode1->i_ino, inode2->i_ino);
+		return -EINVAL;
+	}
 
-	err = fsverity_file_open(inode, filp);
-	if (err)
-		return err;
+	if (type1 == OUTER_INODE && type2 == OUTER_INODE &&
+		!is_inode_flag_set(inode2, FI_SNAPSHOTED) &&
+		inode1->i_size != inode2->i_size) {
+		f2fs_err(sbi,
+			"dedup file size not match inode1[%lu] %lld, inode2[%lu] %lld",
+			inode1->i_ino, inode1->i_size,
+			inode2->i_ino, inode2->i_size);
+		return -EINVAL;
+	}
 
-	filp->f_mode |= FMODE_NOWAIT;
+	ret = f2fs_inode_param_check(sbi, inode1, type1);
+	if (ret)
+		return ret;
 
-	err = dquot_file_open(inode, filp);
-	if (err)
-		return err;
+	ret = f2fs_inode_param_check(sbi, inode2, type2);
+	if (ret)
+		return ret;
 
-	return finish_preallocate_blocks(inode);
+	return 0;
 }
 
-void f2fs_truncate_data_blocks_range(struct dnode_of_data *dn, int count)
+static int f2fs_ioc_modify_check(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	int nr_free = 0, ofs = dn->ofs_in_node, len = count;
-	__le32 *addr;
-	bool compressed_cluster = false;
-	int cluster_index = 0, valid_blocks = 0;
-	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
-	bool released = !atomic_read(&F2FS_I(dn->inode)->i_compr_blocks);
-	block_t blkstart;
-	int blklen = 0;
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct f2fs_modify_check_info info;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
 
-	addr = get_dnode_addr(dn->inode, dn->node_page) + ofs;
-	blkstart = le32_to_cpu(*addr);
+	if (unlikely(f2fs_cp_error(sbi)))
+		return -EIO;
 
-	/* Assumption: truncation starts with cluster */
-	for (; count > 0; count--, addr++, dn->ofs_in_node++, cluster_index++) {
-		block_t blkaddr = le32_to_cpu(*addr);
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-		if (f2fs_compressed_file(dn->inode) &&
-					!(cluster_index & (cluster_size - 1))) {
-			if (compressed_cluster)
-				f2fs_i_compr_blocks_update(dn->inode,
-							valid_blocks, false);
-			compressed_cluster = (blkaddr == COMPRESS_ADDR);
-			valid_blocks = 0;
-		}
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-		if (blkaddr == NULL_ADDR)
-			goto next;
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-		f2fs_set_data_blkaddr(dn, NULL_ADDR);
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-		if (__is_valid_data_blkaddr(blkaddr)) {
-			if (time_to_inject(sbi, FAULT_BLKADDR_CONSISTENCE))
-				goto next;
-			if (!f2fs_is_valid_blkaddr_raw(sbi, blkaddr,
-						DATA_GENERIC_ENHANCE)) {
-				f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-				goto next;
-			}
-			if (compressed_cluster)
-				valid_blocks++;
-		}
+	if (f2fs_has_inline_data(inode))
+		return -EINVAL;
 
-		if (blkstart + blklen == blkaddr) {
-			blklen++;
-		} else {
-			f2fs_invalidate_blocks(sbi, blkstart, blklen);
-			blkstart = blkaddr;
-			blklen = 1;
-		}
+	if (copy_from_user(&info,
+		(struct f2fs_modify_check_info __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-		if (!released || blkaddr != COMPRESS_ADDR)
-			nr_free++;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-		continue;
+	inode_lock(inode);
+	if (info.mode & DEDUP_SET_MODIFY_CHECK) {
+		struct address_space *mapping = inode->i_mapping;
+		bool dirty = false;
+		int nrpages = 0;
 
-next:
-		if (blklen)
-			f2fs_invalidate_blocks(sbi, blkstart, blklen);
+		if (mapping_mapped(mapping)) {
+			f2fs_err(sbi, "inode[%lu] has mapped vma", inode->i_ino);
+			ret = -EBUSY;
+			goto out;
+		}
 
-		blkstart = le32_to_cpu(*(addr + 1));
-		blklen = 0;
-	}
+		ret = f2fs_inode_param_check(sbi, inode, NORMAL_INODE);
+		if (ret)
+			goto out;
 
-	if (blklen)
-		f2fs_invalidate_blocks(sbi, blkstart, blklen);
+		if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) ||
+				mapping_tagged(mapping, PAGECACHE_TAG_WRITEBACK)) {
+			dirty = true;
+			nrpages = get_dirty_pages(inode);
+		}
 
-	if (compressed_cluster)
-		f2fs_i_compr_blocks_update(dn->inode, valid_blocks, false);
+		if (dirty && (info.flag & DEDUP_SYNC_DATA)) {
+			ret = f2fs_filemap_write_and_wait_range(inode);
+			if (ret) {
+				f2fs_err(sbi, "inode[%lu] write data fail(%d)\n",
+						inode->i_ino, ret);
+				goto out;
+			}
+		} else if (dirty) {
+			f2fs_err(sbi, "inode[%lu] have dirty page[%d]\n",
+					inode->i_ino, nrpages);
+			ret = -EINVAL;
+			goto out;
+		}
 
-	if (nr_free) {
-		pgoff_t fofs;
-		/*
-		 * once we invalidate valid blkaddr in range [ofs, ofs + count],
-		 * we will invalidate all blkaddr in the whole range.
-		 */
-		fofs = f2fs_start_bidx_of_node(ofs_of_node(dn->node_page),
-							dn->inode) + ofs;
-		f2fs_update_read_extent_cache_range(dn, fofs, 0, len);
-		f2fs_update_age_extent_cache_range(dn, fofs, len);
-		dec_valid_block_count(sbi, dn->inode, nr_free);
+		f2fs_set_modify_check(inode, &info);
+	} else if (info.mode & DEDUP_GET_MODIFY_CHECK) {
+		f2fs_get_modify_check(inode, &info);
+	} else if (info.mode & DEDUP_CLEAR_MODIFY_CHECK) {
+		f2fs_clear_modify_check(inode, &info);
+	} else {
+		ret = -EINVAL;
 	}
-	dn->ofs_in_node = ofs;
 
-	f2fs_update_time(sbi, REQ_TIME);
-	trace_f2fs_truncate_data_blocks_range(dn->inode, dn->nid,
-					 dn->ofs_in_node, nr_free);
-}
+out:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
 
-void f2fs_truncate_data_blocks(struct dnode_of_data *dn)
-{
-	f2fs_truncate_data_blocks_range(dn, ADDRS_PER_BLOCK(dn->inode));
+	if (copy_to_user((struct f2fs_modify_check_info __user *)arg,
+		&info, sizeof(info)))
+		ret = -EFAULT;
+
+	return ret;
 }
 
-static int truncate_partial_data_page(struct inode *inode, u64 from,
-								bool cache_only)
+static int f2fs_ioc_dedup_permission_check(struct file *filp, unsigned long arg)
 {
-	loff_t offset = from & (PAGE_SIZE - 1);
-	pgoff_t index = from >> PAGE_SHIFT;
-	struct address_space *mapping = inode->i_mapping;
-	struct page *page;
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (!offset && !cache_only)
-		return 0;
+	if (unlikely(f2fs_cp_error(sbi)))
+		return -EIO;
 
-	if (cache_only) {
-		page = find_lock_page(mapping, index);
-		if (page && PageUptodate(page))
-			goto truncate_out;
-		f2fs_put_page(page, 1);
-		return 0;
-	}
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	page = f2fs_get_lock_data_page(inode, index, true);
-	if (IS_ERR(page))
-		return PTR_ERR(page) == -ENOENT ? 0 : PTR_ERR(page);
-truncate_out:
-	f2fs_wait_on_page_writeback(page, DATA, true, true);
-	zero_user(page, offset, PAGE_SIZE - offset);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	/* An encrypted inode should have a key and truncate the last page. */
-	f2fs_bug_on(F2FS_I_SB(inode), cache_only && IS_ENCRYPTED(inode));
-	if (!cache_only)
-		set_page_dirty(page);
-	f2fs_put_page(page, 1);
-	return 0;
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
+
+	if (f2fs_has_inline_data(inode))
+		return -EINVAL;
+
+	return f2fs_inode_param_check(sbi, inode, OUTER_INODE);
 }
 
-int f2fs_do_truncate_blocks(struct inode *inode, u64 from, bool lock)
+static int f2fs_copy_data(struct inode *dst_inode,
+		struct inode *src_inode, pgoff_t page_idx, int len)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dst_inode);
+	struct address_space *src_mapping = src_inode->i_mapping;
+	struct address_space *dst_mapping = dst_inode->i_mapping;
+	filler_t *filler = NULL;
+	struct page *page, *newpage;
+	pgoff_t copy_idx = page_idx;
+	int i, page_len = 0, ret = 0;
 	struct dnode_of_data dn;
-	pgoff_t free_from;
-	int count = 0, err = 0;
-	struct page *ipage;
-	bool truncate_page = false;
-
-	trace_f2fs_truncate_blocks_enter(inode, from);
-
-	free_from = (pgoff_t)F2FS_BLK_ALIGN(from);
+	DEFINE_READAHEAD(ractl, NULL, NULL, src_mapping, page_idx);
+	LIST_HEAD(pages);
 
-	if (free_from >= max_file_blocks(inode))
-		goto free_partial;
+	page_cache_ra_unbounded(&ractl, len, 0);
 
-	if (lock)
-		f2fs_lock_op(sbi);
-
-	ipage = f2fs_get_node_page(sbi, inode->i_ino);
-	if (IS_ERR(ipage)) {
-		err = PTR_ERR(ipage);
-		goto out;
+	for (i = 0; i < len; i++, page_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = read_cache_page(src_mapping, page_idx, filler, NULL);
+		if (IS_ERR(page)) {
+			ret = PTR_ERR(page);
+			goto out;
+		}
+		page_len++;
+		LOG_PAGE_INTO_LIST(pages, page);
 	}
 
-	if (f2fs_has_inline_data(inode)) {
-		f2fs_truncate_inline_inode(inode, ipage, from);
-		f2fs_put_page(ipage, 1);
-		truncate_page = true;
-		goto out;
-	}
+	for (i = 0; i < page_len; i++, copy_idx++) {
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = find_lock_page(src_mapping, copy_idx);
+		if (!page) {
+			ret = -ENOMEM;
+			break;
+		}
 
-	set_new_dnode(&dn, inode, ipage, NULL, 0);
-	err = f2fs_get_dnode_of_data(&dn, free_from, LOOKUP_NODE_RA);
-	if (err) {
-		if (err == -ENOENT)
-			goto free_next;
-		goto out;
-	}
+		if (f2fs_is_hole_blkaddr(src_inode, copy_idx)) {
+			f2fs_put_page(page, 1);
+			continue;
+		}
 
-	count = ADDRS_PER_PAGE(dn.node_page, inode);
+		set_new_dnode(&dn, dst_inode, NULL, NULL, 0);
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE))
+			ret = -ENOSPC;
+		else
+			ret = f2fs_get_block_locked(&dn, copy_idx);
+		f2fs_put_dnode(&dn);
+		if (ret) {
+			f2fs_put_page(page, 1);
+			break;
+		}
 
-	count -= dn.ofs_in_node;
-	f2fs_bug_on(sbi, count < 0);
+		if (time_to_inject(sbi, FAULT_DEDUP_CLONE))
+			newpage = NULL;
+		else
+			newpage = f2fs_grab_cache_page(dst_mapping, copy_idx, true);
+		if (!newpage) {
+			ret = -ENOMEM;
+			f2fs_put_page(page, 1);
+			break;
+		}
+		memcpy_page(newpage, 0, page, 0, PAGE_SIZE);
 
-	if (dn.ofs_in_node || IS_INODE(dn.node_page)) {
-		f2fs_truncate_data_blocks_range(&dn, count);
-		free_from += count;
+		set_page_dirty(newpage);
+		f2fs_put_page(newpage, 1);
+		f2fs_put_page(page, 1);
 	}
 
-	f2fs_put_dnode(&dn);
-free_next:
-	err = f2fs_truncate_inode_blocks(inode, free_from);
 out:
-	if (lock)
-		f2fs_unlock_op(sbi);
-free_partial:
-	/* lastly zero out the first data page */
-	if (!err)
-		err = truncate_partial_data_page(inode, from, truncate_page);
+	while (!list_empty(&pages))
+		FREE_FIRST_PAGE_IN_LIST(pages);
 
-	trace_f2fs_truncate_blocks_exit(inode, err);
-	return err;
+	return ret;
 }
 
-int f2fs_truncate_blocks(struct inode *inode, u64 from, bool lock)
+static int f2fs_clone_data(struct inode *dst_inode,
+		struct inode *src_inode)
 {
-	u64 free_from = from;
-	int err;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
+	pgoff_t page_idx = 0, last_idx;
+	int blk_per_seg = sbi->blocks_per_seg;
+	int count;
+	int ret = 0;
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	/*
-	 * for compressed file, only support cluster size
-	 * aligned truncation.
-	 */
-	if (f2fs_compressed_file(inode))
-		free_from = round_up(from,
-				F2FS_I(inode)->i_cluster_size << PAGE_SHIFT);
-#endif
+	f2fs_balance_fs(sbi, true);
+	last_idx = DIV_ROUND_UP(i_size_read(src_inode), PAGE_SIZE);
+	count = last_idx - page_idx;
 
-	err = f2fs_do_truncate_blocks(inode, free_from, lock);
-	if (err)
-		return err;
+	while (count) {
+		int len = min(blk_per_seg, count);
+		ret = f2fs_copy_data(dst_inode, src_inode, page_idx, len);
+		if (ret < 0)
+			break;
 
-#ifdef CONFIG_F2FS_FS_COMPRESSION
-	/*
-	 * For compressed file, after release compress blocks, don't allow write
-	 * direct, but we should allow write direct after truncate to zero.
-	 */
-	if (f2fs_compressed_file(inode) && !free_from
-			&& is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
-		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
+		filemap_fdatawrite(dst_inode->i_mapping);
+		count -= len;
+		page_idx += len;
+	}
 
-	if (from != free_from) {
-		err = f2fs_truncate_partial_cluster(inode, from, lock);
-		if (err)
-			return err;
+	if (!ret)
+		ret = f2fs_filemap_write_and_wait_range(dst_inode);
+
+	return ret;
+}
+
+static int __f2fs_ioc_clone_file(struct inode *dst_inode,
+		struct inode *src_inode, struct f2fs_clone_info *info)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
+	int ret;
+
+	ret = f2fs_convert_inline_inode(dst_inode);
+	if (ret) {
+		f2fs_err(sbi,
+			"inode[%lu] convert inline inode failed, ret:%d",
+			dst_inode->i_ino, ret);
+		return ret;
+	}
+
+	if (info->flags & DEDUP_CLONE_META) {
+		dst_inode->i_uid = src_inode->i_uid;
+		dst_inode->i_gid = src_inode->i_gid;
+		dst_inode->i_size = src_inode->i_size;
+	}
+
+	if (info->flags & DEDUP_CLONE_DATA) {
+		dst_inode->i_size = src_inode->i_size;
+		ret = f2fs_clone_data(dst_inode, src_inode);
+		if (ret) {
+			/* No need to truncate, beacuse tmpfile will be removed. */
+			f2fs_err(sbi,
+				"src inode[%lu] dst inode[%lu] ioc clone failed. ret=%d",
+				src_inode->i_ino, dst_inode->i_ino, ret);
+			return ret;
+		}
 	}
-#endif
+
+	set_inode_flag(dst_inode, FI_DATA_UN_MODIFY);
 
 	return 0;
 }
 
-int f2fs_truncate(struct inode *inode)
+static int f2fs_ioc_clone_file(struct file *filp, unsigned long arg)
 {
-	int err;
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct inode *src_inode;
+	struct f2fs_clone_info info;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct fd f;
+	int ret = 0;
 
 	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
 		return -EIO;
 
-	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
-				S_ISLNK(inode->i_mode)))
-		return 0;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	trace_f2fs_truncate(inode);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	if (time_to_inject(F2FS_I_SB(inode), FAULT_TRUNCATE))
-		return -EIO;
+	if (copy_from_user(&info, (struct f2fs_clone_info __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-	err = f2fs_dquot_initialize(inode);
-	if (err)
-		return err;
+	f = fdget_pos(info.src_fd);
+	if (!f.file)
+		return -EBADF;
 
-	/* we should check inline_data size */
-	if (!f2fs_may_inline_data(inode)) {
-		err = f2fs_convert_inline_inode(inode);
-		if (err)
-			return err;
+	src_inode = file_inode(f.file);
+	if (inode->i_sb != src_inode->i_sb) {
+		f2fs_err(sbi, "%s: files should be in same FS ino:%lu, src_ino:%lu",
+				__func__, inode->i_ino, src_inode->i_ino);
+		ret = -EINVAL;
+		goto out;
 	}
 
-	err = f2fs_truncate_blocks(inode, i_size_read(inode), true);
-	if (err)
-		return err;
+	if (!f2fs_inode_support_dedup(sbi, src_inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		goto out;
+
+	inode_lock(inode);
+	ret = f2fs_dedup_param_check(sbi, src_inode, info.flags &
+		DEDUP_FOR_SNAPSHOT ? NORMAL_INODE : OUTER_INODE,
+			inode, INNER_INODE);
+	if (ret)
+		goto unlock;
+
+	ret = __f2fs_ioc_clone_file(inode, src_inode, &info);
+	if (ret)
+		goto unlock;
+
+	F2FS_I(inode)->i_flags |= F2FS_IMMUTABLE_FL;
+	f2fs_set_inode_flags(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
+
+unlock:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+out:
+	fdput_pos(f);
+	return ret;
+}
+
+static inline void _truncate_error_handle(struct inode *inode,
+		int ret)
+{
+	set_sbi_flag(F2FS_I_SB(inode), SBI_NEED_FSCK);
+	f2fs_err(F2FS_I_SB(inode),
+		"truncate data failed, inode:%lu ret:%d",
+		inode->i_ino, ret);
+}
+
+int f2fs_truncate_dedup_inode(struct inode *inode, unsigned int flag)
+{
+	int ret = 0;
+
+	if (!f2fs_is_outer_inode(inode)) {
+		f2fs_err(F2FS_I_SB(inode),
+			"inode:%lu is not dedup inode", inode->i_ino);
+		f2fs_bug_on(F2FS_I_SB(inode), 1);
+		return 0;
+	}
+
+	clear_inode_flag(inode, flag);
+
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_TRUNCATE)) {
+		ret = -EIO;
+		goto err;
+	}
+	ret = f2fs_truncate_blocks(inode, 0, false);
+	if (ret)
+		goto err;
+
+	ret = f2fs_set_inode_addr(inode, DEDUP_ADDR);
+	if (ret)
+		goto err;
 
-	inode->i_mtime = inode->i_ctime = current_time(inode);
-	f2fs_mark_inode_dirty_sync(inode, false);
 	return 0;
+err:
+	_truncate_error_handle(inode, ret);
+	return ret;
 }
 
-static bool f2fs_force_buffered_io(struct inode *inode, int rw)
+static int is_inode_match_dir_crypt_policy(struct dentry *dir,
+		struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (!fscrypt_dio_supported(inode))
-		return true;
-	if (fsverity_active(inode))
-		return true;
-	if (f2fs_compressed_file(inode))
-		return true;
-	if (f2fs_has_inline_data(inode))
-		return true;
+	if (time_to_inject(sbi, FAULT_DEDUP_CRYPT_POLICY))
+		return -EPERM;
 
-	/* disallow direct IO if any of devices has unaligned blksize */
-	if (f2fs_is_multi_device(sbi) && !sbi->aligned_blksize)
-		return true;
-	/*
-	 * for blkzoned device, fallback direct IO to buffered IO, so
-	 * all IOs can be serialized by log-structured write.
-	 */
-	if (f2fs_sb_has_blkzoned(sbi) && (rw == WRITE))
-		return true;
-	if (f2fs_lfs_mode(sbi) && rw == WRITE && F2FS_IO_ALIGNED(sbi))
-		return true;
-	if (is_sbi_flag_set(sbi, SBI_CP_DISABLED))
-		return true;
+	if (IS_ENCRYPTED(d_inode(dir)) &&
+		!fscrypt_has_permitted_context(d_inode(dir), inode)) {
+		f2fs_err(sbi, "inode[%lu] not match dir[%lu] fscrypt policy",
+			inode->i_ino, d_inode(dir)->i_ino);
+		return -EPERM;
+	}
 
-	return false;
+	return 0;
 }
 
-int f2fs_getattr(struct user_namespace *mnt_userns, const struct path *path,
-		 struct kstat *stat, u32 request_mask, unsigned int query_flags)
+static int deduped_files_match_fscrypt_policy(struct file *file1,
+		struct file *file2)
 {
-	struct inode *inode = d_inode(path->dentry);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_inode *ri = NULL;
-	unsigned int flags;
+	struct dentry *dir1 = dget_parent(file_dentry(file1));
+	struct dentry *dir2 = dget_parent(file_dentry(file2));
+	struct inode *inode1 = file_inode(file1);
+	struct inode *inode2 = file_inode(file2);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode1);
+	int err = 0;
 
-	if (f2fs_has_extra_attr(inode) &&
-			f2fs_sb_has_inode_crtime(F2FS_I_SB(inode)) &&
-			F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) {
-		stat->result_mask |= STATX_BTIME;
-		stat->btime.tv_sec = fi->i_crtime.tv_sec;
-		stat->btime.tv_nsec = fi->i_crtime.tv_nsec;
+	if (time_to_inject(sbi, FAULT_DEDUP_CRYPT_POLICY)) {
+		err = -EPERM;
+		goto out;
 	}
 
-	/*
-	 * Return the DIO alignment restrictions if requested.  We only return
-	 * this information when requested, since on encrypted files it might
-	 * take a fair bit of work to get if the file wasn't opened recently.
-	 *
-	 * f2fs sometimes supports DIO reads but not DIO writes.  STATX_DIOALIGN
-	 * cannot represent that, so in that case we report no DIO support.
-	 */
-	if ((request_mask & STATX_DIOALIGN) && S_ISREG(inode->i_mode)) {
-		unsigned int bsize = i_blocksize(inode);
+	if (IS_ENCRYPTED(d_inode(dir1)) &&
+		!fscrypt_has_permitted_context(d_inode(dir1), inode2)) {
+		f2fs_err(sbi, "dir[%lu] inode[%lu] and inode[%lu] fscrypt policy not match.",
+			d_inode(dir1)->i_ino, inode1->i_ino, inode2->i_ino);
+		err = -EPERM;
+		goto out;
+	}
 
-		stat->result_mask |= STATX_DIOALIGN;
-		if (!f2fs_force_buffered_io(inode, WRITE)) {
-			stat->dio_mem_align = bsize;
-			stat->dio_offset_align = bsize;
-		}
+	if (IS_ENCRYPTED(d_inode(dir2)) &&
+		!fscrypt_has_permitted_context(d_inode(dir2), inode1)) {
+		f2fs_err(sbi, "inode[%lu] and dir[%lu] inode[%lu] fscrypt policy not match.",
+			inode1->i_ino, d_inode(dir2)->i_ino, inode2->i_ino);
+		err = -EPERM;
 	}
 
-	flags = fi->i_flags;
-	if (flags & F2FS_COMPR_FL)
-		stat->attributes |= STATX_ATTR_COMPRESSED;
-	if (flags & F2FS_APPEND_FL)
-		stat->attributes |= STATX_ATTR_APPEND;
-	if (IS_ENCRYPTED(inode))
-		stat->attributes |= STATX_ATTR_ENCRYPTED;
-	if (flags & F2FS_IMMUTABLE_FL)
-		stat->attributes |= STATX_ATTR_IMMUTABLE;
-	if (flags & F2FS_NODUMP_FL)
-		stat->attributes |= STATX_ATTR_NODUMP;
-	if (IS_VERITY(inode))
-		stat->attributes |= STATX_ATTR_VERITY;
+out:
+	dput(dir2);
+	dput(dir1);
+	return err;
+}
 
-	stat->attributes_mask |= (STATX_ATTR_COMPRESSED |
-				  STATX_ATTR_APPEND |
-				  STATX_ATTR_ENCRYPTED |
-				  STATX_ATTR_IMMUTABLE |
-				  STATX_ATTR_NODUMP |
-				  STATX_ATTR_VERITY);
+static int f2fs_compare_page(struct page *src, struct page *dst)
+{
+	int ret;
+	char *src_kaddr = kmap_atomic(src);
+	char *dst_kaddr = kmap_atomic(dst);
 
-	generic_fillattr(mnt_userns, inode, stat);
+	flush_dcache_page(src);
+	flush_dcache_page(dst);
 
-	/* we need to show initial sectors used for inline_data/dentries */
-	if ((S_ISREG(inode->i_mode) && f2fs_has_inline_data(inode)) ||
-					f2fs_has_inline_dentry(inode))
-		stat->blocks += (stat->size + 511) >> 9;
+	ret = memcmp(dst_kaddr, src_kaddr, PAGE_SIZE);
+	kunmap_atomic(src_kaddr);
+	kunmap_atomic(dst_kaddr);
 
-	return 0;
+	return ret;
 }
 
-#ifdef CONFIG_F2FS_FS_POSIX_ACL
-static void __setattr_copy(struct user_namespace *mnt_userns,
-			   struct inode *inode, const struct iattr *attr)
+static inline void lock_two_pages(struct page *page1, struct page *page2)
 {
-	unsigned int ia_valid = attr->ia_valid;
-
-	i_uid_update(mnt_userns, attr, inode);
-	i_gid_update(mnt_userns, attr, inode);
-	if (ia_valid & ATTR_ATIME)
-		inode->i_atime = attr->ia_atime;
-	if (ia_valid & ATTR_MTIME)
-		inode->i_mtime = attr->ia_mtime;
-	if (ia_valid & ATTR_CTIME)
-		inode->i_ctime = attr->ia_ctime;
-	if (ia_valid & ATTR_MODE) {
-		umode_t mode = attr->ia_mode;
-		vfsgid_t vfsgid = i_gid_into_vfsgid(mnt_userns, inode);
-
-		if (!vfsgid_in_group_p(vfsgid) &&
-		    !capable_wrt_inode_uidgid(mnt_userns, inode, CAP_FSETID))
-			mode &= ~S_ISGID;
-		set_acl_inode(inode, mode);
-	}
+	lock_page(page1);
+	if (page1 != page2)
+		lock_page(page2);
 }
-#else
-#define __setattr_copy setattr_copy
-#endif
 
-int f2fs_setattr(struct user_namespace *mnt_userns, struct dentry *dentry,
-		 struct iattr *attr)
+static inline void unlock_two_pages(struct page *page1, struct page *page2)
 {
-	struct inode *inode = d_inode(dentry);
-	int err;
-
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
-
-	if (unlikely(IS_IMMUTABLE(inode)))
-		return -EPERM;
+	unlock_page(page1);
+	if (page1 != page2)
+		unlock_page(page2);
+}
 
-	if (unlikely(IS_APPEND(inode) &&
-			(attr->ia_valid & (ATTR_MODE | ATTR_UID |
-				  ATTR_GID | ATTR_TIMES_SET))))
-		return -EPERM;
+static bool dedup_file_is_same(struct inode *src, struct inode *dst, int nr_pages)
+{
+	struct page *src_page, *dst_page;
+	pgoff_t index, last_idx;
+	int i, ret;
+	bool same = true;
 
-	if ((attr->ia_valid & ATTR_SIZE) &&
-		!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	if (time_to_inject(F2FS_I_SB(src), FAULT_DEDUP_SAME_FILE))
+		return false;
 
-	err = setattr_prepare(mnt_userns, dentry, attr);
-	if (err)
-		return err;
+	if (i_size_read(src) != i_size_read(dst))
+		return false;
 
-	err = fscrypt_prepare_setattr(dentry, attr);
-	if (err)
-		return err;
+	last_idx = DIV_ROUND_UP(i_size_read(src), PAGE_SIZE);
 
-	err = fsverity_prepare_setattr(dentry, attr);
-	if (err)
-		return err;
+	for (i = 0; i < nr_pages; i++) {
+		index = get_random_u32() % last_idx;
 
-	if (is_quota_modification(mnt_userns, inode, attr)) {
-		err = f2fs_dquot_initialize(inode);
-		if (err)
-			return err;
-	}
-	if (i_uid_needs_update(mnt_userns, attr, inode) ||
-	    i_gid_needs_update(mnt_userns, attr, inode)) {
-		f2fs_lock_op(F2FS_I_SB(inode));
-		err = dquot_transfer(mnt_userns, inode, attr);
-		if (err) {
-			set_sbi_flag(F2FS_I_SB(inode),
-					SBI_QUOTA_NEED_REPAIR);
-			f2fs_unlock_op(F2FS_I_SB(inode));
-			return err;
+		src_page = read_mapping_page(src->i_mapping, index, NULL);
+		if (IS_ERR(src_page)) {
+			ret = PTR_ERR(src_page);
+			same = false;
+			break;
 		}
-		/*
-		 * update uid/gid under lock_op(), so that dquot and inode can
-		 * be updated atomically.
-		 */
-		i_uid_update(mnt_userns, attr, inode);
-		i_gid_update(mnt_userns, attr, inode);
-		f2fs_mark_inode_dirty_sync(inode, true);
-		f2fs_unlock_op(F2FS_I_SB(inode));
-	}
-
-	if (attr->ia_valid & ATTR_SIZE) {
-		loff_t old_size = i_size_read(inode);
 
-		if (attr->ia_size > MAX_INLINE_DATA(inode)) {
-			/*
-			 * should convert inline inode before i_size_write to
-			 * keep smaller than inline_data size with inline flag.
-			 */
-			err = f2fs_convert_inline_inode(inode);
-			if (err)
-				return err;
+		dst_page = read_mapping_page(dst->i_mapping, index, NULL);
+		if (IS_ERR(dst_page)) {
+			ret = PTR_ERR(dst_page);
+			put_page(src_page);
+			same = false;
+			break;
 		}
 
-		f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-		filemap_invalidate_lock(inode->i_mapping);
-
-		truncate_setsize(inode, attr->ia_size);
-
-		if (attr->ia_size <= old_size)
-			err = f2fs_truncate(inode);
-		/*
-		 * do not trim all blocks after i_size if target size is
-		 * larger than i_size.
-		 */
-		filemap_invalidate_unlock(inode->i_mapping);
-		f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-		if (err)
-			return err;
-
-		spin_lock(&F2FS_I(inode)->i_size_lock);
-		inode->i_mtime = inode->i_ctime = current_time(inode);
-		F2FS_I(inode)->last_disk_size = i_size_read(inode);
-		spin_unlock(&F2FS_I(inode)->i_size_lock);
+		lock_two_pages(src_page, dst_page);
+		if (!PageUptodate(src_page) || !PageUptodate(dst_page) ||
+				src_page->mapping != src->i_mapping ||
+				dst_page->mapping != dst->i_mapping) {
+			ret = -EINVAL;
+			same = false;
+			goto unlock;
+		}
+		ret = f2fs_compare_page(src_page, dst_page);
+		if (ret)
+			same = false;
+unlock:
+		unlock_two_pages(src_page, dst_page);
+		put_page(dst_page);
+		put_page(src_page);
+
+		if (!same) {
+			f2fs_err(F2FS_I_SB(src),
+					"src: %lu, dst: %lu page index: %lu is diff ret[%d]",
+					src->i_ino, dst->i_ino, index, ret);
+			break;
+		}
 	}
 
-	__setattr_copy(mnt_userns, inode, attr);
+	return same;
+}
 
-	if (attr->ia_valid & ATTR_MODE) {
-		err = posix_acl_chmod(mnt_userns, inode, f2fs_get_inode_mode(inode));
+int f2fs_filemap_write_and_wait_range(struct inode *inode)
+{
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_WRITEBACK)) {
+		int ret;
 
-		if (is_inode_flag_set(inode, FI_ACL_MODE)) {
-			if (!err)
-				inode->i_mode = F2FS_I(inode)->i_acl_mode;
-			clear_inode_flag(inode, FI_ACL_MODE);
-		}
+		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+						i_size_read(inode) / 2);
+		if (!ret)
+			ret = -EIO;
+		return ret;
 	}
 
-	/* file size may changed here */
-	f2fs_mark_inode_dirty_sync(inode, true);
-
-	/* inode change will produce dirty node pages flushed by checkpoint */
-	f2fs_balance_fs(F2FS_I_SB(inode), true);
-
-	return err;
+	return filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
 }
 
-const struct inode_operations f2fs_file_inode_operations = {
-	.getattr	= f2fs_getattr,
-	.setattr	= f2fs_setattr,
-	.get_acl	= f2fs_get_acl,
-	.set_acl	= f2fs_set_acl,
-	.listxattr	= f2fs_listxattr,
-	.fiemap		= f2fs_fiemap,
-	.fileattr_get	= f2fs_fileattr_get,
-	.fileattr_set	= f2fs_fileattr_set,
-};
-
-static int fill_zero(struct inode *inode, pgoff_t index,
-					loff_t start, loff_t len)
+static int __f2fs_ioc_create_layered_inode(struct inode *inode,
+		struct inode *inner_inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct page *page;
-
-	if (!len)
-		return 0;
+	int ret;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 
-	f2fs_balance_fs(sbi, true);
+	if (!dedup_file_is_same(inode, inner_inode, DEDUP_COMPARE_PAGES))
+		return -ESTALE;
 
 	f2fs_lock_op(sbi);
-	page = f2fs_get_new_data_page(inode, NULL, index, false);
-	f2fs_unlock_op(sbi);
-
-	if (IS_ERR(page))
-		return PTR_ERR(page);
 
-	f2fs_wait_on_page_writeback(page, DATA, true, true);
-	zero_user(page, start, len);
-	set_page_dirty(page);
-	f2fs_put_page(page, 1);
-	return 0;
-}
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		ret = -ENOSPC;
+	else
+		ret = f2fs_acquire_orphan_inode(sbi);
+	if (ret) {
+		f2fs_err(sbi,
+			"create layer file acquire orphan fail, ino[%lu], inner[%lu]",
+			inode->i_ino, inner_inode->i_ino);
+		f2fs_unlock_op(sbi);
+		return ret;
+	}
+	f2fs_add_orphan_inode(inode);
 
-int f2fs_truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end)
-{
-	int err;
+	f2fs_down_write(&F2FS_I(inner_inode)->i_sem);
+	igrab(inner_inode);
+	set_inode_flag(inner_inode, FI_INNER_INODE);
+	set_inode_flag(inner_inode, FI_DEDUPED);
+	if (is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		set_inode_flag(inner_inode, FI_SNAPSHOT_PREPARED);
+	f2fs_i_links_write(inner_inode, true);
+	f2fs_up_write(&F2FS_I(inner_inode)->i_sem);
 
-	while (pg_start < pg_end) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
+	f2fs_down_write(&fi->i_sem);
+	fi->inner_inode = inner_inode;
+	set_inode_flag(inode, FI_DEDUPED);
+	//set_inode_flag(inode, FI_DOING_DEDUP);
+	f2fs_up_write(&fi->i_sem);
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		err = f2fs_get_dnode_of_data(&dn, pg_start, LOOKUP_NODE);
-		if (err) {
-			if (err == -ENOENT) {
-				pg_start = f2fs_get_next_page_offset(&dn,
-								pg_start);
-				continue;
-			}
-			return err;
-		}
+	f2fs_remove_orphan_inode(sbi, inner_inode->i_ino);
+	f2fs_unlock_op(sbi);
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, pg_end - pg_start);
+	wait_event(fi->dedup_wq,
+			atomic_read(&fi->inflight_read_io) == 0);
+	dedup_wait_dio(inode);
 
-		f2fs_bug_on(F2FS_I_SB(inode), count == 0 || count > end_offset);
+	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+	/* GC may dirty pages before holding lock */
+	ret = f2fs_filemap_write_and_wait_range(inode);
+	if (ret)
+		goto out;
 
-		f2fs_truncate_data_blocks_range(&dn, count);
-		f2fs_put_dnode(&dn);
+	f2fs_lock_op(sbi);
+	f2fs_remove_orphan_inode(sbi, inode->i_ino);
+	ret = f2fs_truncate_dedup_inode(inode, FI_DOING_DEDUP);
+	/*
+	 * Since system may do checkpoint after unlock cp,
+	 * we set cp_ver here to let fsync know dedup have finish.
+	 */
+	F2FS_I(inode)->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_unlock_op(sbi);
 
-		pg_start += count;
-	}
-	return 0;
+out:
+	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+	f2fs_dedup_info(sbi, "inode[%lu] create layered success, inner[%lu] ret: %d",
+			inode->i_ino, inner_inode->i_ino, ret);
+	trace_f2fs_dedup_ioc_create_layered_inode(inode, inner_inode);
+	return ret;
 }
 
-static int f2fs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
+static int f2fs_ioc_create_layered_inode(struct file *filp, unsigned long arg)
 {
-	pgoff_t pg_start, pg_end;
-	loff_t off_start, off_end;
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct inode *inner_inode;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_dedup_src info;
+	struct dentry *dir;
+	struct fd f;
 	int ret;
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		return ret;
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
 
-	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
-	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	off_start = offset & (PAGE_SIZE - 1);
-	off_end = (offset + len) & (PAGE_SIZE - 1);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	if (pg_start == pg_end) {
-		ret = fill_zero(inode, pg_start, off_start,
-						off_end - off_start);
-		if (ret)
-			return ret;
-	} else {
-		if (off_start) {
-			ret = fill_zero(inode, pg_start++, off_start,
-						PAGE_SIZE - off_start);
-			if (ret)
-				return ret;
-		}
-		if (off_end) {
-			ret = fill_zero(inode, pg_end, 0, off_end);
-			if (ret)
-				return ret;
-		}
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-		if (pg_start < pg_end) {
-			loff_t blk_start, blk_end;
-			struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	if (copy_from_user(&info, (struct f2fs_dedup_src __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-			f2fs_balance_fs(sbi, true);
+	f = fdget_pos(info.inner_fd);
+	if (!f.file)
+		return -EBADF;
 
-			blk_start = (loff_t)pg_start << PAGE_SHIFT;
-			blk_end = (loff_t)pg_end << PAGE_SHIFT;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		goto out;
 
-			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-			filemap_invalidate_lock(inode->i_mapping);
+	inode_lock(inode);
+	if (f2fs_is_deduped_inode(inode)) {
+		f2fs_err(sbi, "The inode[%lu] has been two layer file.",
+			inode->i_ino);
+		ret = -EINVAL;
+		goto unlock;
+	}
 
-			truncate_pagecache_range(inode, blk_start, blk_end - 1);
+	inner_inode = file_inode(f.file);
+	if (inode->i_sb != inner_inode->i_sb) {
+		f2fs_err(sbi, "%s files should be in same FS ino:%lu, inner_ino:%lu",
+				__func__, inode->i_ino, inner_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock;
+	}
 
-			f2fs_lock_op(sbi);
-			ret = f2fs_truncate_hole(inode, pg_start, pg_end);
-			f2fs_unlock_op(sbi);
+	if (!IS_IMMUTABLE(inner_inode)) {
+		f2fs_err(sbi, "create layer fail inner[%lu] is not immutable.",
+			inner_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock;
+	}
 
-			filemap_invalidate_unlock(inode->i_mapping);
-			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-		}
+	ret = f2fs_dedup_param_check(sbi, inode, OUTER_INODE,
+			inner_inode, INNER_INODE);
+	if (ret)
+		goto unlock;
+
+	if (inode->i_nlink == 0) {
+		f2fs_err(sbi,
+			"The inode[%lu] has been removed.", inode->i_ino);
+		ret = -ENOENT;
+		goto unlock;
 	}
 
+	dir = dget_parent(file_dentry(filp));
+	ret = is_inode_match_dir_crypt_policy(dir, inner_inode);
+	dput(dir);
+	if (ret)
+		goto unlock;
+
+	filemap_fdatawrite(inode->i_mapping);
+	ret = f2fs_filemap_write_and_wait_range(inode);
+	if (ret)
+		goto unlock;
+
+	ret = __f2fs_ioc_create_layered_inode(inode, inner_inode);
+
+unlock:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+out:
+	fdput_pos(f);
 	return ret;
 }
 
-static int __read_out_blkaddrs(struct inode *inode, block_t *blkaddr,
-				int *do_replace, pgoff_t off, pgoff_t len)
+static int __f2fs_ioc_dedup_file(struct inode *base_inode,
+		struct inode *dedup_inode)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	int ret, done, i;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup_inode);
+	struct inode *inner = get_inner_inode(base_inode);
+	int ret = 0;
+	struct f2fs_inode_info *fi = F2FS_I(dedup_inode);
 
-next_dnode:
-	set_new_dnode(&dn, inode, NULL, NULL, 0);
-	ret = f2fs_get_dnode_of_data(&dn, off, LOOKUP_NODE_RA);
-	if (ret && ret != -ENOENT) {
+	if (!inner)
+		return -EBADF;
+
+	if (is_inode_flag_set(dedup_inode, FI_SNAPSHOTED))
+		goto skip;
+	if (!dedup_file_is_same(base_inode, dedup_inode, DEDUP_COMPARE_PAGES)) {
+		put_inner_inode(inner);
+		return -ESTALE;
+	}
+skip:
+	f2fs_lock_op(sbi);
+
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		ret = -ENOSPC;
+	else
+		ret = f2fs_acquire_orphan_inode(sbi);
+	if (ret) {
+		f2fs_unlock_op(sbi);
+		f2fs_err(sbi,
+			"dedup file acquire orphan fail, ino[%lu], base ino[%lu]",
+			dedup_inode->i_ino, base_inode->i_ino);
+		put_inner_inode(inner);
 		return ret;
-	} else if (ret == -ENOENT) {
-		if (dn.max_level == 0)
-			return -ENOENT;
-		done = min((pgoff_t)ADDRS_PER_BLOCK(inode) -
-						dn.ofs_in_node, len);
-		blkaddr += done;
-		do_replace += done;
-		goto next;
 	}
+	f2fs_add_orphan_inode(dedup_inode);
 
-	done = min((pgoff_t)ADDRS_PER_PAGE(dn.node_page, inode) -
-							dn.ofs_in_node, len);
-	for (i = 0; i < done; i++, blkaddr++, do_replace++, dn.ofs_in_node++) {
-		*blkaddr = f2fs_data_blkaddr(&dn);
+	f2fs_down_write(&fi->i_sem);
+	fi->inner_inode = inner;
+	set_inode_flag(dedup_inode, FI_DEDUPED);
+	set_inode_flag(dedup_inode, FI_DOING_DEDUP);
+	f2fs_up_write(&fi->i_sem);
 
-		if (__is_valid_data_blkaddr(*blkaddr) &&
-			!f2fs_is_valid_blkaddr(sbi, *blkaddr,
-					DATA_GENERIC_ENHANCE)) {
-			f2fs_put_dnode(&dn);
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			return -EFSCORRUPTED;
-		}
+	f2fs_down_write(&F2FS_I(inner)->i_sem);
+	f2fs_i_links_write(inner, true);
+	f2fs_up_write(&F2FS_I(inner)->i_sem);
+	f2fs_unlock_op(sbi);
 
-		if (!f2fs_is_checkpointed_data(sbi, *blkaddr)) {
+	wait_event(fi->dedup_wq,
+			atomic_read(&fi->inflight_read_io) == 0);
+	dedup_wait_dio(dedup_inode);
 
-			if (f2fs_lfs_mode(sbi)) {
-				f2fs_put_dnode(&dn);
-				return -EOPNOTSUPP;
-			}
+	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+	/* GC may dirty pages before holding lock */
+	ret = f2fs_filemap_write_and_wait_range(dedup_inode);
+	if (ret)
+		goto out;
 
-			/* do not invalidate this block address */
-			f2fs_update_data_blkaddr(&dn, NULL_ADDR);
-			*do_replace = 1;
-		}
-	}
-	f2fs_put_dnode(&dn);
-next:
-	len -= done;
-	off += done;
-	if (len)
-		goto next_dnode;
-	return 0;
+	f2fs_lock_op(sbi);
+	f2fs_remove_orphan_inode(sbi, dedup_inode->i_ino);
+	ret = f2fs_truncate_dedup_inode(dedup_inode, FI_DOING_DEDUP);
+	/*
+	 * Since system may do checkpoint after unlock cp,
+	 * we set cp_ver here to let fsync know dedup have finish.
+	 */
+	F2FS_I(dedup_inode)->dedup_cp_ver = cur_cp_version(F2FS_CKPT(sbi));
+	f2fs_unlock_op(sbi);
+out:
+	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+	f2fs_dedup_info(sbi, "%s inode[%lu] dedup success, inner[%lu], ret[%d]",
+			__func__, dedup_inode->i_ino, inner->i_ino, ret);
+	trace_f2fs_dedup_ioc_dedup_inode(dedup_inode, inner);
+	return ret;
 }
 
-static int __roll_back_blkaddrs(struct inode *inode, block_t *blkaddr,
-				int *do_replace, pgoff_t off, int len)
+static int f2fs_ioc_dedup_file(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct dnode_of_data dn;
-	int ret, i;
+	struct inode *dedup_inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct inode *base_inode, *inner_inode;
+	struct dentry *dir;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dedup_inode);
+	struct f2fs_dedup_dst info;
+	struct fd f;
+	int ret;
 
-	for (i = 0; i < len; i++, do_replace++, blkaddr++) {
-		if (*do_replace == 0)
-			continue;
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(dedup_inode))))
+		return -EIO;
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, off + i, LOOKUP_NODE_RA);
-		if (ret) {
-			dec_valid_block_count(sbi, inode, 1);
-			f2fs_invalidate_blocks(sbi, *blkaddr, 1);
-		} else {
-			f2fs_update_data_blkaddr(&dn, *blkaddr);
-		}
-		f2fs_put_dnode(&dn);
-	}
-	return 0;
-}
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-static int __clone_blkaddrs(struct inode *src_inode, struct inode *dst_inode,
-			block_t *blkaddr, int *do_replace,
-			pgoff_t src, pgoff_t dst, pgoff_t len, bool full)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
-	pgoff_t i = 0;
-	int ret;
+	if (!inode_owner_or_capable(mnt_userns, dedup_inode))
+		return -EACCES;
 
-	while (i < len) {
-		if (blkaddr[i] == NULL_ADDR && !full) {
-			i++;
-			continue;
-		}
+	if (!f2fs_inode_support_dedup(sbi, dedup_inode))
+		return -EOPNOTSUPP;
 
-		if (do_replace[i] || blkaddr[i] == NULL_ADDR) {
-			struct dnode_of_data dn;
-			struct node_info ni;
-			size_t new_size;
-			pgoff_t ilen;
+	if (copy_from_user(&info, (struct f2fs_dedup_dst __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-			set_new_dnode(&dn, dst_inode, NULL, NULL, 0);
-			ret = f2fs_get_dnode_of_data(&dn, dst + i, ALLOC_NODE);
-			if (ret)
-				return ret;
+	f = fdget_pos(info.base_fd);
+	if (!f.file)
+		return -EBADF;
 
-			ret = f2fs_get_node_info(sbi, dn.nid, &ni, false);
-			if (ret) {
-				f2fs_put_dnode(&dn);
-				return ret;
-			}
+	base_inode = file_inode(f.file);
+	if (dedup_inode->i_sb != base_inode->i_sb) {
+		f2fs_err(sbi, "%s: files should be in same FS ino:%lu, base_ino:%lu",
+				__func__, dedup_inode->i_ino, base_inode->i_ino);
+		ret = -EINVAL;
+		goto out;
+	}
 
-			ilen = min((pgoff_t)
-				ADDRS_PER_PAGE(dn.node_page, dst_inode) -
-						dn.ofs_in_node, len - i);
-			do {
-				dn.data_blkaddr = f2fs_data_blkaddr(&dn);
-				f2fs_truncate_data_blocks_range(&dn, 1);
+	if (!f2fs_inode_support_dedup(sbi, base_inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
 
-				if (do_replace[i]) {
-					f2fs_i_blocks_write(src_inode,
-							1, false, false);
-					f2fs_i_blocks_write(dst_inode,
-							1, true, false);
-					f2fs_replace_block(sbi, &dn, dn.data_blkaddr,
-					blkaddr[i], ni.version, true, false);
+	if (base_inode == dedup_inode) {
+		f2fs_err(sbi, "%s: input inode[%lu] and [%lu] are same.",
+			__func__, base_inode->i_ino, dedup_inode->i_ino);
+		ret = -EINVAL;
+		goto out;
+	}
 
-					do_replace[i] = 0;
-				}
-				dn.ofs_in_node++;
-				i++;
-				new_size = (loff_t)(dst + i) << PAGE_SHIFT;
-				if (dst_inode->i_size < new_size)
-					f2fs_i_size_write(dst_inode, new_size);
-			} while (--ilen && (do_replace[i] || blkaddr[i] == NULL_ADDR));
+	// if try to dedup, clear FI_SNAPSHOT_PREPARED of base inode, else set for snapshot
+	if (is_inode_flag_set(dedup_inode, FI_SNAPSHOTED)) {
+		set_inode_flag(base_inode, FI_SNAPSHOT_PREPARED);
+	} else {
+		clear_inode_flag(base_inode, FI_SNAPSHOT_PREPARED);
+	}
 
-			f2fs_put_dnode(&dn);
-		} else {
-			struct page *psrc, *pdst;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		goto out;
 
-			psrc = f2fs_get_lock_data_page(src_inode,
-							src + i, true);
-			if (IS_ERR(psrc))
-				return PTR_ERR(psrc);
-			pdst = f2fs_get_new_data_page(dst_inode, NULL, dst + i,
-								true);
-			if (IS_ERR(pdst)) {
-				f2fs_put_page(psrc, 1);
-				return PTR_ERR(pdst);
-			}
-			memcpy_page(pdst, 0, psrc, 0, PAGE_SIZE);
-			set_page_dirty(pdst);
-			f2fs_put_page(pdst, 1);
-			f2fs_put_page(psrc, 1);
+	// dedup_inode lock already hold if create snapshot
+	if (!is_inode_flag_set(dedup_inode, FI_SNAPSHOTED))
+		inode_lock(dedup_inode);
+	if (!inode_trylock(base_inode)) {
+		f2fs_err(sbi, "inode[%lu] can't get lock", base_inode->i_ino);
+		ret = -EAGAIN;
+		goto unlock2;
+	}
 
-			ret = f2fs_truncate_hole(src_inode,
-						src + i, src + i + 1);
-			if (ret)
-				return ret;
-			i++;
-		}
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(base_inode) ||
+		f2fs_seqzone_file(dedup_inode)) {
+		ret = -EINVAL;
+		goto unlock1;
+	}
+#endif
+	if (f2fs_is_deduped_inode(dedup_inode)) {
+		f2fs_err(sbi, "dedup inode[%lu] has been two layer inode",
+			dedup_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock1;
 	}
-	return 0;
-}
 
-static int __exchange_data_block(struct inode *src_inode,
-			struct inode *dst_inode, pgoff_t src, pgoff_t dst,
-			pgoff_t len, bool full)
-{
-	block_t *src_blkaddr;
-	int *do_replace;
-	pgoff_t olen;
-	int ret;
+	if (dedup_inode->i_nlink == 0) {
+		f2fs_err(sbi,
+			"dedup inode[%lu] has been removed.", dedup_inode->i_ino);
+		ret = -ENOENT;
+		goto unlock1;
+	}
 
-	while (len) {
-		olen = min((pgoff_t)4 * ADDRS_PER_BLOCK(src_inode), len);
+	if (!f2fs_is_outer_inode(base_inode)) {
+		f2fs_err(sbi, "base inode[%lu] is not outer inode",
+			base_inode->i_ino);
+		ret = -EINVAL;
+		goto unlock1;
+	}
 
-		src_blkaddr = f2fs_kvzalloc(F2FS_I_SB(src_inode),
-					array_size(olen, sizeof(block_t)),
-					GFP_NOFS);
-		if (!src_blkaddr)
-			return -ENOMEM;
-
-		do_replace = f2fs_kvzalloc(F2FS_I_SB(src_inode),
-					array_size(olen, sizeof(int)),
-					GFP_NOFS);
-		if (!do_replace) {
-			kvfree(src_blkaddr);
-			return -ENOMEM;
-		}
+	ret = f2fs_dedup_param_check(sbi, base_inode, OUTER_INODE,
+			dedup_inode, OUTER_INODE);
+	if (ret)
+		goto unlock1;
 
-		ret = __read_out_blkaddrs(src_inode, src_blkaddr,
-					do_replace, src, olen);
-		if (ret)
-			goto roll_back;
+	dir = dget_parent(file_dentry(filp));
+	inner_inode = get_inner_inode(base_inode);
+	if (likely(inner_inode)) {
+		ret = is_inode_match_dir_crypt_policy(dir, inner_inode);
+		put_inner_inode(inner_inode);
+	}
+	dput(dir);
+	if (ret)
+		goto unlock1;
 
-		ret = __clone_blkaddrs(src_inode, dst_inode, src_blkaddr,
-					do_replace, src, dst, olen, full);
-		if (ret)
-			goto roll_back;
+	ret = deduped_files_match_fscrypt_policy(filp, f.file);
+	if (ret)
+		goto unlock1;
 
-		src += olen;
-		dst += olen;
-		len -= olen;
+	filemap_fdatawrite(dedup_inode->i_mapping);
+	ret = f2fs_filemap_write_and_wait_range(dedup_inode);
+	if (ret)
+		goto unlock1;
 
-		kvfree(src_blkaddr);
-		kvfree(do_replace);
-	}
-	return 0;
+	ret = __f2fs_ioc_dedup_file(base_inode, dedup_inode);
 
-roll_back:
-	__roll_back_blkaddrs(src_inode, src_blkaddr, do_replace, src, olen);
-	kvfree(src_blkaddr);
-	kvfree(do_replace);
+unlock1:
+	inode_unlock(base_inode);
+unlock2:
+	if (!is_inode_flag_set(dedup_inode, FI_SNAPSHOTED))
+		inode_unlock(dedup_inode);
+	mnt_drop_write_file(filp);
+out:
+	fdput_pos(f);
 	return ret;
 }
 
-static int f2fs_do_collapse(struct inode *inode, loff_t offset, loff_t len)
+static int f2fs_ioc_dedup_revoke(struct file *filp, unsigned long arg)
 {
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	pgoff_t nrpages = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-	pgoff_t start = offset >> PAGE_SHIFT;
-	pgoff_t end = (offset + len) >> PAGE_SHIFT;
-	int ret;
+	int ret = 0;
 
-	f2fs_balance_fs(sbi, true);
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
 
-	/* avoid gc operation during block exchange */
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	f2fs_lock_op(sbi);
-	f2fs_drop_extent_tree(inode);
-	truncate_pagecache(inode, offset);
-	ret = __exchange_data_block(inode, inode, end, start, nrpages - end, true);
-	f2fs_unlock_op(sbi);
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	inode_lock(inode);
+	ret = f2fs_revoke_deduped_inode(inode, __func__);
+	inode_unlock(inode);
+
+	mnt_drop_write_file(filp);
 	return ret;
 }
 
-static int f2fs_collapse_range(struct inode *inode, loff_t offset, loff_t len)
+static int f2fs_ioc_get_dedupd_file_info(struct file *filp, unsigned long arg)
 {
-	loff_t new_size;
-	int ret;
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_dedup_file_info info = {0};
+	struct inode *inner_inode;
+	int ret = 0;
 
-	if (offset + len >= i_size_read(inode))
-		return -EINVAL;
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
 
-	/* collapse range should be aligned to block size of f2fs. */
-	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
-		return -EINVAL;
+	if (!f2fs_inode_support_dedup(sbi, inode))
+		return -EOPNOTSUPP;
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		return ret;
+	inode_lock(inode);
 
-	/* write out all dirty pages from offset */
-	ret = filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
-	if (ret)
-		return ret;
+	if (!is_inode_flag_set(inode, FI_DEDUPED)) {
+		info.is_layered = false;
+		info.is_deduped = false;
+	} else {
+		info.is_layered = true;
+		inner_inode = F2FS_I(inode)->inner_inode;
+		if (inner_inode) {
+			f2fs_down_write(&F2FS_I(inner_inode)->i_sem);
+			if (inner_inode->i_nlink > 1)
+				info.is_deduped = true;
+
+			info.group = inner_inode->i_ino;
+			f2fs_up_write(&F2FS_I(inner_inode)->i_sem);
+		}
+	}
 
-	ret = f2fs_do_collapse(inode, offset, len);
-	if (ret)
-		return ret;
+	inode_unlock(inode);
 
-	/* write out all moved pages, if possible */
-	filemap_invalidate_lock(inode->i_mapping);
-	filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
-	truncate_pagecache(inode, offset);
+	if (copy_to_user((struct f2fs_dedup_file_info __user *)arg, &info, sizeof(info)))
+		ret = -EFAULT;
 
-	new_size = i_size_read(inode) - len;
-	ret = f2fs_truncate_blocks(inode, new_size, true);
-	filemap_invalidate_unlock(inode->i_mapping);
-	if (!ret)
-		f2fs_i_size_write(inode, new_size);
 	return ret;
 }
 
-static int f2fs_do_zero_range(struct dnode_of_data *dn, pgoff_t start,
-								pgoff_t end)
+/* used for dedup big data statistics */
+static int f2fs_ioc_get_dedup_sysinfo(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	pgoff_t index = start;
-	unsigned int ofs_in_node = dn->ofs_in_node;
-	blkcnt_t count = 0;
-	int ret;
-
-	for (; index < end; index++, dn->ofs_in_node++) {
-		if (f2fs_data_blkaddr(dn) == NULL_ADDR)
-			count++;
-	}
+	return 0;
+}
 
-	dn->ofs_in_node = ofs_in_node;
-	ret = f2fs_reserve_new_blocks(dn, count);
-	if (ret)
-		return ret;
+static int f2fs_ioc_create_snapshot(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	int ret = 0;
+	struct inode *base_inode;
+	struct f2fs_dedup_dst info;
+	struct fd f;
 
-	dn->ofs_in_node = ofs_in_node;
-	for (index = start; index < end; index++, dn->ofs_in_node++) {
-		dn->data_blkaddr = f2fs_data_blkaddr(dn);
-		/*
-		 * f2fs_reserve_new_blocks will not guarantee entire block
-		 * allocation.
-		 */
-		if (dn->data_blkaddr == NULL_ADDR) {
-			ret = -ENOSPC;
-			break;
-		}
+	if (copy_from_user(&info, (struct f2fs_dedup_dst __user *)arg, sizeof(info)))
+		return -EFAULT;
 
-		if (dn->data_blkaddr == NEW_ADDR)
-			continue;
+	inode_lock(inode);
 
-		if (!f2fs_is_valid_blkaddr(sbi, dn->data_blkaddr,
-					DATA_GENERIC_ENHANCE)) {
-			ret = -EFSCORRUPTED;
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			break;
+	//do not create snapshot for a snapshot file
+	if (is_inode_flag_set(inode, FI_SNAPSHOTED)) {
+		ret = -EOPNOTSUPP;
+		goto unlock;
+	}
+	set_inode_flag(inode, FI_SNAPSHOTED);
+	ret = f2fs_ioc_dedup_file(filp, arg);
+	if (!ret) {
+		f = fdget_pos(info.base_fd);
+		if (!f.file) {
+			ret = -EBADF;
+			goto unlock;
 		}
-
-		f2fs_invalidate_blocks(sbi, dn->data_blkaddr, 1);
-		f2fs_set_data_blkaddr(dn, NEW_ADDR);
+		base_inode = file_inode(f.file);
+		inode_lock(base_inode);
+		clear_inode_flag(base_inode, FI_SNAPSHOT_PREPARED);
+		//clone ctime/mtime... from base to snapshot
+		inode->i_atime.tv_sec = base_inode->i_atime.tv_sec;
+		inode->i_ctime.tv_sec = base_inode->i_ctime.tv_sec;
+		inode->i_mtime.tv_sec = base_inode->i_mtime.tv_sec;
+		inode->i_atime.tv_nsec = base_inode->i_atime.tv_nsec;
+		inode->i_ctime.tv_nsec = base_inode->i_ctime.tv_nsec;
+		inode->i_mtime.tv_nsec = base_inode->i_mtime.tv_nsec;
+		inode_unlock(base_inode);
+		fdput_pos(f);
+	} else {
+		clear_inode_flag(inode, FI_SNAPSHOTED);
 	}
 
-	f2fs_update_read_extent_cache_range(dn, start, 0, index - start);
-	f2fs_update_age_extent_cache_range(dn, start, index - start);
-
+unlock:
+	inode_unlock(inode);
 	return ret;
 }
 
-static int f2fs_zero_range(struct inode *inode, loff_t offset, loff_t len,
-								int mode)
+static int f2fs_ioc_prepare_snapshot(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct address_space *mapping = inode->i_mapping;
-	pgoff_t index, pg_start, pg_end;
-	loff_t new_size = i_size_read(inode);
-	loff_t off_start, off_end;
+	struct inode *inode = file_inode(filp);
 	int ret = 0;
 
-	ret = inode_newsize_ok(inode, (len + offset));
-	if (ret)
-		return ret;
-
-	ret = f2fs_convert_inline_inode(inode);
+	set_inode_flag(inode, FI_SNAPSHOT_PREPARED);
+	ret = f2fs_ioc_create_layered_inode(filp, arg);
 	if (ret)
-		return ret;
+		clear_inode_flag(inode, FI_SNAPSHOT_PREPARED);
+	return ret;
+}
 
-	ret = filemap_write_and_wait_range(mapping, offset, offset + len - 1);
-	if (ret)
-		return ret;
+#endif
+static vm_fault_t f2fs_filemap_fault(struct vm_fault *vmf)
+{
+	struct inode *inode = file_inode(vmf->vma->vm_file);
+	vm_fault_t ret;
 
-	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
-	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+	ret = filemap_fault(vmf);
+	if (!ret)
+		f2fs_update_iostat(F2FS_I_SB(inode), inode,
+					APP_MAPPED_READ_IO, F2FS_BLKSIZE);
 
-	off_start = offset & (PAGE_SIZE - 1);
-	off_end = (offset + len) & (PAGE_SIZE - 1);
+	trace_f2fs_filemap_fault(inode, vmf->pgoff, (unsigned long)ret);
 
-	if (pg_start == pg_end) {
-		ret = fill_zero(inode, pg_start, off_start,
-						off_end - off_start);
-		if (ret)
-			return ret;
+	return ret;
+}
 
-		new_size = max_t(loff_t, new_size, offset + len);
-	} else {
-		if (off_start) {
-			ret = fill_zero(inode, pg_start++, off_start,
-						PAGE_SIZE - off_start);
-			if (ret)
-				return ret;
+static vm_fault_t f2fs_vm_page_mkwrite(struct vm_fault *vmf)
+{
+	struct page *page = vmf->page;
+	struct inode *inode = file_inode(vmf->vma->vm_file);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	bool need_alloc = true;
+	int err = 0;
 
-			new_size = max_t(loff_t, new_size,
-					(loff_t)pg_start << PAGE_SHIFT);
-		}
+	if (unlikely(IS_IMMUTABLE(inode)))
+		return VM_FAULT_SIGBUS;
 
-		for (index = pg_start; index < pg_end;) {
-			struct dnode_of_data dn;
-			unsigned int end_offset;
-			pgoff_t end;
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		inode_lock(inode);
+		err = f2fs_reserve_compress_blocks(inode, NULL);
+		inode_unlock(inode);
+		if (err < 0)
+			goto err;
+#else
+		return VM_FAULT_SIGBUS;
+#endif
+	}
 
-			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-			filemap_invalidate_lock(mapping);
+	if (unlikely(f2fs_cp_error(sbi))) {
+		err = -EIO;
+		goto err;
+	}
 
-			truncate_pagecache_range(inode,
-				(loff_t)index << PAGE_SHIFT,
-				((loff_t)pg_end << PAGE_SHIFT) - 1);
+	if (!f2fs_is_checkpoint_ready(sbi)) {
+		err = -ENOSPC;
+		goto err;
+	}
 
-			f2fs_lock_op(sbi);
+	err = f2fs_convert_inline_inode(inode);
+	if (err)
+		goto err;
 
-			set_new_dnode(&dn, inode, NULL, NULL, 0);
-			ret = f2fs_get_dnode_of_data(&dn, index, ALLOC_NODE);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	inode_lock(inode);
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		err = f2fs_revoke_deduped_inode(inode, __func__);
+		if (err) {
+			inode_unlock(inode);
+			goto err;
+		}
+	}
+	inode_unlock(inode);
+#endif
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (f2fs_compressed_file(inode)) {
+		int ret;
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		ret = f2fs_is_compressed_cluster(inode, page->index);
+
+		if (ret < 0) {
+			err = ret;
+			goto err;
+		} else if (ret) {
+			need_alloc = false;
+		}
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		inode_lock(inode);
+		clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+		inode_unlock(inode);
+#endif
+	}
+#endif
+	/* should do out of any locked page */
+	if (need_alloc)
+		f2fs_balance_fs(sbi, true);
+
+	sb_start_pagefault(inode->i_sb);
+
+	f2fs_bug_on(sbi, f2fs_has_inline_data(inode));
+
+	file_update_time(vmf->vma->vm_file);
+	filemap_invalidate_lock_shared(inode->i_mapping);
+	lock_page(page);
+	if (unlikely(page->mapping != inode->i_mapping ||
+			page_offset(page) > i_size_read(inode) ||
+			!PageUptodate(page))) {
+		unlock_page(page);
+		err = -EFAULT;
+		goto out_sem;
+	}
+
+	if (need_alloc) {
+		/* block allocation */
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_block_locked(&dn, page->index);
+	}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	if (!need_alloc) {
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_dnode_of_data(&dn, page->index, LOOKUP_NODE);
+		f2fs_put_dnode(&dn);
+	}
+#endif
+	if (err) {
+		unlock_page(page);
+		goto out_sem;
+	}
+
+	f2fs_wait_on_page_writeback(page, DATA, false, true);
+
+	/* wait for GCed page writeback via META_MAPPING */
+	f2fs_wait_on_block_writeback(inode, dn.data_blkaddr);
+
+	/*
+	 * check to see if the page is mapped already (no holes)
+	 */
+	if (PageMappedToDisk(page))
+		goto out_sem;
+
+	/* page is wholly or partially inside EOF */
+	if (((loff_t)(page->index + 1) << PAGE_SHIFT) >
+						i_size_read(inode)) {
+		loff_t offset;
+
+		offset = i_size_read(inode) & ~PAGE_MASK;
+		zero_user_segment(page, offset, PAGE_SIZE);
+	}
+	set_page_dirty(page);
+	if (!PageUptodate(page))
+		SetPageUptodate(page);
+
+	f2fs_update_iostat(sbi, inode, APP_MAPPED_IO, F2FS_BLKSIZE);
+	f2fs_update_time(sbi, REQ_TIME);
+
+	trace_f2fs_vm_page_mkwrite(page, DATA);
+out_sem:
+	filemap_invalidate_unlock_shared(inode->i_mapping);
+
+	sb_end_pagefault(inode->i_sb);
+err:
+	return block_page_mkwrite_return(err);
+}
+
+static const struct vm_operations_struct f2fs_file_vm_ops = {
+	.fault		= f2fs_filemap_fault,
+	.map_pages	= filemap_map_pages,
+	.page_mkwrite	= f2fs_vm_page_mkwrite,
+};
+
+static int get_parent_ino(struct inode *inode, nid_t *pino)
+{
+	struct dentry *dentry;
+
+	/*
+	 * Make sure to get the non-deleted alias.  The alias associated with
+	 * the open file descriptor being fsync()'ed may be deleted already.
+	 */
+	dentry = d_find_alias(inode);
+	if (!dentry)
+		return 0;
+
+	*pino = parent_ino(dentry);
+	dput(dentry);
+	return 1;
+}
+
+static inline enum cp_reason_type need_do_checkpoint(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	enum cp_reason_type cp_reason = CP_NO_NEEDED;
+
+	if (!S_ISREG(inode->i_mode))
+		cp_reason = CP_NON_REGULAR;
+	else if (f2fs_compressed_file(inode))
+		cp_reason = CP_COMPRESSED;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	/*
+	 * If inode have do dedup or revoke recently, we need to do
+	 * checkpoint to avoid roll forward recovery after fsync,
+	 * which may cause data inconsistency.
+	 */
+	else if (F2FS_I(inode)->dedup_cp_ver == cur_cp_version(F2FS_CKPT(sbi)))
+		cp_reason = CP_DEDUPED;
+#endif
+	else if (inode->i_nlink != 1)
+		cp_reason = CP_HARDLINK;
+	else if (is_sbi_flag_set(sbi, SBI_NEED_CP))
+		cp_reason = CP_SB_NEED_CP;
+	else if (file_wrong_pino(inode))
+		cp_reason = CP_WRONG_PINO;
+	else if (!f2fs_space_for_roll_forward(sbi))
+		cp_reason = CP_NO_SPC_ROLL;
+	else if (!f2fs_is_checkpointed_node(sbi, F2FS_I(inode)->i_pino))
+		cp_reason = CP_NODE_NEED_CP;
+	else if (test_opt(sbi, FASTBOOT))
+		cp_reason = CP_FASTBOOT_MODE;
+	else if (F2FS_OPTION(sbi).active_logs == 2)
+		cp_reason = CP_SPEC_LOG_NUM;
+	else if (F2FS_OPTION(sbi).fsync_mode == FSYNC_MODE_STRICT &&
+		f2fs_need_dentry_mark(sbi, inode->i_ino) &&
+		f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
+							TRANS_DIR_INO))
+		cp_reason = CP_RECOVER_DIR;
+	else if (f2fs_exist_written_data(sbi, F2FS_I(inode)->i_pino,
+							XATTR_DIR_INO))
+		cp_reason = CP_XATTR_DIR;
+
+	return cp_reason;
+}
+
+static bool need_inode_page_update(struct f2fs_sb_info *sbi, nid_t ino)
+{
+	struct page *i = find_get_page(NODE_MAPPING(sbi), ino);
+	bool ret = false;
+	/* But we need to avoid that there are some inode updates */
+	if ((i && PageDirty(i)) || f2fs_need_inode_block_update(sbi, ino))
+		ret = true;
+	f2fs_put_page(i, 0);
+	return ret;
+}
+
+static void try_to_fix_pino(struct inode *inode)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	nid_t pino;
+
+	f2fs_down_write(&fi->i_sem);
+	if (file_wrong_pino(inode) && inode->i_nlink == 1 &&
+			get_parent_ino(inode, &pino)) {
+		f2fs_i_pino_write(inode, pino);
+		file_got_pino(inode);
+	}
+	f2fs_up_write(&fi->i_sem);
+}
+
+static int f2fs_do_sync_file(struct file *file, loff_t start, loff_t end,
+						int datasync, bool atomic)
+{
+	struct inode *inode = file->f_mapping->host;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	nid_t ino = inode->i_ino;
+	int ret = 0;
+	enum cp_reason_type cp_reason = 0;
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_ALL,
+		.nr_to_write = LONG_MAX,
+		.for_reclaim = 0,
+	};
+	unsigned int seq_id = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *outer = NULL;
+
+	if(is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED)) {
+		inner = get_inner_inode(outer);
+		if (inner) {
+			outer = inode;
+			inode = inner;
+		}
+	}
+#endif
+	if (unlikely(f2fs_readonly(inode->i_sb)))
+		return 0;
+
+	trace_f2fs_sync_file_enter(inode);
+
+	if (S_ISDIR(inode->i_mode))
+		goto go_write;
+
+	/* if fdatasync is triggered, let's do in-place-update */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (datasync || get_dirty_pages(inode) <= DEF_MIN_FSYNC_BLOCKS)
+#else
+	if (datasync || get_dirty_pages(inode) <= SM_I(sbi)->min_fsync_blocks)
+#endif
+		set_inode_flag(inode, FI_NEED_IPU);
+	ret = file_write_and_wait_range(file, start, end);
+	clear_inode_flag(inode, FI_NEED_IPU);
+
+	if (ret || is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
+		trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
+		return ret;
+	}
+
+	/* if the inode is dirty, let's recover all the time */
+	if (!f2fs_skip_inode_update(inode, datasync)) {
+		f2fs_write_inode(inode, NULL);
+		goto go_write;
+	}
+
+	/*
+	 * if there is no written data, don't waste time to write recovery info.
+	 */
+	if (!is_inode_flag_set(inode, FI_APPEND_WRITE) &&
+			!f2fs_exist_written_data(sbi, ino, APPEND_INO)) {
+
+		/* it may call write_inode just prior to fsync */
+		if (need_inode_page_update(sbi, ino))
+			goto go_write;
+
+		if (is_inode_flag_set(inode, FI_UPDATE_WRITE) ||
+				f2fs_exist_written_data(sbi, ino, UPDATE_INO))
+			goto flush_out;
+		goto out;
+	} else {
+		/*
+		 * for OPU case, during fsync(), node can be persisted before
+		 * data when lower device doesn't support write barrier, result
+		 * in data corruption after SPO.
+		 * So for strict fsync mode, force to use atomic write semantics
+		 * to keep write order in between data/node and last node to
+		 * avoid potential data corruption.
+		 */
+		if (F2FS_OPTION(sbi).fsync_mode ==
+				FSYNC_MODE_STRICT && !atomic)
+			atomic = true;
+	}
+go_write:
+	/*
+	 * Both of fdatasync() and fsync() are able to be recovered from
+	 * sudden-power-off.
+	 */
+	f2fs_down_read(&F2FS_I(inode)->i_sem);
+	cp_reason = need_do_checkpoint(inode);
+	f2fs_up_read(&F2FS_I(inode)->i_sem);
+
+	if (cp_reason) {
+		/* all the dirty node pages should be flushed for POR */
+		ret = f2fs_sync_fs(inode->i_sb, 1);
+
+		/*
+		 * We've secured consistency through sync_fs. Following pino
+		 * will be used only for fsynced inodes after checkpoint.
+		 */
+		try_to_fix_pino(inode);
+		clear_inode_flag(inode, FI_APPEND_WRITE);
+		clear_inode_flag(inode, FI_UPDATE_WRITE);
+		goto out;
+	}
+sync_nodes:
+	atomic_inc(&sbi->wb_sync_req[NODE]);
+	ret = f2fs_fsync_node_pages(sbi, inode, &wbc, atomic, &seq_id);
+	atomic_dec(&sbi->wb_sync_req[NODE]);
+	if (ret)
+		goto out;
+
+	/* if cp_error was enabled, we should avoid infinite loop */
+	if (unlikely(f2fs_cp_error(sbi))) {
+		ret = -EIO;
+		goto out;
+	}
+
+	if (f2fs_need_inode_block_update(sbi, ino)) {
+		f2fs_mark_inode_dirty_sync(inode, true);
+		f2fs_write_inode(inode, NULL);
+		goto sync_nodes;
+	}
+
+	/*
+	 * If it's atomic_write, it's just fine to keep write ordering. So
+	 * here we don't need to wait for node write completion, since we use
+	 * node chain which serializes node blocks. If one of node writes are
+	 * reordered, we can see simply broken chain, resulting in stopping
+	 * roll-forward recovery. It means we'll recover all or none node blocks
+	 * given fsync mark.
+	 */
+	if (!atomic) {
+		ret = f2fs_wait_on_node_pages_writeback(sbi, seq_id);
+		if (ret)
+			goto out;
+	}
+
+	/* once recovery info is written, don't need to tack this */
+	f2fs_remove_ino_entry(sbi, ino, APPEND_INO);
+	clear_inode_flag(inode, FI_APPEND_WRITE);
+flush_out:
+	if ((!atomic && F2FS_OPTION(sbi).fsync_mode != FSYNC_MODE_NOBARRIER) ||
+	    (atomic && !test_opt(sbi, NOBARRIER) && f2fs_sb_has_blkzoned(sbi)))
+		ret = f2fs_issue_flush(sbi, inode->i_ino);
+	if (!ret) {
+		f2fs_remove_ino_entry(sbi, ino, UPDATE_INO);
+		clear_inode_flag(inode, FI_UPDATE_WRITE);
+		f2fs_remove_ino_entry(sbi, ino, FLUSH_INO);
+	}
+	f2fs_update_time(sbi, REQ_TIME);
+out:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		inode = outer;
+		put_inner_inode(inner);
+	}
+#endif
+	trace_f2fs_sync_file_exit(inode, cp_reason, datasync, ret);
+	return ret;
+}
+
+int f2fs_sync_file(struct file *file, loff_t start, loff_t end, int datasync)
+{
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(file_inode(file)))))
+		return -EIO;
+	return f2fs_do_sync_file(file, start, end, datasync, false);
+}
+
+static bool __found_offset(struct address_space *mapping,
+		struct dnode_of_data *dn, pgoff_t index, int whence)
+{
+	block_t blkaddr = f2fs_data_blkaddr(dn);
+	struct inode *inode = mapping->host;
+	bool compressed_cluster = false;
+
+	if (f2fs_compressed_file(inode)) {
+		block_t first_blkaddr = data_blkaddr(dn->inode, dn->node_page,
+		    ALIGN_DOWN(dn->ofs_in_node, F2FS_I(inode)->i_cluster_size));
+
+		compressed_cluster = first_blkaddr == COMPRESS_ADDR;
+	}
+
+	switch (whence) {
+	case SEEK_DATA:
+		if (__is_valid_data_blkaddr(blkaddr))
+			return true;
+		if (blkaddr == NEW_ADDR &&
+		    xa_get_mark(&mapping->i_pages, index, PAGECACHE_TAG_DIRTY))
+			return true;
+		if (compressed_cluster)
+			return true;
+		break;
+	case SEEK_HOLE:
+		if (compressed_cluster)
+			return false;
+		if (blkaddr == NULL_ADDR)
+			return true;
+		break;
+	}
+	return false;
+}
+
+static loff_t f2fs_seek_block(struct file *file, loff_t offset, int whence)
+{
+	struct inode *inode = file->f_mapping->host;
+	loff_t maxbytes = inode->i_sb->s_maxbytes;
+	struct dnode_of_data dn;
+	pgoff_t pgofs, end_offset;
+	loff_t data_ofs = offset;
+	loff_t isize;
+	int err = 0;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL, *exter = NULL;
+#endif
+
+	inode_lock(inode);
+
+	isize = i_size_read(inode);
+	if (offset >= isize)
+		goto fail;
+
+	/* handle inline data case */
+	if (f2fs_has_inline_data(inode)) {
+		if (whence == SEEK_HOLE) {
+			data_ofs = isize;
+			goto found;
+		} else if (whence == SEEK_DATA) {
+			data_ofs = offset;
+			goto found;
+		}
+	}
+
+	pgofs = (pgoff_t)(offset >> PAGE_SHIFT);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	inner = get_inner_inode(inode);
+	if (inner) {
+		exter = inode;
+		inode = inner;
+	}
+#endif
+	for (; data_ofs < isize; data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_dnode_of_data(&dn, pgofs, LOOKUP_NODE);
+		if (err && err != -ENOENT) {
+			goto fail;
+		} else if (err == -ENOENT) {
+			/* direct node does not exists */
+			if (whence == SEEK_DATA) {
+				pgofs = f2fs_get_next_page_offset(&dn, pgofs);
+				continue;
+			} else {
+				goto found;
+			}
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+
+		/* find data/hole in dnode block */
+		for (; dn.ofs_in_node < end_offset;
+				dn.ofs_in_node++, pgofs++,
+				data_ofs = (loff_t)pgofs << PAGE_SHIFT) {
+			block_t blkaddr;
+
+			blkaddr = f2fs_data_blkaddr(&dn);
+
+			if (__is_valid_data_blkaddr(blkaddr) &&
+				!f2fs_is_valid_blkaddr(F2FS_I_SB(inode),
+					blkaddr, DATA_GENERIC_ENHANCE)) {
+				f2fs_put_dnode(&dn);
+				goto fail;
+			}
+
+			if (__found_offset(file->f_mapping, &dn,
+							pgofs, whence)) {
+				f2fs_put_dnode(&dn);
+				goto found;
+			}
+		}
+		f2fs_put_dnode(&dn);
+	}
+
+	if (whence == SEEK_DATA)
+		goto fail;
+found:
+	if (whence == SEEK_HOLE && data_ofs > isize)
+		data_ofs = isize;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		inode = exter;
+		put_inner_inode(inner);
+	}
+#endif
+	inode_unlock(inode);
+	return vfs_setpos(file, data_ofs, maxbytes);
+fail:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (inner) {
+		inode = exter;
+		put_inner_inode(inner);
+	}
+#endif
+	inode_unlock(inode);
+	return -ENXIO;
+}
+
+static loff_t f2fs_llseek(struct file *file, loff_t offset, int whence)
+{
+	struct inode *inode = file->f_mapping->host;
+	loff_t maxbytes = inode->i_sb->s_maxbytes;
+
+	if (f2fs_compressed_file(inode))
+		maxbytes = max_file_blocks(inode) << F2FS_BLKSIZE_BITS;
+
+	switch (whence) {
+	case SEEK_SET:
+	case SEEK_CUR:
+	case SEEK_END:
+		return generic_file_llseek_size(file, offset, whence,
+						maxbytes, i_size_read(inode));
+	case SEEK_DATA:
+	case SEEK_HOLE:
+		if (offset < 0)
+			return -ENXIO;
+		return f2fs_seek_block(file, offset, whence);
+	}
+
+	return -EINVAL;
+}
+
+static int f2fs_file_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	struct inode *inode = file_inode(file);
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+
+	if (!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	file_accessed(file);
+	vma->vm_ops = &f2fs_file_vm_ops;
+
+	f2fs_down_read(&F2FS_I(inode)->i_sem);
+	set_inode_flag(inode, FI_MMAP_FILE);
+	f2fs_up_read(&F2FS_I(inode)->i_sem);
+
+	return 0;
+}
+
+static int finish_preallocate_blocks(struct inode *inode)
+{
+	int ret;
+
+	inode_lock(inode);
+	if (is_inode_flag_set(inode, FI_OPENED_FILE)) {
+		inode_unlock(inode);
+		return 0;
+	}
+
+	if (!file_should_truncate(inode)) {
+		set_inode_flag(inode, FI_OPENED_FILE);
+		inode_unlock(inode);
+		return 0;
+	}
+
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
+
+	truncate_setsize(inode, i_size_read(inode));
+	ret = f2fs_truncate(inode);
+
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	if (!ret)
+		set_inode_flag(inode, FI_OPENED_FILE);
+
+	inode_unlock(inode);
+	if (ret)
+		return ret;
+
+	file_dont_truncate(inode);
+	return 0;
+}
+
+static int f2fs_release_file(struct inode *inode, struct file *filp);
+static int f2fs_file_open(struct inode *inode, struct file *filp)
+{
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL;
+#endif
+	int err;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_DEDUP_OPEN))
+		return -EIO;
+#endif
+
+	err = fscrypt_file_open(inode, filp);
+	if (err)
+		return err;
+
+	if (!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	err = fsverity_file_open(inode, filp);
+	if (err)
+		return err;
+
+	filp->f_mode |= FMODE_NOWAIT;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	err = dquot_file_open(inode, filp);
+	if (err)
+		return err;
+
+	if (f2fs_is_outer_inode(inode)) {
+		inner = get_inner_inode(inode);
+		if (inner) {
+			err = f2fs_file_open(inner, filp);
+		} else {
+			f2fs_release_file(inode, filp);
+			return -ENOENT;
+		}
+		put_inner_inode(inner);
+		return err;
+	}
+	return finish_preallocate_blocks(inode);
+#else
+	err = dquot_file_open(inode, filp);
+	if (err)
+		return err;
+
+	return finish_preallocate_blocks(inode);
+#endif
+}
+
+void f2fs_truncate_data_blocks_range(struct dnode_of_data *dn, int count)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	int nr_free = 0, ofs = dn->ofs_in_node, len = count;
+	__le32 *addr;
+	bool compressed_cluster = false;
+	int cluster_index = 0, valid_blocks = 0;
+	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
+	bool released = !atomic_read(&F2FS_I(dn->inode)->i_compr_blocks);
+
+	addr = get_dnode_addr(dn->inode, dn->node_page) + ofs;
+
+	/* Assumption: truncation starts with cluster */
+	for (; count > 0; count--, addr++, dn->ofs_in_node++, cluster_index++) {
+		block_t blkaddr = le32_to_cpu(*addr);
+
+		if (f2fs_compressed_file(dn->inode) &&
+					!(cluster_index & (cluster_size - 1))) {
+			if (compressed_cluster)
+				f2fs_i_compr_blocks_update(dn->inode,
+							valid_blocks, false);
+			compressed_cluster = (blkaddr == COMPRESS_ADDR);
+			valid_blocks = 0;
+		}
+
+		if (blkaddr == NULL_ADDR)
+			continue;
+
+#ifdef CONFIG_F2FS_FS_SEQZONE
+		if (f2fs_seqzone_file(dn->inode))
+			dn->seqzone_index = NULL_ADDR;
+#endif
+		f2fs_set_data_blkaddr(dn, NULL_ADDR);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (blkaddr == DEDUP_ADDR)
+			continue;
+#endif
+		if (__is_valid_data_blkaddr(blkaddr)) {
+			if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
+					DATA_GENERIC_ENHANCE))
+				continue;
+			if (compressed_cluster)
+				valid_blocks++;
+		}
+
+		f2fs_invalidate_blocks(sbi, blkaddr);
+
+		if (!released || blkaddr != COMPRESS_ADDR)
+			nr_free++;
+	}
+
+	if (compressed_cluster)
+		f2fs_i_compr_blocks_update(dn->inode, valid_blocks, false);
+
+	if (nr_free) {
+		pgoff_t fofs;
+		/*
+		 * once we invalidate valid blkaddr in range [ofs, ofs + count],
+		 * we will invalidate all blkaddr in the whole range.
+		 */
+		fofs = f2fs_start_bidx_of_node(ofs_of_node(dn->node_page),
+							dn->inode) + ofs;
+		f2fs_update_read_extent_cache_range(dn, fofs, 0, len);
+		f2fs_update_age_extent_cache_range(dn, fofs, len);
+		dec_valid_block_count(sbi, dn->inode, nr_free);
+	}
+	dn->ofs_in_node = ofs;
+
+	f2fs_update_time(sbi, REQ_TIME);
+	trace_f2fs_truncate_data_blocks_range(dn->inode, dn->nid,
+					 dn->ofs_in_node, nr_free);
+}
+
+void f2fs_truncate_data_blocks(struct dnode_of_data *dn)
+{
+	f2fs_truncate_data_blocks_range(dn, ADDRS_PER_BLOCK(dn->inode));
+}
+
+static int truncate_partial_data_page(struct inode *inode, u64 from,
+								bool cache_only)
+{
+	loff_t offset = from & (PAGE_SIZE - 1);
+	pgoff_t index = from >> PAGE_SHIFT;
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+
+	if (!offset && !cache_only)
+		return 0;
+
+	if (cache_only) {
+		page = find_lock_page(mapping, index);
+		if (page && PageUptodate(page))
+			goto truncate_out;
+		f2fs_put_page(page, 1);
+		return 0;
+	}
+
+	page = f2fs_get_lock_data_page(inode, index, true);
+	if (IS_ERR(page))
+		return PTR_ERR(page) == -ENOENT ? 0 : PTR_ERR(page);
+truncate_out:
+	f2fs_wait_on_page_writeback(page, DATA, true, true);
+	zero_user(page, offset, PAGE_SIZE - offset);
+
+	/* An encrypted inode should have a key and truncate the last page. */
+	f2fs_bug_on(F2FS_I_SB(inode), cache_only && IS_ENCRYPTED(inode));
+	if (!cache_only)
+		set_page_dirty(page);
+	f2fs_put_page(page, 1);
+	return 0;
+}
+
+int f2fs_do_truncate_blocks(struct inode *inode, u64 from, bool lock)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	pgoff_t free_from;
+	int count = 0, err = 0;
+	struct page *ipage;
+	bool truncate_page = false;
+
+	trace_f2fs_truncate_blocks_enter(inode, from);
+
+	free_from = (pgoff_t)F2FS_BLK_ALIGN(from);
+
+	if (free_from >= max_file_blocks(inode))
+		goto free_partial;
+
+	if (lock)
+		f2fs_lock_op(sbi);
+
+	ipage = f2fs_get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(ipage)) {
+		err = PTR_ERR(ipage);
+		goto out;
+	}
+
+	if (f2fs_has_inline_data(inode)) {
+		f2fs_truncate_inline_inode(inode, ipage, from);
+		f2fs_put_page(ipage, 1);
+		truncate_page = true;
+		goto out;
+	}
+
+	set_new_dnode(&dn, inode, ipage, NULL, 0);
+	err = f2fs_get_dnode_of_data(&dn, free_from, LOOKUP_NODE_RA);
+	if (err) {
+		if (err == -ENOENT)
+			goto free_next;
+		goto out;
+	}
+
+	count = ADDRS_PER_PAGE(dn.node_page, inode);
+
+	count -= dn.ofs_in_node;
+	f2fs_bug_on(sbi, count < 0);
+
+	if (dn.ofs_in_node || IS_INODE(dn.node_page)) {
+		f2fs_truncate_data_blocks_range(&dn, count);
+		free_from += count;
+	}
+
+	f2fs_put_dnode(&dn);
+free_next:
+	err = f2fs_truncate_inode_blocks(inode, free_from);
+out:
+	if (lock)
+		f2fs_unlock_op(sbi);
+free_partial:
+	/* lastly zero out the first data page */
+	if (!err)
+		err = truncate_partial_data_page(inode, from, truncate_page);
+
+	trace_f2fs_truncate_blocks_exit(inode, err);
+	return err;
+}
+
+int f2fs_truncate_blocks(struct inode *inode, u64 from, bool lock)
+{
+	u64 free_from = from;
+	int err;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/*
+	 * for compressed file, only support cluster size
+	 * aligned truncation.
+	 */
+	if (f2fs_compressed_file(inode))
+		free_from = round_up(from,
+				F2FS_I(inode)->i_cluster_size << PAGE_SHIFT);
+#endif
+
+	err = f2fs_do_truncate_blocks(inode, free_from, lock);
+	if (err)
+		return err;
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	/*
+	 * For compressed file, after release compress blocks, don't allow write
+	 * direct, but we should allow write direct after truncate to zero.
+	 */
+	if (f2fs_compressed_file(inode) && !free_from
+			&& is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
+
+	if (from != free_from) {
+		err = f2fs_truncate_partial_cluster(inode, from, lock);
+		if (err)
+			return err;
+	}
+#endif
+
+	return 0;
+}
+
+int f2fs_truncate(struct inode *inode)
+{
+	int err;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+
+	if (!(S_ISREG(inode->i_mode) || S_ISDIR(inode->i_mode) ||
+				S_ISLNK(inode->i_mode)))
+		return 0;
+
+	trace_f2fs_truncate(inode);
+
+	if (time_to_inject(F2FS_I_SB(inode), FAULT_TRUNCATE))
+		return -EIO;
+
+	err = f2fs_dquot_initialize(inode);
+	if (err)
+		return err;
+
+	/* we should check inline_data size */
+	if (!f2fs_may_inline_data(inode)) {
+		err = f2fs_convert_inline_inode(inode);
+		if (err)
+			return err;
+	}
+
+	err = f2fs_truncate_blocks(inode, i_size_read(inode), true);
+	if (err)
+		return err;
+
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, false);
+	return 0;
+}
+
+static bool f2fs_force_buffered_io(struct inode *inode, int rw)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	if (!fscrypt_dio_supported(inode))
+		return true;
+	if (fsverity_active(inode))
+		return true;
+	if (f2fs_compressed_file(inode))
+		return true;
+	if (f2fs_has_inline_data(inode))
+		return true;
+
+	/* disallow direct IO if any of devices has unaligned blksize */
+	if (f2fs_is_multi_device(sbi) && !sbi->aligned_blksize)
+		return true;
+	/*
+	 * for blkzoned device, fallback direct IO to buffered IO, so
+	 * all IOs can be serialized by log-structured write.
+	 */
+	if (f2fs_sb_has_blkzoned(sbi) && (rw == WRITE))
+		return true;
+	if (f2fs_lfs_mode(sbi) && rw == WRITE && F2FS_IO_ALIGNED(sbi))
+		return true;
+	if (is_sbi_flag_set(sbi, SBI_CP_DISABLED))
+		return true;
+
+	return false;
+}
+
+int f2fs_getattr(struct user_namespace *mnt_userns, const struct path *path,
+		 struct kstat *stat, u32 request_mask, unsigned int query_flags)
+{
+	struct inode *inode = d_inode(path->dentry);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri = NULL;
+	unsigned int flags;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL;
+#endif
+
+	if (f2fs_has_extra_attr(inode) &&
+			f2fs_sb_has_inode_crtime(F2FS_I_SB(inode)) &&
+			F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime)) {
+		stat->result_mask |= STATX_BTIME;
+		stat->btime.tv_sec = fi->i_crtime.tv_sec;
+		stat->btime.tv_nsec = fi->i_crtime.tv_nsec;
+	}
+
+	/*
+	 * Return the DIO alignment restrictions if requested.  We only return
+	 * this information when requested, since on encrypted files it might
+	 * take a fair bit of work to get if the file wasn't opened recently.
+	 *
+	 * f2fs sometimes supports DIO reads but not DIO writes.  STATX_DIOALIGN
+	 * cannot represent that, so in that case we report no DIO support.
+	 */
+	if ((request_mask & STATX_DIOALIGN) && S_ISREG(inode->i_mode)) {
+		unsigned int bsize = i_blocksize(inode);
+
+		stat->result_mask |= STATX_DIOALIGN;
+		if (!f2fs_force_buffered_io(inode, WRITE)) {
+			stat->dio_mem_align = bsize;
+			stat->dio_offset_align = bsize;
+		}
+	}
+
+	flags = fi->i_flags;
+	if ((flags & F2FS_COMPR_FL) && may_compress)
+		stat->attributes |= STATX_ATTR_COMPRESSED;
+	if (flags & F2FS_APPEND_FL)
+		stat->attributes |= STATX_ATTR_APPEND;
+	if (IS_ENCRYPTED(inode))
+		stat->attributes |= STATX_ATTR_ENCRYPTED;
+	if (flags & F2FS_IMMUTABLE_FL)
+		stat->attributes |= STATX_ATTR_IMMUTABLE;
+	if (flags & F2FS_NODUMP_FL)
+		stat->attributes |= STATX_ATTR_NODUMP;
+	if (flags & F2FS_NOCOMP_FL)
+		stat->attributes |= STATX_ATTR_NOCOMPR;
+	if (IS_VERITY(inode))
+		stat->attributes |= STATX_ATTR_VERITY;
+
+	stat->attributes_mask |= (STATX_ATTR_COMPRESSED |
+				  STATX_ATTR_APPEND |
+				  STATX_ATTR_ENCRYPTED |
+				  STATX_ATTR_IMMUTABLE |
+				  STATX_ATTR_NODUMP |
+				  STATX_ATTR_NOCOMPR |
+				  STATX_ATTR_VERITY);
+
+	generic_fillattr(mnt_userns, inode, stat);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	inner = get_inner_inode(inode);
+	if (inner) {
+		f2fs_down_read(&F2FS_I(inner)->i_sem);
+		if (inner->i_nlink == 0)
+			f2fs_bug_on(F2FS_I_SB(inode), 1);
+		else
+			stat->blocks = inner->i_blocks / inner->i_nlink;
+		f2fs_up_read(&F2FS_I(inner)->i_sem);
+	}
+	put_inner_inode(inner);
+#endif
+
+	/* we need to show initial sectors used for inline_data/dentries */
+	if ((S_ISREG(inode->i_mode) && f2fs_has_inline_data(inode)) ||
+					f2fs_has_inline_dentry(inode))
+		stat->blocks += (stat->size + 511) >> 9;
+
+	return 0;
+}
+
+#ifdef CONFIG_F2FS_FS_POSIX_ACL
+static void __setattr_copy(struct user_namespace *mnt_userns,
+			   struct inode *inode, const struct iattr *attr)
+{
+	unsigned int ia_valid = attr->ia_valid;
+
+	i_uid_update(mnt_userns, attr, inode);
+	i_gid_update(mnt_userns, attr, inode);
+	if (ia_valid & ATTR_ATIME)
+		inode->i_atime = attr->ia_atime;
+	if (ia_valid & ATTR_MTIME)
+		inode->i_mtime = attr->ia_mtime;
+	if (ia_valid & ATTR_CTIME)
+		inode->i_ctime = attr->ia_ctime;
+	if (ia_valid & ATTR_MODE) {
+		umode_t mode = attr->ia_mode;
+		vfsgid_t vfsgid = i_gid_into_vfsgid(mnt_userns, inode);
+
+		if (!vfsgid_in_group_p(vfsgid) &&
+		    !capable_wrt_inode_uidgid(mnt_userns, inode, CAP_FSETID))
+			mode &= ~S_ISGID;
+		set_acl_inode(inode, mode);
+	}
+}
+#else
+#define __setattr_copy setattr_copy
+#endif
+
+int f2fs_setattr(struct user_namespace *mnt_userns, struct dentry *dentry,
+		 struct iattr *attr)
+{
+	struct inode *inode = d_inode(dentry);
+	int err;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+
+	if (unlikely(IS_IMMUTABLE(inode)))
+		return -EPERM;
+
+	if (unlikely(IS_APPEND(inode) &&
+			(attr->ia_valid & (ATTR_MODE | ATTR_UID |
+				  ATTR_GID | ATTR_TIMES_SET))))
+		return -EPERM;
+
+	if ((attr->ia_valid & ATTR_SIZE) &&
+		!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	err = setattr_prepare(mnt_userns, dentry, attr);
+	if (err)
+		return err;
+
+	err = fscrypt_prepare_setattr(dentry, attr);
+	if (err)
+		return err;
+
+	err = fsverity_prepare_setattr(dentry, attr);
+	if (err)
+		return err;
+
+	if (is_quota_modification(mnt_userns, inode, attr)) {
+		err = f2fs_dquot_initialize(inode);
+		if (err)
+			return err;
+	}
+	if (i_uid_needs_update(mnt_userns, attr, inode) ||
+	    i_gid_needs_update(mnt_userns, attr, inode)) {
+		f2fs_lock_op(F2FS_I_SB(inode));
+		err = dquot_transfer(mnt_userns, inode, attr);
+		if (err) {
+			set_sbi_flag(F2FS_I_SB(inode),
+					SBI_QUOTA_NEED_REPAIR);
+			f2fs_unlock_op(F2FS_I_SB(inode));
+			return err;
+		}
+		/*
+		 * update uid/gid under lock_op(), so that dquot and inode can
+		 * be updated atomically.
+		 */
+		i_uid_update(mnt_userns, attr, inode);
+		i_gid_update(mnt_userns, attr, inode);
+		f2fs_mark_inode_dirty_sync(inode, true);
+		f2fs_unlock_op(F2FS_I_SB(inode));
+	}
+
+	if (attr->ia_valid & ATTR_SIZE) {
+		loff_t old_size = i_size_read(inode);
+
+		if (attr->ia_size > MAX_INLINE_DATA(inode)) {
+			/*
+			 * should convert inline inode before i_size_write to
+			 * keep smaller than inline_data size with inline flag.
+			 */
+			err = f2fs_convert_inline_inode(inode);
+			if (err)
+				return err;
+		}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		if (attr->ia_size <= old_size && f2fs_compressed_file(inode) &&
+		    is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			err = f2fs_reserve_compress_blocks(inode, NULL);
+			if (err < 0)
+				return err;
+		}
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+		/*
+		 * caller have hold inode lock
+		 */
+		if (is_inode_flag_set(inode, FI_SNAPSHOTED))
+			return -EOPNOTSUPP;
+		if (attr->ia_size <= old_size && f2fs_is_outer_inode(inode)) {
+			mark_file_modified(inode);
+			err = f2fs_revoke_deduped_inode(inode, __func__);
+			if (err)
+				return err;
+		}
+#endif
+
+		f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		filemap_invalidate_lock(inode->i_mapping);
+
+		truncate_setsize(inode, attr->ia_size);
+
+		if (attr->ia_size <= old_size)
+			err = f2fs_truncate(inode);
+		/*
+		 * do not trim all blocks after i_size if target size is
+		 * larger than i_size.
+		 */
+		filemap_invalidate_unlock(inode->i_mapping);
+		f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		if (err)
+			return err;
+
+		spin_lock(&F2FS_I(inode)->i_size_lock);
+		inode->i_mtime = inode->i_ctime = current_time(inode);
+		F2FS_I(inode)->last_disk_size = i_size_read(inode);
+		spin_unlock(&F2FS_I(inode)->i_size_lock);
+	}
+
+	__setattr_copy(mnt_userns, inode, attr);
+
+	if (attr->ia_valid & ATTR_MODE) {
+		err = posix_acl_chmod(mnt_userns, inode, f2fs_get_inode_mode(inode));
+
+		if (is_inode_flag_set(inode, FI_ACL_MODE)) {
+			if (!err)
+				inode->i_mode = F2FS_I(inode)->i_acl_mode;
+			clear_inode_flag(inode, FI_ACL_MODE);
+		}
+	}
+
+	/* file size may changed here */
+	f2fs_mark_inode_dirty_sync(inode, true);
+
+	/* inode change will produce dirty node pages flushed by checkpoint */
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
+
+	return err;
+}
+
+const struct inode_operations f2fs_file_inode_operations = {
+	.getattr	= f2fs_getattr,
+	.setattr	= f2fs_setattr,
+	.get_acl	= f2fs_get_acl,
+	.set_acl	= f2fs_set_acl,
+	.listxattr	= f2fs_listxattr,
+	.fiemap		= f2fs_fiemap,
+	.fileattr_get	= f2fs_fileattr_get,
+	.fileattr_set	= f2fs_fileattr_set,
+};
+
+static int fill_zero(struct inode *inode, pgoff_t index,
+					loff_t start, loff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct page *page;
+
+	if (!len)
+		return 0;
+
+	f2fs_balance_fs(sbi, true);
+
+	f2fs_lock_op(sbi);
+	page = f2fs_get_new_data_page(inode, NULL, index, false);
+	f2fs_unlock_op(sbi);
+
+	if (IS_ERR(page))
+		return PTR_ERR(page);
+
+	f2fs_wait_on_page_writeback(page, DATA, true, true);
+	zero_user(page, start, len);
+	set_page_dirty(page);
+	f2fs_put_page(page, 1);
+	return 0;
+}
+
+int f2fs_truncate_hole(struct inode *inode, pgoff_t pg_start, pgoff_t pg_end)
+{
+	int err;
+
+	while (pg_start < pg_end) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		err = f2fs_get_dnode_of_data(&dn, pg_start, LOOKUP_NODE);
+		if (err) {
+			if (err == -ENOENT) {
+				pg_start = f2fs_get_next_page_offset(&dn,
+								pg_start);
+				continue;
+			}
+			return err;
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, pg_end - pg_start);
+
+		f2fs_bug_on(F2FS_I_SB(inode), count == 0 || count > end_offset);
+
+		f2fs_truncate_data_blocks_range(&dn, count);
+		f2fs_put_dnode(&dn);
+
+		pg_start += count;
+	}
+	return 0;
+}
+
+static int f2fs_punch_hole(struct inode *inode, loff_t offset, loff_t len)
+{
+	pgoff_t pg_start, pg_end;
+	loff_t off_start, off_end;
+	int ret;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+
+	off_start = offset & (PAGE_SIZE - 1);
+	off_end = (offset + len) & (PAGE_SIZE - 1);
+
+	if (pg_start == pg_end) {
+		ret = fill_zero(inode, pg_start, off_start,
+						off_end - off_start);
+		if (ret)
+			return ret;
+	} else {
+		if (off_start) {
+			ret = fill_zero(inode, pg_start++, off_start,
+						PAGE_SIZE - off_start);
+			if (ret)
+				return ret;
+		}
+		if (off_end) {
+			ret = fill_zero(inode, pg_end, 0, off_end);
+			if (ret)
+				return ret;
+		}
+
+		if (pg_start < pg_end) {
+			loff_t blk_start, blk_end;
+			struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+			f2fs_balance_fs(sbi, true);
+
+			blk_start = (loff_t)pg_start << PAGE_SHIFT;
+			blk_end = (loff_t)pg_end << PAGE_SHIFT;
+
+			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+			filemap_invalidate_lock(inode->i_mapping);
+
+			truncate_pagecache_range(inode, blk_start, blk_end - 1);
+
+			f2fs_lock_op(sbi);
+			ret = f2fs_truncate_hole(inode, pg_start, pg_end);
+			f2fs_unlock_op(sbi);
+
+			filemap_invalidate_unlock(inode->i_mapping);
+			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+		}
+	}
+
+	return ret;
+}
+
+static int __read_out_blkaddrs(struct inode *inode, block_t *blkaddr,
+				int *do_replace, pgoff_t off, pgoff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	int ret, done, i;
+
+next_dnode:
+	set_new_dnode(&dn, inode, NULL, NULL, 0);
+	ret = f2fs_get_dnode_of_data(&dn, off, LOOKUP_NODE_RA);
+	if (ret && ret != -ENOENT) {
+		return ret;
+	} else if (ret == -ENOENT) {
+		if (dn.max_level == 0)
+			return -ENOENT;
+		done = min((pgoff_t)ADDRS_PER_BLOCK(inode) -
+						dn.ofs_in_node, len);
+		blkaddr += done;
+		do_replace += done;
+		goto next;
+	}
+
+	done = min((pgoff_t)ADDRS_PER_PAGE(dn.node_page, inode) -
+							dn.ofs_in_node, len);
+	for (i = 0; i < done; i++, blkaddr++, do_replace++, dn.ofs_in_node++) {
+		*blkaddr = f2fs_data_blkaddr(&dn);
+
+		if (__is_valid_data_blkaddr(*blkaddr) &&
+			!f2fs_is_valid_blkaddr(sbi, *blkaddr,
+					DATA_GENERIC_ENHANCE)) {
+			f2fs_put_dnode(&dn);
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			return -EFSCORRUPTED;
+		}
+
+		if (!f2fs_is_checkpointed_data(sbi, *blkaddr)) {
+
+			if (f2fs_lfs_mode(sbi)) {
+				f2fs_put_dnode(&dn);
+				return -EOPNOTSUPP;
+			}
+
+			/* do not invalidate this block address */
+			f2fs_update_data_blkaddr(&dn, NULL_ADDR);
+			*do_replace = 1;
+		}
+	}
+	f2fs_put_dnode(&dn);
+next:
+	len -= done;
+	off += done;
+	if (len)
+		goto next_dnode;
+	return 0;
+}
+
+static int __roll_back_blkaddrs(struct inode *inode, block_t *blkaddr,
+				int *do_replace, pgoff_t off, int len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct dnode_of_data dn;
+	int ret, i;
+
+	for (i = 0; i < len; i++, do_replace++, blkaddr++) {
+		if (*do_replace == 0)
+			continue;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, off + i, LOOKUP_NODE_RA);
+		if (ret) {
+			dec_valid_block_count(sbi, inode, 1);
+			f2fs_invalidate_blocks(sbi, *blkaddr);
+		} else {
+			f2fs_update_data_blkaddr(&dn, *blkaddr);
+		}
+		f2fs_put_dnode(&dn);
+	}
+	return 0;
+}
+
+static int __clone_blkaddrs(struct inode *src_inode, struct inode *dst_inode,
+			block_t *blkaddr, int *do_replace,
+			pgoff_t src, pgoff_t dst, pgoff_t len, bool full)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src_inode);
+	pgoff_t i = 0;
+	int ret;
+
+	while (i < len) {
+		if (blkaddr[i] == NULL_ADDR && !full) {
+			i++;
+			continue;
+		}
+
+		if (do_replace[i] || blkaddr[i] == NULL_ADDR) {
+			struct dnode_of_data dn;
+			struct node_info ni;
+			size_t new_size;
+			pgoff_t ilen;
+
+			set_new_dnode(&dn, dst_inode, NULL, NULL, 0);
+			ret = f2fs_get_dnode_of_data(&dn, dst + i, ALLOC_NODE);
+			if (ret)
+				return ret;
+
+			ret = f2fs_get_node_info(sbi, dn.nid, &ni, false);
+			if (ret) {
+				f2fs_put_dnode(&dn);
+				return ret;
+			}
+
+			ilen = min((pgoff_t)
+				ADDRS_PER_PAGE(dn.node_page, dst_inode) -
+						dn.ofs_in_node, len - i);
+			do {
+				dn.data_blkaddr = f2fs_data_blkaddr(&dn);
+				f2fs_truncate_data_blocks_range(&dn, 1);
+
+				if (do_replace[i]) {
+					f2fs_i_blocks_write(src_inode,
+							1, false, false);
+					f2fs_i_blocks_write(dst_inode,
+							1, true, false);
+					f2fs_replace_block(sbi, &dn, dn.data_blkaddr,
+					blkaddr[i], ni.version, true, false);
+
+					do_replace[i] = 0;
+				}
+				dn.ofs_in_node++;
+				i++;
+				new_size = (loff_t)(dst + i) << PAGE_SHIFT;
+				if (dst_inode->i_size < new_size)
+					f2fs_i_size_write(dst_inode, new_size);
+			} while (--ilen && (do_replace[i] || blkaddr[i] == NULL_ADDR));
+
+			f2fs_put_dnode(&dn);
+		} else {
+			struct page *psrc, *pdst;
+
+			psrc = f2fs_get_lock_data_page(src_inode,
+							src + i, true);
+			if (IS_ERR(psrc))
+				return PTR_ERR(psrc);
+			pdst = f2fs_get_new_data_page(dst_inode, NULL, dst + i,
+								true);
+			if (IS_ERR(pdst)) {
+				f2fs_put_page(psrc, 1);
+				return PTR_ERR(pdst);
+			}
+			memcpy_page(pdst, 0, psrc, 0, PAGE_SIZE);
+			set_page_dirty(pdst);
+			f2fs_put_page(pdst, 1);
+			f2fs_put_page(psrc, 1);
+
+			ret = f2fs_truncate_hole(src_inode,
+						src + i, src + i + 1);
+			if (ret)
+				return ret;
+			i++;
+		}
+	}
+	return 0;
+}
+
+static int __exchange_data_block(struct inode *src_inode,
+			struct inode *dst_inode, pgoff_t src, pgoff_t dst,
+			pgoff_t len, bool full)
+{
+	block_t *src_blkaddr;
+	int *do_replace;
+	pgoff_t olen;
+	int ret;
+
+	while (len) {
+		olen = min((pgoff_t)4 * ADDRS_PER_BLOCK(src_inode), len);
+
+		src_blkaddr = f2fs_kvzalloc(F2FS_I_SB(src_inode),
+					array_size(olen, sizeof(block_t)),
+					GFP_NOFS);
+		if (!src_blkaddr)
+			return -ENOMEM;
+
+		do_replace = f2fs_kvzalloc(F2FS_I_SB(src_inode),
+					array_size(olen, sizeof(int)),
+					GFP_NOFS);
+		if (!do_replace) {
+			kvfree(src_blkaddr);
+			return -ENOMEM;
+		}
+
+		ret = __read_out_blkaddrs(src_inode, src_blkaddr,
+					do_replace, src, olen);
+		if (ret)
+			goto roll_back;
+
+		ret = __clone_blkaddrs(src_inode, dst_inode, src_blkaddr,
+					do_replace, src, dst, olen, full);
+		if (ret)
+			goto roll_back;
+
+		src += olen;
+		dst += olen;
+		len -= olen;
+
+		kvfree(src_blkaddr);
+		kvfree(do_replace);
+	}
+	return 0;
+
+roll_back:
+	__roll_back_blkaddrs(src_inode, src_blkaddr, do_replace, src, olen);
+	kvfree(src_blkaddr);
+	kvfree(do_replace);
+	return ret;
+}
+
+static int f2fs_do_collapse(struct inode *inode, loff_t offset, loff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	pgoff_t nrpages = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	pgoff_t start = offset >> PAGE_SHIFT;
+	pgoff_t end = (offset + len) >> PAGE_SHIFT;
+	int ret;
+
+	f2fs_balance_fs(sbi, true);
+
+	/* avoid gc operation during block exchange */
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
+
+	f2fs_lock_op(sbi);
+	f2fs_drop_extent_tree(inode);
+	truncate_pagecache(inode, offset);
+	ret = __exchange_data_block(inode, inode, end, start, nrpages - end, true);
+	f2fs_unlock_op(sbi);
+
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	return ret;
+}
+
+static int f2fs_collapse_range(struct inode *inode, loff_t offset, loff_t len)
+{
+	loff_t new_size;
+	int ret;
+
+	if (offset + len >= i_size_read(inode))
+		return -EINVAL;
+
+	/* collapse range should be aligned to block size of f2fs. */
+	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+		return -EINVAL;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	/* write out all dirty pages from offset */
+	ret = filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+	if (ret)
+		return ret;
+
+	ret = f2fs_do_collapse(inode, offset, len);
+	if (ret)
+		return ret;
+
+	/* write out all moved pages, if possible */
+	filemap_invalidate_lock(inode->i_mapping);
+	filemap_write_and_wait_range(inode->i_mapping, offset, LLONG_MAX);
+	truncate_pagecache(inode, offset);
+
+	new_size = i_size_read(inode) - len;
+	ret = f2fs_truncate_blocks(inode, new_size, true);
+	filemap_invalidate_unlock(inode->i_mapping);
+	if (!ret)
+		f2fs_i_size_write(inode, new_size);
+	return ret;
+}
+
+static int f2fs_do_zero_range(struct dnode_of_data *dn, pgoff_t start,
+								pgoff_t end)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	pgoff_t index = start;
+	unsigned int ofs_in_node = dn->ofs_in_node;
+	blkcnt_t count = 0;
+	int ret;
+
+	for (; index < end; index++, dn->ofs_in_node++) {
+		if (f2fs_data_blkaddr(dn) == NULL_ADDR)
+			count++;
+	}
+
+	dn->ofs_in_node = ofs_in_node;
+	ret = f2fs_reserve_new_blocks(dn, count);
+	if (ret)
+		return ret;
+
+	dn->ofs_in_node = ofs_in_node;
+	for (index = start; index < end; index++, dn->ofs_in_node++) {
+		dn->data_blkaddr = f2fs_data_blkaddr(dn);
+		/*
+		 * f2fs_reserve_new_blocks will not guarantee entire block
+		 * allocation.
+		 */
+		if (dn->data_blkaddr == NULL_ADDR) {
+			ret = -ENOSPC;
+			break;
+		}
+
+		if (dn->data_blkaddr == NEW_ADDR)
+			continue;
+
+		if (!f2fs_is_valid_blkaddr(sbi, dn->data_blkaddr,
+					DATA_GENERIC_ENHANCE)) {
+			ret = -EFSCORRUPTED;
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			break;
+		}
+
+		f2fs_invalidate_blocks(sbi, dn->data_blkaddr);
+#ifdef CONFIG_F2FS_FS_SEQZONE
+		if (f2fs_seqzone_file(dn->inode))
+			dn->seqzone_index = NULL_ADDR;
+#endif
+		f2fs_set_data_blkaddr(dn, NEW_ADDR);
+	}
+
+	f2fs_update_read_extent_cache_range(dn, start, 0, index - start);
+	f2fs_update_age_extent_cache_range(dn, start, index - start);
+
+	return ret;
+}
+
+static int f2fs_zero_range(struct inode *inode, loff_t offset, loff_t len,
+								int mode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	pgoff_t index, pg_start, pg_end;
+	loff_t new_size = i_size_read(inode);
+	loff_t off_start, off_end;
+	int ret = 0;
+
+	ret = inode_newsize_ok(inode, (len + offset));
+	if (ret)
+		return ret;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	ret = filemap_write_and_wait_range(mapping, offset, offset + len - 1);
+	if (ret)
+		return ret;
+
+	pg_start = ((unsigned long long) offset) >> PAGE_SHIFT;
+	pg_end = ((unsigned long long) offset + len) >> PAGE_SHIFT;
+
+	off_start = offset & (PAGE_SIZE - 1);
+	off_end = (offset + len) & (PAGE_SIZE - 1);
+
+	if (pg_start == pg_end) {
+		ret = fill_zero(inode, pg_start, off_start,
+						off_end - off_start);
+		if (ret)
+			return ret;
+
+		new_size = max_t(loff_t, new_size, offset + len);
+	} else {
+		if (off_start) {
+			ret = fill_zero(inode, pg_start++, off_start,
+						PAGE_SIZE - off_start);
+			if (ret)
+				return ret;
+
+			new_size = max_t(loff_t, new_size,
+					(loff_t)pg_start << PAGE_SHIFT);
+		}
+
+		for (index = pg_start; index < pg_end;) {
+			struct dnode_of_data dn;
+			unsigned int end_offset;
+			pgoff_t end;
+
+			f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+			filemap_invalidate_lock(mapping);
+
+			truncate_pagecache_range(inode,
+				(loff_t)index << PAGE_SHIFT,
+				((loff_t)pg_end << PAGE_SHIFT) - 1);
+
+			f2fs_lock_op(sbi);
+
+			set_new_dnode(&dn, inode, NULL, NULL, 0);
+			ret = f2fs_get_dnode_of_data(&dn, index, ALLOC_NODE);
 			if (ret) {
 				f2fs_unlock_op(sbi);
 				filemap_invalidate_unlock(mapping);
@@ -1646,2698 +3445,4647 @@ static int f2fs_zero_range(struct inode *inode, loff_t offset, loff_t len,
 				goto out;
 			}
 
-			end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-			end = min(pg_end, end_offset - dn.ofs_in_node + index);
+			end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+			end = min(pg_end, end_offset - dn.ofs_in_node + index);
+
+			ret = f2fs_do_zero_range(&dn, index, end);
+			f2fs_put_dnode(&dn);
+
+			f2fs_unlock_op(sbi);
+			filemap_invalidate_unlock(mapping);
+			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+			f2fs_balance_fs(sbi, dn.node_changed);
+
+			if (ret)
+				goto out;
+
+			index = end;
+			new_size = max_t(loff_t, new_size,
+					(loff_t)index << PAGE_SHIFT);
+		}
+
+		if (off_end) {
+			ret = fill_zero(inode, pg_end, 0, off_end);
+			if (ret)
+				goto out;
+
+			new_size = max_t(loff_t, new_size, offset + len);
+		}
+	}
+
+out:
+	if (new_size > i_size_read(inode)) {
+		if (mode & FALLOC_FL_KEEP_SIZE)
+			file_set_keep_isize(inode);
+		else
+			f2fs_i_size_write(inode, new_size);
+	}
+	return ret;
+}
+
+static int f2fs_insert_range(struct inode *inode, loff_t offset, loff_t len)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	pgoff_t nr, pg_start, pg_end, delta, idx;
+	loff_t new_size;
+	int ret = 0;
+
+	new_size = i_size_read(inode) + len;
+	ret = inode_newsize_ok(inode, new_size);
+	if (ret)
+		return ret;
+
+	if (offset >= i_size_read(inode))
+		return -EINVAL;
+
+	/* insert range should be aligned to block size of f2fs. */
+	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+		return -EINVAL;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		return ret;
+
+	f2fs_balance_fs(sbi, true);
+
+	filemap_invalidate_lock(mapping);
+	ret = f2fs_truncate_blocks(inode, i_size_read(inode), true);
+	filemap_invalidate_unlock(mapping);
+	if (ret)
+		return ret;
+
+	/* write out all dirty pages from offset */
+	ret = filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
+	if (ret)
+		return ret;
+
+	pg_start = offset >> PAGE_SHIFT;
+	pg_end = (offset + len) >> PAGE_SHIFT;
+	delta = pg_end - pg_start;
+	idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+
+	/* avoid gc operation during block exchange */
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(mapping);
+	truncate_pagecache(inode, offset);
+
+	while (!ret && idx > pg_start) {
+		nr = idx - pg_start;
+		if (nr > delta)
+			nr = delta;
+		idx -= nr;
+
+		f2fs_lock_op(sbi);
+		f2fs_drop_extent_tree(inode);
+
+		ret = __exchange_data_block(inode, inode, idx,
+					idx + delta, nr, false);
+		f2fs_unlock_op(sbi);
+	}
+	filemap_invalidate_unlock(mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	/* write out all moved pages, if possible */
+	filemap_invalidate_lock(mapping);
+	filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
+	truncate_pagecache(inode, offset);
+	filemap_invalidate_unlock(mapping);
+
+	if (!ret)
+		f2fs_i_size_write(inode, new_size);
+	return ret;
+}
+
+static int f2fs_expand_inode_data(struct inode *inode, loff_t offset,
+					loff_t len, int mode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_map_blocks map = { .m_next_pgofs = NULL,
+			.m_next_extent = NULL, .m_seg_type = NO_CHECK_TYPE,
+			.m_may_create = true };
+	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
+			.init_gc_type = FG_GC,
+			.should_migrate_blocks = false,
+			.err_gc_skipped = true,
+			.nr_free_secs = 0 };
+	pgoff_t pg_start, pg_end;
+	loff_t new_size;
+	loff_t off_end;
+	block_t expanded = 0;
+	int err;
+
+	err = inode_newsize_ok(inode, (len + offset));
+	if (err)
+		return err;
+
+	err = f2fs_convert_inline_inode(inode);
+	if (err)
+		return err;
+
+	f2fs_balance_fs(sbi, true);
+
+	pg_start = ((unsigned long long)offset) >> PAGE_SHIFT;
+	pg_end = ((unsigned long long)offset + len) >> PAGE_SHIFT;
+	off_end = (offset + len) & (PAGE_SIZE - 1);
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode))
+		map.m_seqblk = pg_start;
+#endif
+	map.m_lblk = pg_start;
+	map.m_len = pg_end - pg_start;
+	if (off_end)
+		map.m_len++;
+
+	if (!map.m_len)
+		return 0;
+
+	if (f2fs_is_pinned_file(inode)) {
+		block_t sec_blks = CAP_BLKS_PER_SEC(sbi);
+		block_t sec_len = roundup(map.m_len, sec_blks);
+
+		map.m_len = sec_blks;
+next_alloc:
+		if (has_not_enough_free_secs(sbi, 0,
+			GET_SEC_FROM_SEG(sbi, overprovision_segments(sbi)))) {
+			f2fs_down_write(&sbi->gc_lock);
+			err = f2fs_gc(sbi, &gc_control);
+			if (err && err != -ENODATA)
+				goto out_err;
+		}
+
+		f2fs_down_write(&sbi->pin_sem);
+
+		f2fs_lock_op(sbi);
+		f2fs_allocate_new_section(sbi, CURSEG_COLD_DATA_PINNED, false);
+		f2fs_unlock_op(sbi);
+
+		map.m_seg_type = CURSEG_COLD_DATA_PINNED;
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_DIO);
+		file_dont_truncate(inode);
+
+		f2fs_up_write(&sbi->pin_sem);
+
+		expanded += map.m_len;
+		sec_len -= map.m_len;
+		map.m_lblk += map.m_len;
+		if (!err && sec_len)
+			goto next_alloc;
+
+		map.m_len = expanded;
+	} else {
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_AIO);
+		expanded = map.m_len;
+	}
+out_err:
+	if (err) {
+		pgoff_t last_off;
+
+		if (!expanded)
+			return err;
+
+		last_off = pg_start + expanded - 1;
+
+		/* update new size to the failed position */
+		new_size = (last_off == pg_end) ? offset + len :
+					(loff_t)(last_off + 1) << PAGE_SHIFT;
+	} else {
+		new_size = ((loff_t)pg_end << PAGE_SHIFT) + off_end;
+	}
+
+	if (new_size > i_size_read(inode)) {
+		if (mode & FALLOC_FL_KEEP_SIZE)
+			file_set_keep_isize(inode);
+		else
+			f2fs_i_size_write(inode, new_size);
+	}
+
+	return err;
+}
+
+static long f2fs_fallocate(struct file *file, int mode,
+				loff_t offset, loff_t len)
+{
+	struct inode *inode = file_inode(file);
+	long ret = 0;
+
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
+		return -ENOSPC;
+	if (!f2fs_is_compress_backend_ready(inode))
+		return -EOPNOTSUPP;
+
+	/* f2fs only support ->fallocate for regular file */
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (IS_ENCRYPTED(inode) &&
+		(mode & (FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE)))
+		return -EOPNOTSUPP;
+
+	/*
+	 * Pinned file should not support partial truncation since the block
+	 * can be used by applications.
+	 */
+	inode_lock(inode);
+	if ((f2fs_compressed_file(inode) || f2fs_is_pinned_file(inode)) &&
+		(mode & (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_COLLAPSE_RANGE |
+			FALLOC_FL_ZERO_RANGE | FALLOC_FL_INSERT_RANGE))) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (f2fs_compressed_file(inode)) {
+			CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+			if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+				ret = f2fs_reserve_compress_blocks(inode, NULL);
+				if (ret < 0)
+					goto out;
+			}
+			ret = f2fs_decompress_inode(inode);
+			if (ret < 0)
+				goto out;
+		} else {
+			ret = -EOPNOTSUPP;
+			goto out;
+		}
+#else
+		ret = -EOPNOTSUPP;
+		goto out;
+#endif
+	}
+
+	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
+			FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |
+			FALLOC_FL_INSERT_RANGE)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(inode) && (mode &
+		(FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE))) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+#endif
+	ret = file_modified(file);
+	if (ret)
+		goto out;
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode) &&
+			f2fs_revoke_deduped_inode(inode, __func__)) {
+		ret = -EIO;
+		goto out;
+	}
+#endif
+	if (mode & FALLOC_FL_PUNCH_HOLE) {
+		int i;
+		if (offset >= inode->i_size)
+			goto out;
+
+		f2fs_info(F2FS_I_SB(inode), "punch ino %lu isize %lld offset %lld len %lld\n",
+			inode->i_ino, i_size_read(inode), offset, len);
+		for (i = 0; i < BITS_TO_LONGS(FI_MAX); i++)
+			f2fs_info(F2FS_I_SB(inode), "flags[%d] %lx", i, F2FS_I(inode)->flags[i]);
+		ret = f2fs_punch_hole(inode, offset, len);
+	} else if (mode & FALLOC_FL_COLLAPSE_RANGE) {
+		ret = f2fs_collapse_range(inode, offset, len);
+	} else if (mode & FALLOC_FL_ZERO_RANGE) {
+		int i;
+		f2fs_info(F2FS_I_SB(inode), "zero ino %lu isize %lld offset %lld len %lld mode %d\n",
+			inode->i_ino, i_size_read(inode), offset, len, mode);
+		for (i = 0; i < BITS_TO_LONGS(FI_MAX); i++)
+			f2fs_info(F2FS_I_SB(inode), "flags[%d] %lx", i, F2FS_I(inode)->flags[i]);
+		ret = f2fs_zero_range(inode, offset, len, mode);
+	} else if (mode & FALLOC_FL_INSERT_RANGE) {
+		ret = f2fs_insert_range(inode, offset, len);
+	} else {
+		ret = f2fs_expand_inode_data(inode, offset, len, mode);
+	}
+
+#ifdef CONFIG_F2FS_APPBOOST
+	/* file change, update mtime */
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, false);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+#else
+	if (!ret) {
+		inode->i_mtime = inode->i_ctime = current_time(inode);
+		f2fs_mark_inode_dirty_sync(inode, false);
+		f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	}
+#endif
+out:
+	inode_unlock(inode);
+
+	trace_f2fs_fallocate(inode, mode, offset, len, ret);
+	return ret;
+}
+
+static int f2fs_release_file(struct inode *inode, struct file *filp)
+{
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner = NULL;
+#endif
+	/*
+	 * f2fs_release_file is called at every close calls. So we should
+	 * not drop any inmemory pages by close called by other process.
+	 */
+	if (!(filp->f_mode & FMODE_WRITE) ||
+			atomic_read(&inode->i_writecount) != 1)
+		return 0;
+
+	inode_lock(inode);
+	f2fs_abort_atomic_write(inode, true);
+	inode_unlock(inode);
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_outer_inode(inode)) {
+		inner = get_inner_inode(inode);
+		if (inner)
+			f2fs_release_file(inner, filp);
+		put_inner_inode(inner);
+	}
+#endif
+	return 0;
+}
+
+static int f2fs_file_flush(struct file *file, fl_owner_t id)
+{
+	struct inode *inode = file_inode(file);
+
+	/*
+	 * If the process doing a transaction is crashed, we should do
+	 * roll-back. Otherwise, other reader/write can see corrupted database
+	 * until all the writers close its file. Since this should be done
+	 * before dropping file lock, it needs to do in ->flush.
+	 */
+	if (F2FS_I(inode)->atomic_write_task == current &&
+				(current->flags & PF_EXITING)) {
+		inode_lock(inode);
+		f2fs_abort_atomic_write(inode, true);
+		inode_unlock(inode);
+	}
+
+	return 0;
+}
+
+static int f2fs_setflags_common(struct inode *inode, u32 iflags, u32 mask)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	u32 masked_flags = fi->i_flags & mask;
+
+	/* mask can be shrunk by flags_valid selector */
+	iflags &= mask;
+
+	/* Is it quota file? Do not allow user to mess with it */
+	if (IS_NOQUOTA(inode))
+		return -EPERM;
+
+	if ((iflags ^ masked_flags) & F2FS_CASEFOLD_FL) {
+		if (!f2fs_sb_has_casefold(F2FS_I_SB(inode)))
+			return -EOPNOTSUPP;
+		if (!f2fs_empty_dir(inode))
+			return -ENOTEMPTY;
+	}
+
+	if (iflags & (F2FS_COMPR_FL | F2FS_NOCOMP_FL)) {
+		if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
+			return -EOPNOTSUPP;
+		if ((iflags & F2FS_COMPR_FL) && (iflags & F2FS_NOCOMP_FL))
+			return -EINVAL;
+	}
+
+	if ((iflags ^ masked_flags) & F2FS_COMPR_FL) {
+		if (masked_flags & F2FS_COMPR_FL) {
+			if (!f2fs_disable_compressed_file(inode))
+				return -EINVAL;
+		} else {
+			/* try to convert inline_data to support compression */
+			int err = f2fs_convert_inline_inode(inode);
+			if (err)
+				return err;
+
+			f2fs_down_write(&F2FS_I(inode)->i_sem);
+			if (!f2fs_may_compress(inode) ||
+					(S_ISREG(inode->i_mode) &&
+					F2FS_HAS_BLOCKS(inode))) {
+				f2fs_up_write(&F2FS_I(inode)->i_sem);
+				return -EINVAL;
+			}
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(inode)) {
+				f2fs_up_write(&F2FS_I(inode)->i_sem);
+				return -EINVAL;
+			}
+#endif
+			if (!may_set_compr_fl) {
+				f2fs_up_write(&F2FS_I(inode)->i_sem);
+				return -EOPNOTSUPP;
+			}
+			err = set_compress_context(inode);
+			f2fs_up_write(&F2FS_I(inode)->i_sem);
+
+			if (err)
+				return err;
+		}
+	}
+
+	fi->i_flags = iflags | (fi->i_flags & ~mask);
+	f2fs_bug_on(F2FS_I_SB(inode), (fi->i_flags & F2FS_COMPR_FL) &&
+					(fi->i_flags & F2FS_NOCOMP_FL));
+
+	if (fi->i_flags & F2FS_PROJINHERIT_FL)
+		set_inode_flag(inode, FI_PROJ_INHERIT);
+	else
+		clear_inode_flag(inode, FI_PROJ_INHERIT);
+
+	inode->i_ctime = current_time(inode);
+	f2fs_set_inode_flags(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
+	return 0;
+}
+
+/* FS_IOC_[GS]ETFLAGS and FS_IOC_FS[GS]ETXATTR support */
+
+/*
+ * To make a new on-disk f2fs i_flag gettable via FS_IOC_GETFLAGS, add an entry
+ * for it to f2fs_fsflags_map[], and add its FS_*_FL equivalent to
+ * F2FS_GETTABLE_FS_FL.  To also make it settable via FS_IOC_SETFLAGS, also add
+ * its FS_*_FL equivalent to F2FS_SETTABLE_FS_FL.
+ *
+ * Translating flags to fsx_flags value used by FS_IOC_FSGETXATTR and
+ * FS_IOC_FSSETXATTR is done by the VFS.
+ */
+
+static const struct {
+	u32 iflag;
+	u32 fsflag;
+} f2fs_fsflags_map[] = {
+	{ F2FS_COMPR_FL,	FS_COMPR_FL },
+	{ F2FS_SYNC_FL,		FS_SYNC_FL },
+	{ F2FS_IMMUTABLE_FL,	FS_IMMUTABLE_FL },
+	{ F2FS_APPEND_FL,	FS_APPEND_FL },
+	{ F2FS_NODUMP_FL,	FS_NODUMP_FL },
+	{ F2FS_NOATIME_FL,	FS_NOATIME_FL },
+	{ F2FS_NOCOMP_FL,	FS_NOCOMP_FL },
+	{ F2FS_INDEX_FL,	FS_INDEX_FL },
+	{ F2FS_DIRSYNC_FL,	FS_DIRSYNC_FL },
+	{ F2FS_PROJINHERIT_FL,	FS_PROJINHERIT_FL },
+	{ F2FS_CASEFOLD_FL,	FS_CASEFOLD_FL },
+};
+
+#define F2FS_GETTABLE_FS_FL (		\
+		FS_COMPR_FL |		\
+		FS_SYNC_FL |		\
+		FS_IMMUTABLE_FL |	\
+		FS_APPEND_FL |		\
+		FS_NODUMP_FL |		\
+		FS_NOATIME_FL |		\
+		FS_NOCOMP_FL |		\
+		FS_INDEX_FL |		\
+		FS_DIRSYNC_FL |		\
+		FS_PROJINHERIT_FL |	\
+		FS_ENCRYPT_FL |		\
+		FS_INLINE_DATA_FL |	\
+		FS_NOCOW_FL |		\
+		FS_VERITY_FL |		\
+		FS_CASEFOLD_FL)
+
+#define F2FS_SETTABLE_FS_FL (		\
+		FS_COMPR_FL |		\
+		FS_SYNC_FL |		\
+		FS_IMMUTABLE_FL |	\
+		FS_APPEND_FL |		\
+		FS_NODUMP_FL |		\
+		FS_NOATIME_FL |		\
+		FS_NOCOMP_FL |		\
+		FS_DIRSYNC_FL |		\
+		FS_PROJINHERIT_FL |	\
+		FS_CASEFOLD_FL)
+
+/* Convert f2fs on-disk i_flags to FS_IOC_{GET,SET}FLAGS flags */
+static inline u32 f2fs_iflags_to_fsflags(u32 iflags)
+{
+	u32 fsflags = 0;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
+		if (iflags & f2fs_fsflags_map[i].iflag)
+			fsflags |= f2fs_fsflags_map[i].fsflag;
+
+	return fsflags;
+}
+
+/* Convert FS_IOC_{GET,SET}FLAGS flags to f2fs on-disk i_flags */
+static inline u32 f2fs_fsflags_to_iflags(u32 fsflags)
+{
+	u32 iflags = 0;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
+		if (fsflags & f2fs_fsflags_map[i].fsflag)
+			iflags |= f2fs_fsflags_map[i].iflag;
+
+	return iflags;
+}
+
+static int f2fs_ioc_getversion(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+
+	return put_user(inode->i_generation, (int __user *)arg);
+}
+
+static int f2fs_ioc_start_atomic_write(struct file *filp, bool truncate)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	loff_t isize;
+	int ret;
+
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
+
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
+
+	if (filp->f_flags & O_DIRECT)
+		return -EINVAL;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	inode_lock(inode);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(inode)) {
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(inode, NULL);
+			if (ret < 0)
+				goto out;
+		}
+		ret = f2fs_decompress_inode(inode);
+		if (ret < 0)
+			goto out;
+	}
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret)
+			goto out;
+	}
+#endif
+
+	if (!f2fs_disable_compressed_file(inode)) {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	if (f2fs_is_atomic_file(inode))
+		goto out;
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto out;
+
+	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+
+	/*
+	 * Should wait end_io to count F2FS_WB_CP_DATA correctly by
+	 * f2fs_is_atomic_file.
+	 */
+	if (get_dirty_pages(inode))
+		f2fs_warn(sbi, "Unexpected flush for atomic writes: ino=%lu, npages=%u",
+			  inode->i_ino, get_dirty_pages(inode));
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret) {
+		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+		goto out;
+	}
+
+	/* Check if the inode already has a COW inode */
+	if (fi->cow_inode == NULL) {
+		/* Create a COW inode for atomic write */
+		struct dentry *dentry = file_dentry(filp);
+		struct inode *dir = d_inode(dentry->d_parent);
+
+		ret = f2fs_get_tmpfile(mnt_userns, dir, &fi->cow_inode);
+		if (ret) {
+			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+			goto out;
+		}
+
+		set_inode_flag(fi->cow_inode, FI_COW_FILE);
+		clear_inode_flag(fi->cow_inode, FI_INLINE_DATA);
+
+		/* Set the COW inode's atomic_inode to the atomic inode */
+		F2FS_I(fi->cow_inode)->atomic_inode = inode;
+	} else {
+		/* Reuse the already created COW inode */
+		f2fs_bug_on(sbi, get_dirty_pages(fi->cow_inode));
+
+		invalidate_mapping_pages(fi->cow_inode->i_mapping, 0, -1);
+
+		ret = f2fs_do_truncate_blocks(fi->cow_inode, 0, true);
+		if (ret) {
+			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+			goto out;
+		}
+	}
+
+	f2fs_write_inode(inode, NULL);
+
+	stat_inc_atomic_inode(inode);
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi) &&
+		IS_ENCRYPTED(inode) && !f2fs_compressed_file(inode) &&
+		f2fs_seqzone_file(inode))
+		if (f2fs_inode_support_dedup(sbi, inode))
+			set_inode_flag(fi->cow_inode, FI_SEQZONE);
+#endif
+	set_inode_flag(inode, FI_ATOMIC_FILE);
+
+	isize = i_size_read(inode);
+	fi->original_i_size = isize;
+	if (truncate) {
+		set_inode_flag(inode, FI_ATOMIC_REPLACE);
+		truncate_inode_pages_final(inode->i_mapping);
+		f2fs_i_size_write(inode, 0);
+		isize = 0;
+	}
+	f2fs_i_size_write(fi->cow_inode, isize);
+
+	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+
+	f2fs_update_time(sbi, REQ_TIME);
+	fi->atomic_write_task = current;
+	stat_update_max_atomic_write(inode);
+	fi->atomic_write_cnt = 0;
+out:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+	return ret;
+}
+
+static int f2fs_ioc_commit_atomic_write(struct file *filp)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	int ret;
+
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
+
+	inode_lock(inode);
+
+	if (f2fs_is_atomic_file(inode)) {
+		ret = f2fs_commit_atomic_write(inode);
+		if (!ret)
+			ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 0, true);
+
+		f2fs_abort_atomic_write(inode, ret);
+	} else {
+		ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 1, false);
+	}
+
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+	return ret;
+}
+
+static int f2fs_ioc_abort_atomic_write(struct file *filp)
+{
+	struct inode *inode = file_inode(filp);
+	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	int ret;
+
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
+
+	if (!inode_owner_or_capable(mnt_userns, inode))
+		return -EACCES;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	inode_lock(inode);
+
+	f2fs_abort_atomic_write(inode, true);
+
+	inode_unlock(inode);
+
+	mnt_drop_write_file(filp);
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	return ret;
+}
+
+static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct super_block *sb = sbi->sb;
+	__u32 in;
+	int ret = 0;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (get_user(in, (__u32 __user *)arg))
+		return -EFAULT;
+
+	if (in != F2FS_GOING_DOWN_FULLSYNC) {
+		ret = mnt_want_write_file(filp);
+		if (ret) {
+			if (ret == -EROFS) {
+				ret = 0;
+				f2fs_stop_checkpoint(sbi, false,
+						STOP_CP_REASON_SHUTDOWN);
+				set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+				trace_f2fs_shutdown(sbi, in, ret);
+			}
+			return ret;
+		}
+	}
+
+	switch (in) {
+	case F2FS_GOING_DOWN_FULLSYNC:
+		ret = freeze_bdev(sb->s_bdev);
+		if (ret)
+			goto out;
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		thaw_bdev(sb->s_bdev);
+		break;
+	case F2FS_GOING_DOWN_METASYNC:
+		/* do checkpoint only */
+		ret = f2fs_sync_fs(sb, 1);
+		if (ret) {
+			if (ret == -EIO)
+				ret = 0;
+			goto out;
+		}
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		break;
+	case F2FS_GOING_DOWN_NOSYNC:
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		break;
+	case F2FS_GOING_DOWN_METAFLUSH:
+		f2fs_sync_meta_pages(sbi, META, LONG_MAX, FS_META_IO);
+		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
+		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
+		break;
+	case F2FS_GOING_DOWN_NEED_FSCK:
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		set_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);
+		set_sbi_flag(sbi, SBI_IS_DIRTY);
+		/* do checkpoint only */
+		ret = f2fs_sync_fs(sb, 1);
+		if (ret == -EIO)
+			ret = 0;
+		goto out;
+	default:
+		ret = -EINVAL;
+		goto out;
+	}
 
-			ret = f2fs_do_zero_range(&dn, index, end);
-			f2fs_put_dnode(&dn);
+	f2fs_stop_gc_thread(sbi);
+	f2fs_stop_discard_thread(sbi);
 
-			f2fs_unlock_op(sbi);
-			filemap_invalidate_unlock(mapping);
-			f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	f2fs_drop_discard_cmd(sbi);
+	clear_opt(sbi, DISCARD);
+
+	f2fs_update_time(sbi, REQ_TIME);
+out:
+	if (in != F2FS_GOING_DOWN_FULLSYNC)
+		mnt_drop_write_file(filp);
+
+	trace_f2fs_shutdown(sbi, in, ret);
+
+	return ret;
+}
+
+static int f2fs_ioc_fitrim(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct super_block *sb = inode->i_sb;
+	struct fstrim_range range;
+	int ret;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (!f2fs_hw_support_discard(F2FS_SB(sb)))
+		return -EOPNOTSUPP;
+
+	if (copy_from_user(&range, (struct fstrim_range __user *)arg,
+				sizeof(range)))
+		return -EFAULT;
+
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	range.minlen = max((unsigned int)range.minlen,
+			   bdev_discard_granularity(sb->s_bdev));
+	ret = f2fs_trim_fs(F2FS_SB(sb), &range);
+	mnt_drop_write_file(filp);
+	if (ret < 0)
+		return ret;
+
+	if (copy_to_user((struct fstrim_range __user *)arg, &range,
+				sizeof(range)))
+		return -EFAULT;
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	return 0;
+}
+
+static bool uuid_is_nonzero(__u8 u[16])
+{
+	int i;
+
+	for (i = 0; i < 16; i++)
+		if (u[i])
+			return true;
+	return false;
+}
+
+static int f2fs_ioc_set_encryption_policy(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(inode)))
+		return -EOPNOTSUPP;
+
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+
+	return fscrypt_ioctl_set_policy(filp, (const void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_policy(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+	return fscrypt_ioctl_get_policy(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_pwsalt(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	u8 encrypt_pw_salt[16];
+	int err;
+
+	if (!f2fs_sb_has_encrypt(sbi))
+		return -EOPNOTSUPP;
+
+	err = mnt_want_write_file(filp);
+	if (err)
+		return err;
+
+	f2fs_down_write(&sbi->sb_lock);
+
+	if (uuid_is_nonzero(sbi->raw_super->encrypt_pw_salt))
+		goto got_it;
+
+	/* update superblock with uuid */
+	generate_random_uuid(sbi->raw_super->encrypt_pw_salt);
+
+	err = f2fs_commit_super(sbi, false);
+	if (err) {
+		/* undo new data */
+		memset(sbi->raw_super->encrypt_pw_salt, 0, 16);
+		goto out_err;
+	}
+got_it:
+	memcpy(encrypt_pw_salt, sbi->raw_super->encrypt_pw_salt, 16);
+out_err:
+	f2fs_up_write(&sbi->sb_lock);
+	mnt_drop_write_file(filp);
+
+	if (!err && copy_to_user((__u8 __user *)arg, encrypt_pw_salt, 16))
+		err = -EFAULT;
+
+	return err;
+}
+
+static int f2fs_ioc_get_encryption_policy_ex(struct file *filp,
+					     unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_get_policy_ex(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_add_encryption_key(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_add_key(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_remove_encryption_key(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_remove_key(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_remove_encryption_key_all_users(struct file *filp,
+						    unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_remove_key_all_users(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_key_status(struct file *filp,
+					      unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_get_key_status(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_get_encryption_nonce(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
+
+	return fscrypt_ioctl_get_nonce(filp, (void __user *)arg);
+}
+
+static int f2fs_ioc_gc(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
+			.no_bg_gc = false,
+			.should_migrate_blocks = false,
+			.nr_free_secs = 0 };
+	__u32 sync;
+	int ret;
 
-			f2fs_balance_fs(sbi, dn.node_changed);
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-			if (ret)
-				goto out;
+	if (get_user(sync, (__u32 __user *)arg))
+		return -EFAULT;
 
-			index = end;
-			new_size = max_t(loff_t, new_size,
-					(loff_t)index << PAGE_SHIFT);
-		}
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-		if (off_end) {
-			ret = fill_zero(inode, pg_end, 0, off_end);
-			if (ret)
-				goto out;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-			new_size = max_t(loff_t, new_size, offset + len);
+	if (!sync) {
+		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
+			ret = -EBUSY;
+			goto out;
 		}
+	} else {
+		f2fs_down_write(&sbi->gc_lock);
 	}
 
+	gc_control.init_gc_type = sync ? FG_GC : BG_GC;
+	gc_control.err_gc_skipped = sync;
+	ret = f2fs_gc(sbi, &gc_control);
 out:
-	if (new_size > i_size_read(inode)) {
-		if (mode & FALLOC_FL_KEEP_SIZE)
-			file_set_keep_isize(inode);
-		else
-			f2fs_i_size_write(inode, new_size);
-	}
+	mnt_drop_write_file(filp);
 	return ret;
 }
 
-static int f2fs_insert_range(struct inode *inode, loff_t offset, loff_t len)
+static int __f2fs_ioc_gc_range(struct file *filp, struct f2fs_gc_range *range)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct address_space *mapping = inode->i_mapping;
-	pgoff_t nr, pg_start, pg_end, delta, idx;
-	loff_t new_size;
-	int ret = 0;
-
-	new_size = i_size_read(inode) + len;
-	ret = inode_newsize_ok(inode, new_size);
-	if (ret)
-		return ret;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
+	struct f2fs_gc_control gc_control = {
+			.init_gc_type = range->sync ? FG_GC : BG_GC,
+			.no_bg_gc = false,
+			.should_migrate_blocks = false,
+			.err_gc_skipped = range->sync,
+			.nr_free_secs = 0 };
+	u64 end;
+	int ret;
 
-	if (offset >= i_size_read(inode))
-		return -EINVAL;
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	/* insert range should be aligned to block size of f2fs. */
-	if (offset & (F2FS_BLKSIZE - 1) || len & (F2FS_BLKSIZE - 1))
+	end = range->start + range->len;
+	if (end < range->start || range->start < MAIN_BLKADDR(sbi) ||
+					end >= MAX_BLKADDR(sbi))
 		return -EINVAL;
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		return ret;
-
-	f2fs_balance_fs(sbi, true);
-
-	filemap_invalidate_lock(mapping);
-	ret = f2fs_truncate_blocks(inode, i_size_read(inode), true);
-	filemap_invalidate_unlock(mapping);
-	if (ret)
-		return ret;
-
-	/* write out all dirty pages from offset */
-	ret = filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
+	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	pg_start = offset >> PAGE_SHIFT;
-	pg_end = (offset + len) >> PAGE_SHIFT;
-	delta = pg_end - pg_start;
-	idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-
-	/* avoid gc operation during block exchange */
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(mapping);
-	truncate_pagecache(inode, offset);
-
-	while (!ret && idx > pg_start) {
-		nr = idx - pg_start;
-		if (nr > delta)
-			nr = delta;
-		idx -= nr;
-
-		f2fs_lock_op(sbi);
-		f2fs_drop_extent_tree(inode);
-
-		ret = __exchange_data_block(inode, inode, idx,
-					idx + delta, nr, false);
-		f2fs_unlock_op(sbi);
+do_more:
+	if (!range->sync) {
+		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
+			ret = -EBUSY;
+			goto out;
+		}
+	} else {
+		f2fs_down_write(&sbi->gc_lock);
 	}
-	filemap_invalidate_unlock(mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-
-	/* write out all moved pages, if possible */
-	filemap_invalidate_lock(mapping);
-	filemap_write_and_wait_range(mapping, offset, LLONG_MAX);
-	truncate_pagecache(inode, offset);
-	filemap_invalidate_unlock(mapping);
 
-	if (!ret)
-		f2fs_i_size_write(inode, new_size);
+	gc_control.victim_segno = GET_SEGNO(sbi, range->start);
+	ret = f2fs_gc(sbi, &gc_control);
+	if (ret) {
+		if (ret == -EBUSY)
+			ret = -EAGAIN;
+		goto out;
+	}
+	range->start += CAP_BLKS_PER_SEC(sbi);
+	if (range->start <= end)
+		goto do_more;
+out:
+	mnt_drop_write_file(filp);
 	return ret;
 }
 
-static int f2fs_expand_inode_data(struct inode *inode, loff_t offset,
-					loff_t len, int mode)
+static int f2fs_ioc_gc_range(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_map_blocks map = { .m_next_pgofs = NULL,
-			.m_next_extent = NULL, .m_seg_type = NO_CHECK_TYPE,
-			.m_may_create = true };
-	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
-			.init_gc_type = FG_GC,
-			.should_migrate_blocks = false,
-			.err_gc_skipped = true,
-			.nr_free_secs = 0 };
-	pgoff_t pg_start, pg_end;
-	loff_t new_size;
-	loff_t off_end;
-	block_t expanded = 0;
-	int err;
-
-	err = inode_newsize_ok(inode, (len + offset));
-	if (err)
-		return err;
-
-	err = f2fs_convert_inline_inode(inode);
-	if (err)
-		return err;
-
-	f2fs_balance_fs(sbi, true);
-
-	pg_start = ((unsigned long long)offset) >> PAGE_SHIFT;
-	pg_end = ((unsigned long long)offset + len) >> PAGE_SHIFT;
-	off_end = (offset + len) & (PAGE_SIZE - 1);
+	struct f2fs_gc_range range;
 
-	map.m_lblk = pg_start;
-	map.m_len = pg_end - pg_start;
-	if (off_end)
-		map.m_len++;
+	if (copy_from_user(&range, (struct f2fs_gc_range __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
+	return __f2fs_ioc_gc_range(filp, &range);
+}
 
-	if (!map.m_len)
-		return 0;
+static int f2fs_ioc_write_checkpoint(struct file *filp)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
 
-	if (f2fs_is_pinned_file(inode)) {
-		block_t sec_blks = CAP_BLKS_PER_SEC(sbi);
-		block_t sec_len = roundup(map.m_len, sec_blks);
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-		map.m_len = sec_blks;
-next_alloc:
-		if (has_not_enough_free_secs(sbi, 0,
-			GET_SEC_FROM_SEG(sbi, overprovision_segments(sbi)))) {
-			f2fs_down_write(&sbi->gc_lock);
-			err = f2fs_gc(sbi, &gc_control);
-			if (err && err != -ENODATA)
-				goto out_err;
-		}
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-		f2fs_down_write(&sbi->pin_sem);
+	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
+		f2fs_info(sbi, "Skipping Checkpoint. Checkpoints currently disabled.");
+		return -EINVAL;
+	}
 
-		f2fs_lock_op(sbi);
-		f2fs_allocate_new_section(sbi, CURSEG_COLD_DATA_PINNED, false);
-		f2fs_unlock_op(sbi);
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-		map.m_seg_type = CURSEG_COLD_DATA_PINNED;
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_DIO);
-		file_dont_truncate(inode);
+	ret = f2fs_sync_fs(sbi->sb, 1);
 
-		f2fs_up_write(&sbi->pin_sem);
+	mnt_drop_write_file(filp);
+	return ret;
+}
 
-		expanded += map.m_len;
-		sec_len -= map.m_len;
-		map.m_lblk += map.m_len;
-		if (!err && sec_len)
-			goto next_alloc;
+static int f2fs_defragment_range(struct f2fs_sb_info *sbi,
+					struct file *filp,
+					struct f2fs_defragment *range)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_map_blocks map = { .m_next_extent = NULL,
+					.m_seg_type = NO_CHECK_TYPE,
+					.m_may_create = false };
+	struct extent_info ei = {};
+	pgoff_t pg_start, pg_end, next_pgofs;
+	unsigned int blk_per_seg = sbi->blocks_per_seg;
+	unsigned int total = 0, sec_num;
+	block_t blk_end = 0;
+	bool fragmented = false;
+	int err;
 
-		map.m_len = expanded;
-	} else {
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRE_AIO);
-		expanded = map.m_len;
-	}
-out_err:
-	if (err) {
-		pgoff_t last_off;
+	pg_start = range->start >> PAGE_SHIFT;
+	pg_end = (range->start + range->len) >> PAGE_SHIFT;
 
-		if (!expanded)
-			return err;
+	f2fs_balance_fs(sbi, true);
 
-		last_off = pg_start + expanded - 1;
+	inode_lock(inode);
 
-		/* update new size to the failed position */
-		new_size = (last_off == pg_end) ? offset + len :
-					(loff_t)(last_off + 1) << PAGE_SHIFT;
-	} else {
-		new_size = ((loff_t)pg_end << PAGE_SHIFT) + off_end;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		err = f2fs_revoke_deduped_inode(inode, __func__);
+		if (err)
+			goto unlock_out;
 	}
+#endif
 
-	if (new_size > i_size_read(inode)) {
-		if (mode & FALLOC_FL_KEEP_SIZE)
-			file_set_keep_isize(inode);
-		else
-			f2fs_i_size_write(inode, new_size);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(inode)) {
+		CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+		if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			err = f2fs_reserve_compress_blocks(inode, NULL);
+			if (err < 0)
+				goto unlock_out;
+		}
+		err = f2fs_decompress_inode(inode);
+		if (err < 0)
+			goto unlock_out;
+	}
+#endif
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED) ||
+		f2fs_is_atomic_file(inode)) {
+		err = -EINVAL;
+		goto unlock_out;
 	}
 
-	return err;
-}
-
-static long f2fs_fallocate(struct file *file, int mode,
-				loff_t offset, loff_t len)
-{
-	struct inode *inode = file_inode(file);
-	long ret = 0;
+	/* if in-place-update policy is enabled, don't waste time here */
+	set_inode_flag(inode, FI_OPU_WRITE);
+	if (f2fs_should_update_inplace(inode, NULL)) {
+		err = -EINVAL;
+		goto out;
+	}
 
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
-	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
-		return -ENOSPC;
-	if (!f2fs_is_compress_backend_ready(inode))
-		return -EOPNOTSUPP;
+	/* writeback all dirty pages in the range */
+	err = filemap_write_and_wait_range(inode->i_mapping, range->start,
+						range->start + range->len - 1);
+	if (err)
+		goto out;
 
-	/* f2fs only support ->fallocate for regular file */
-	if (!S_ISREG(inode->i_mode))
-		return -EINVAL;
+	/*
+	 * lookup mapping info in extent cache, skip defragmenting if physical
+	 * block addresses are continuous.
+	 */
+	if (f2fs_lookup_read_extent_cache(inode, pg_start, &ei)) {
+		if ((pgoff_t)ei.fofs + ei.len >= pg_end)
+			goto out;
+	}
 
-	if (IS_ENCRYPTED(inode) &&
-		(mode & (FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_INSERT_RANGE)))
-		return -EOPNOTSUPP;
+	map.m_lblk = pg_start;
+	map.m_next_pgofs = &next_pgofs;
 
 	/*
-	 * Pinned file should not support partial truncation since the block
-	 * can be used by applications.
+	 * lookup mapping info in dnode page cache, skip defragmenting if all
+	 * physical block addresses are continuous even if there are hole(s)
+	 * in logical blocks.
 	 */
-	if ((f2fs_compressed_file(inode) || f2fs_is_pinned_file(inode)) &&
-		(mode & (FALLOC_FL_PUNCH_HOLE | FALLOC_FL_COLLAPSE_RANGE |
-			FALLOC_FL_ZERO_RANGE | FALLOC_FL_INSERT_RANGE)))
-		return -EOPNOTSUPP;
+	while (map.m_lblk < pg_end) {
+		map.m_len = pg_end - map.m_lblk;
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
+		if (err)
+			goto out;
 
-	if (mode & ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE |
-			FALLOC_FL_COLLAPSE_RANGE | FALLOC_FL_ZERO_RANGE |
-			FALLOC_FL_INSERT_RANGE))
-		return -EOPNOTSUPP;
+		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
+			map.m_lblk = next_pgofs;
+			continue;
+		}
 
-	inode_lock(inode);
+		if (blk_end && blk_end != map.m_pblk)
+			fragmented = true;
 
-	ret = file_modified(file);
-	if (ret)
-		goto out;
+		/* record total count of block that we're going to move */
+		total += map.m_len;
 
-	if (mode & FALLOC_FL_PUNCH_HOLE) {
-		if (offset >= inode->i_size)
-			goto out;
+		blk_end = map.m_pblk + map.m_len;
 
-		ret = f2fs_punch_hole(inode, offset, len);
-	} else if (mode & FALLOC_FL_COLLAPSE_RANGE) {
-		ret = f2fs_collapse_range(inode, offset, len);
-	} else if (mode & FALLOC_FL_ZERO_RANGE) {
-		ret = f2fs_zero_range(inode, offset, len, mode);
-	} else if (mode & FALLOC_FL_INSERT_RANGE) {
-		ret = f2fs_insert_range(inode, offset, len);
-	} else {
-		ret = f2fs_expand_inode_data(inode, offset, len, mode);
+		map.m_lblk += map.m_len;
 	}
 
-	if (!ret) {
-		inode->i_mtime = inode->i_ctime = current_time(inode);
-		f2fs_mark_inode_dirty_sync(inode, false);
-		f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	if (!fragmented) {
+		total = 0;
+		goto out;
 	}
 
-out:
-	inode_unlock(inode);
-
-	trace_f2fs_fallocate(inode, mode, offset, len, ret);
-	return ret;
-}
-
-static int f2fs_release_file(struct inode *inode, struct file *filp)
-{
-	/*
-	 * f2fs_release_file is called at every close calls. So we should
-	 * not drop any inmemory pages by close called by other process.
-	 */
-	if (!(filp->f_mode & FMODE_WRITE) ||
-			atomic_read(&inode->i_writecount) != 1)
-		return 0;
-
-	inode_lock(inode);
-	f2fs_abort_atomic_write(inode, true);
-	inode_unlock(inode);
-
-	return 0;
-}
-
-static int f2fs_file_flush(struct file *file, fl_owner_t id)
-{
-	struct inode *inode = file_inode(file);
+	sec_num = DIV_ROUND_UP(total, CAP_BLKS_PER_SEC(sbi));
 
 	/*
-	 * If the process doing a transaction is crashed, we should do
-	 * roll-back. Otherwise, other reader/write can see corrupted database
-	 * until all the writers close its file. Since this should be done
-	 * before dropping file lock, it needs to do in ->flush.
+	 * make sure there are enough free section for LFS allocation, this can
+	 * avoid defragment running in SSR mode when free section are allocated
+	 * intensively
 	 */
-	if (F2FS_I(inode)->atomic_write_task == current &&
-				(current->flags & PF_EXITING)) {
-		inode_lock(inode);
-		f2fs_abort_atomic_write(inode, true);
-		inode_unlock(inode);
+	if (has_not_enough_free_secs(sbi, 0, sec_num)) {
+		err = -EAGAIN;
+		goto out;
 	}
 
-	return 0;
-}
-
-static int f2fs_setflags_common(struct inode *inode, u32 iflags, u32 mask)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	u32 masked_flags = fi->i_flags & mask;
+	map.m_lblk = pg_start;
+	map.m_len = pg_end - pg_start;
+	total = 0;
 
-	/* mask can be shrunk by flags_valid selector */
-	iflags &= mask;
+	while (map.m_lblk < pg_end) {
+		pgoff_t idx;
+		int cnt = 0;
 
-	/* Is it quota file? Do not allow user to mess with it */
-	if (IS_NOQUOTA(inode))
-		return -EPERM;
+do_map:
+		map.m_len = pg_end - map.m_lblk;
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
+		if (err)
+			goto clear_out;
 
-	if ((iflags ^ masked_flags) & F2FS_CASEFOLD_FL) {
-		if (!f2fs_sb_has_casefold(F2FS_I_SB(inode)))
-			return -EOPNOTSUPP;
-		if (!f2fs_empty_dir(inode))
-			return -ENOTEMPTY;
-	}
+		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
+			map.m_lblk = next_pgofs;
+			goto check;
+		}
 
-	if (iflags & (F2FS_COMPR_FL | F2FS_NOCOMP_FL)) {
-		if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
-			return -EOPNOTSUPP;
-		if ((iflags & F2FS_COMPR_FL) && (iflags & F2FS_NOCOMP_FL))
-			return -EINVAL;
-	}
+		set_inode_flag(inode, FI_SKIP_WRITES);
 
-	if ((iflags ^ masked_flags) & F2FS_COMPR_FL) {
-		if (masked_flags & F2FS_COMPR_FL) {
-			if (!f2fs_disable_compressed_file(inode))
-				return -EINVAL;
-		} else {
-			/* try to convert inline_data to support compression */
-			int err = f2fs_convert_inline_inode(inode);
-			if (err)
-				return err;
+		idx = map.m_lblk;
+		while (idx < map.m_lblk + map.m_len && cnt < blk_per_seg) {
+			struct page *page;
 
-			f2fs_down_write(&F2FS_I(inode)->i_sem);
-			if (!f2fs_may_compress(inode) ||
-					(S_ISREG(inode->i_mode) &&
-					F2FS_HAS_BLOCKS(inode))) {
-				f2fs_up_write(&F2FS_I(inode)->i_sem);
-				return -EINVAL;
+			page = f2fs_get_lock_data_page(inode, idx, true);
+			if (IS_ERR(page)) {
+				err = PTR_ERR(page);
+				goto clear_out;
 			}
-			err = set_compress_context(inode);
-			f2fs_up_write(&F2FS_I(inode)->i_sem);
 
-			if (err)
-				return err;
+			f2fs_wait_on_page_writeback(page, DATA, true, true);
+
+			set_page_dirty(page);
+			set_page_private_gcing(page);
+			f2fs_put_page(page, 1);
+
+			idx++;
+			cnt++;
+			total++;
 		}
-	}
 
-	fi->i_flags = iflags | (fi->i_flags & ~mask);
-	f2fs_bug_on(F2FS_I_SB(inode), (fi->i_flags & F2FS_COMPR_FL) &&
-					(fi->i_flags & F2FS_NOCOMP_FL));
+		map.m_lblk = idx;
+check:
+		if (map.m_lblk < pg_end && cnt < blk_per_seg)
+			goto do_map;
 
-	if (fi->i_flags & F2FS_PROJINHERIT_FL)
-		set_inode_flag(inode, FI_PROJ_INHERIT);
-	else
-		clear_inode_flag(inode, FI_PROJ_INHERIT);
+		clear_inode_flag(inode, FI_SKIP_WRITES);
 
-	inode->i_ctime = current_time(inode);
-	f2fs_set_inode_flags(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
-	return 0;
+		err = filemap_fdatawrite(inode->i_mapping);
+		if (err)
+			goto out;
+	}
+clear_out:
+	clear_inode_flag(inode, FI_SKIP_WRITES);
+out:
+	clear_inode_flag(inode, FI_OPU_WRITE);
+unlock_out:
+	inode_unlock(inode);
+	if (!err)
+		range->len = (u64)total << PAGE_SHIFT;
+	return err;
 }
 
-/* FS_IOC_[GS]ETFLAGS and FS_IOC_FS[GS]ETXATTR support */
-
-/*
- * To make a new on-disk f2fs i_flag gettable via FS_IOC_GETFLAGS, add an entry
- * for it to f2fs_fsflags_map[], and add its FS_*_FL equivalent to
- * F2FS_GETTABLE_FS_FL.  To also make it settable via FS_IOC_SETFLAGS, also add
- * its FS_*_FL equivalent to F2FS_SETTABLE_FS_FL.
- *
- * Translating flags to fsx_flags value used by FS_IOC_FSGETXATTR and
- * FS_IOC_FSSETXATTR is done by the VFS.
- */
+static int f2fs_ioc_defragment(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_defragment range;
+	int err;
 
-static const struct {
-	u32 iflag;
-	u32 fsflag;
-} f2fs_fsflags_map[] = {
-	{ F2FS_COMPR_FL,	FS_COMPR_FL },
-	{ F2FS_SYNC_FL,		FS_SYNC_FL },
-	{ F2FS_IMMUTABLE_FL,	FS_IMMUTABLE_FL },
-	{ F2FS_APPEND_FL,	FS_APPEND_FL },
-	{ F2FS_NODUMP_FL,	FS_NODUMP_FL },
-	{ F2FS_NOATIME_FL,	FS_NOATIME_FL },
-	{ F2FS_NOCOMP_FL,	FS_NOCOMP_FL },
-	{ F2FS_INDEX_FL,	FS_INDEX_FL },
-	{ F2FS_DIRSYNC_FL,	FS_DIRSYNC_FL },
-	{ F2FS_PROJINHERIT_FL,	FS_PROJINHERIT_FL },
-	{ F2FS_CASEFOLD_FL,	FS_CASEFOLD_FL },
-};
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-#define F2FS_GETTABLE_FS_FL (		\
-		FS_COMPR_FL |		\
-		FS_SYNC_FL |		\
-		FS_IMMUTABLE_FL |	\
-		FS_APPEND_FL |		\
-		FS_NODUMP_FL |		\
-		FS_NOATIME_FL |		\
-		FS_NOCOMP_FL |		\
-		FS_INDEX_FL |		\
-		FS_DIRSYNC_FL |		\
-		FS_PROJINHERIT_FL |	\
-		FS_ENCRYPT_FL |		\
-		FS_INLINE_DATA_FL |	\
-		FS_NOCOW_FL |		\
-		FS_VERITY_FL |		\
-		FS_CASEFOLD_FL)
+	if (!S_ISREG(inode->i_mode) || f2fs_is_atomic_file(inode))
+		return -EINVAL;
 
-#define F2FS_SETTABLE_FS_FL (		\
-		FS_COMPR_FL |		\
-		FS_SYNC_FL |		\
-		FS_IMMUTABLE_FL |	\
-		FS_APPEND_FL |		\
-		FS_NODUMP_FL |		\
-		FS_NOATIME_FL |		\
-		FS_NOCOMP_FL |		\
-		FS_DIRSYNC_FL |		\
-		FS_PROJINHERIT_FL |	\
-		FS_CASEFOLD_FL)
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-/* Convert f2fs on-disk i_flags to FS_IOC_{GET,SET}FLAGS flags */
-static inline u32 f2fs_iflags_to_fsflags(u32 iflags)
-{
-	u32 fsflags = 0;
-	int i;
+	if (copy_from_user(&range, (struct f2fs_defragment __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
 
-	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
-		if (iflags & f2fs_fsflags_map[i].iflag)
-			fsflags |= f2fs_fsflags_map[i].fsflag;
+	/* verify alignment of offset & size */
+	if (range.start & (F2FS_BLKSIZE - 1) || range.len & (F2FS_BLKSIZE - 1))
+		return -EINVAL;
 
-	return fsflags;
-}
+	if (unlikely((range.start + range.len) >> PAGE_SHIFT >
+					max_file_blocks(inode)))
+		return -EINVAL;
 
-/* Convert FS_IOC_{GET,SET}FLAGS flags to f2fs on-disk i_flags */
-static inline u32 f2fs_fsflags_to_iflags(u32 fsflags)
-{
-	u32 iflags = 0;
-	int i;
+	err = mnt_want_write_file(filp);
+	if (err)
+		return err;
 
-	for (i = 0; i < ARRAY_SIZE(f2fs_fsflags_map); i++)
-		if (fsflags & f2fs_fsflags_map[i].fsflag)
-			iflags |= f2fs_fsflags_map[i].iflag;
+	err = f2fs_defragment_range(sbi, filp, &range);
+	mnt_drop_write_file(filp);
 
-	return iflags;
-}
+	f2fs_update_time(sbi, REQ_TIME);
+	if (err < 0)
+		return err;
 
-static int f2fs_ioc_getversion(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
+	if (copy_to_user((struct f2fs_defragment __user *)arg, &range,
+							sizeof(range)))
+		return -EFAULT;
 
-	return put_user(inode->i_generation, (int __user *)arg);
+	return 0;
 }
 
-static int f2fs_ioc_start_atomic_write(struct file *filp, bool truncate)
+static int f2fs_move_file_range(struct file *file_in, loff_t pos_in,
+			struct file *file_out, loff_t pos_out, size_t len)
 {
-	struct inode *inode = file_inode(filp);
-	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	loff_t isize;
+	struct inode *src = file_inode(file_in);
+	struct inode *dst = file_inode(file_out);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(src);
+	size_t olen = len, dst_max_i_size = 0;
+	size_t dst_osize;
 	int ret;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (file_in->f_path.mnt != file_out->f_path.mnt ||
+				src->i_sb != dst->i_sb)
+		return -EXDEV;
 
-	if (!inode_owner_or_capable(mnt_userns, inode))
-		return -EACCES;
+	if (unlikely(f2fs_readonly(src->i_sb)))
+		return -EROFS;
 
-	if (!S_ISREG(inode->i_mode))
+	if (!S_ISREG(src->i_mode) || !S_ISREG(dst->i_mode))
 		return -EINVAL;
 
-	if (filp->f_flags & O_DIRECT)
+	if (IS_ENCRYPTED(src) || IS_ENCRYPTED(dst))
+		return -EOPNOTSUPP;
+
+	if (pos_out < 0 || pos_in < 0)
 		return -EINVAL;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	if (src == dst) {
+		if (pos_in == pos_out)
+			return 0;
+		if (pos_out > pos_in && pos_out < pos_in + len)
+			return -EINVAL;
+	}
 
-	inode_lock(inode);
+	inode_lock(src);
+	if (src != dst) {
+		ret = -EBUSY;
+		if (!inode_trylock(dst))
+			goto out;
+	}
 
-	if (!f2fs_disable_compressed_file(inode)) {
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(src) || f2fs_seqzone_file(dst)) {
+		ret = -EOPNOTSUPP;
+		goto out_unlock;
+	}
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(src);
+	if (f2fs_is_outer_inode(src)) {
+		ret = f2fs_revoke_deduped_inode(src, __func__);
+		if (ret)
+			goto out_unlock;
+	}
+
+	mark_file_modified(dst);
+	if (f2fs_is_outer_inode(dst)) {
+		ret = f2fs_revoke_deduped_inode(dst, __func__);
+		if (ret)
+			goto out_unlock;
+	}
+#endif
+
+	if (f2fs_is_atomic_file(src) || f2fs_is_atomic_file(dst)) {
 		ret = -EINVAL;
-		goto out;
+		goto out_unlock;
 	}
 
-	if (f2fs_is_atomic_file(inode))
-		goto out;
+	ret = -EINVAL;
+	if (pos_in + len > src->i_size || pos_in + len < pos_in)
+		goto out_unlock;
+	if (len == 0)
+		olen = len = src->i_size - pos_in;
+	if (pos_in + len == src->i_size)
+		len = ALIGN(src->i_size, F2FS_BLKSIZE) - pos_in;
+	if (len == 0) {
+		ret = 0;
+		goto out_unlock;
+	}
 
-	ret = f2fs_convert_inline_inode(inode);
+	dst_osize = dst->i_size;
+	if (pos_out + olen > dst->i_size)
+		dst_max_i_size = pos_out + olen;
+
+	/* verify the end result is block aligned */
+	if (!IS_ALIGNED(pos_in, F2FS_BLKSIZE) ||
+			!IS_ALIGNED(pos_in + len, F2FS_BLKSIZE) ||
+			!IS_ALIGNED(pos_out, F2FS_BLKSIZE))
+		goto out_unlock;
+
+	ret = f2fs_convert_inline_inode(src);
 	if (ret)
-		goto out;
+		goto out_unlock;
 
-	f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+	ret = f2fs_convert_inline_inode(dst);
+	if (ret)
+		goto out_unlock;
 
-	/*
-	 * Should wait end_io to count F2FS_WB_CP_DATA correctly by
-	 * f2fs_is_atomic_file.
-	 */
-	if (get_dirty_pages(inode))
-		f2fs_warn(sbi, "Unexpected flush for atomic writes: ino=%lu, npages=%u",
-			  inode->i_ino, get_dirty_pages(inode));
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret) {
-		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-		goto out;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(src)) {
+		CLEAR_IFLAG_IF_SET(src, F2FS_NOCOMP_FL);
+		CLEAR_IFLAG_IF_SET(dst, F2FS_NOCOMP_FL);
+		if (is_inode_flag_set(src, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(src, NULL);
+			if (ret < 0)
+				goto out_unlock;
+		}
+		ret = f2fs_decompress_inode(src);
+		if (ret < 0)
+			goto out_unlock;
 	}
 
-	/* Check if the inode already has a COW inode */
-	if (fi->cow_inode == NULL) {
-		/* Create a COW inode for atomic write */
-		struct dentry *dentry = file_dentry(filp);
-		struct inode *dir = d_inode(dentry->d_parent);
-
-		ret = f2fs_get_tmpfile(mnt_userns, dir, &fi->cow_inode);
-		if (ret) {
-			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-			goto out;
+	if (f2fs_compressed_file(dst)) {
+		if (is_inode_flag_set(dst, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(dst, NULL);
+			if (ret < 0)
+				goto out_unlock;
 		}
+		ret = f2fs_decompress_inode(dst);
+		if (ret < 0)
+			goto out_unlock;
+	}
+#endif
 
-		set_inode_flag(fi->cow_inode, FI_COW_FILE);
-		clear_inode_flag(fi->cow_inode, FI_INLINE_DATA);
+	/* write out all dirty pages from offset */
+	ret = filemap_write_and_wait_range(src->i_mapping,
+					pos_in, pos_in + len);
+	if (ret)
+		goto out_unlock;
 
-		/* Set the COW inode's atomic_inode to the atomic inode */
-		F2FS_I(fi->cow_inode)->atomic_inode = inode;
-	} else {
-		/* Reuse the already created COW inode */
-		f2fs_bug_on(sbi, get_dirty_pages(fi->cow_inode));
+	ret = filemap_write_and_wait_range(dst->i_mapping,
+					pos_out, pos_out + len);
+	if (ret)
+		goto out_unlock;
 
-		invalidate_mapping_pages(fi->cow_inode->i_mapping, 0, -1);
+	f2fs_balance_fs(sbi, true);
 
-		ret = f2fs_do_truncate_blocks(fi->cow_inode, 0, true);
-		if (ret) {
-			f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-			goto out;
-		}
+	f2fs_down_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
+	if (src != dst) {
+		ret = -EBUSY;
+		if (!f2fs_down_write_trylock(&F2FS_I(dst)->i_gc_rwsem[WRITE]))
+			goto out_src;
 	}
 
-	f2fs_write_inode(inode, NULL);
-
-	stat_inc_atomic_inode(inode);
-
-	set_inode_flag(inode, FI_ATOMIC_FILE);
+	f2fs_lock_op(sbi);
+	ret = __exchange_data_block(src, dst, pos_in >> F2FS_BLKSIZE_BITS,
+				pos_out >> F2FS_BLKSIZE_BITS,
+				len >> F2FS_BLKSIZE_BITS, false);
 
-	isize = i_size_read(inode);
-	fi->original_i_size = isize;
-	if (truncate) {
-		set_inode_flag(inode, FI_ATOMIC_REPLACE);
-		truncate_inode_pages_final(inode->i_mapping);
-		f2fs_i_size_write(inode, 0);
-		isize = 0;
+	if (!ret) {
+		if (dst_max_i_size)
+			f2fs_i_size_write(dst, dst_max_i_size);
+		else if (dst_osize != dst->i_size)
+			f2fs_i_size_write(dst, dst_osize);
 	}
-	f2fs_i_size_write(fi->cow_inode, isize);
-
-	f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+	f2fs_unlock_op(sbi);
 
+	if (src != dst)
+		f2fs_up_write(&F2FS_I(dst)->i_gc_rwsem[WRITE]);
+out_src:
+	f2fs_up_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
+#ifdef CONFIG_F2FS_APPBOOST
+	src->i_mtime = src->i_ctime = current_time(src);
+	f2fs_mark_inode_dirty_sync(src, false);
+	if (src != dst) {
+		dst->i_mtime = dst->i_ctime = current_time(dst);
+		f2fs_mark_inode_dirty_sync(dst, false);
+	}
 	f2fs_update_time(sbi, REQ_TIME);
-	fi->atomic_write_task = current;
-	stat_update_max_atomic_write(inode);
-	fi->atomic_write_cnt = 0;
+#endif
+out_unlock:
+	if (src != dst)
+		inode_unlock(dst);
 out:
-	inode_unlock(inode);
-	mnt_drop_write_file(filp);
+	inode_unlock(src);
 	return ret;
 }
 
-static int f2fs_ioc_commit_atomic_write(struct file *filp)
+static int __f2fs_ioc_move_range(struct file *filp,
+				struct f2fs_move_range *range)
 {
-	struct inode *inode = file_inode(filp);
-	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
-	int ret;
+	struct fd dst;
+	int err;
 
-	if (!(filp->f_mode & FMODE_WRITE))
+	if (!(filp->f_mode & FMODE_READ) ||
+			!(filp->f_mode & FMODE_WRITE))
 		return -EBADF;
 
-	if (!inode_owner_or_capable(mnt_userns, inode))
-		return -EACCES;
+	dst = fdget(range->dst_fd);
+	if (!dst.file)
+		return -EBADF;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	if (!(dst.file->f_mode & FMODE_WRITE)) {
+		err = -EBADF;
+		goto err_out;
+	}
 
-	f2fs_balance_fs(F2FS_I_SB(inode), true);
+	err = mnt_want_write_file(filp);
+	if (err)
+		goto err_out;
 
-	inode_lock(inode);
+	err = f2fs_move_file_range(filp, range->pos_in, dst.file,
+					range->pos_out, range->len);
 
-	if (f2fs_is_atomic_file(inode)) {
-		ret = f2fs_commit_atomic_write(inode);
-		if (!ret)
-			ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 0, true);
+	mnt_drop_write_file(filp);
+err_out:
+	fdput(dst);
+	return err;
+}
 
-		f2fs_abort_atomic_write(inode, ret);
-	} else {
-		ret = f2fs_do_sync_file(filp, 0, LLONG_MAX, 1, false);
-	}
+static int f2fs_ioc_move_range(struct file *filp, unsigned long arg)
+{
+	struct f2fs_move_range range;
 
-	inode_unlock(inode);
-	mnt_drop_write_file(filp);
-	return ret;
+	if (copy_from_user(&range, (struct f2fs_move_range __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
+	return __f2fs_ioc_move_range(filp, &range);
 }
 
-static int f2fs_ioc_abort_atomic_write(struct file *filp)
+static int f2fs_ioc_flush_device(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct user_namespace *mnt_userns = file_mnt_user_ns(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct sit_info *sm = SIT_I(sbi);
+	unsigned int start_segno = 0, end_segno = 0;
+	unsigned int dev_start_segno = 0, dev_end_segno = 0;
+	struct f2fs_flush_device range;
+	struct f2fs_gc_control gc_control = {
+			.init_gc_type = FG_GC,
+			.should_migrate_blocks = true,
+			.err_gc_skipped = true,
+			.nr_free_secs = 0 };
 	int ret;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-	if (!inode_owner_or_capable(mnt_userns, inode))
-		return -EACCES;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
+
+	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
+		return -EINVAL;
+
+	if (copy_from_user(&range, (struct f2fs_flush_device __user *)arg,
+							sizeof(range)))
+		return -EFAULT;
+
+	if (!f2fs_is_multi_device(sbi) || sbi->s_ndevs - 1 <= range.dev_num ||
+			__is_large_section(sbi)) {
+		f2fs_warn(sbi, "Can't flush %u in %d for segs_per_sec %u != 1",
+			  range.dev_num, sbi->s_ndevs, sbi->segs_per_sec);
+		return -EINVAL;
+	}
 
 	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	inode_lock(inode);
+	if (range.dev_num != 0)
+		dev_start_segno = GET_SEGNO(sbi, FDEV(range.dev_num).start_blk);
+	dev_end_segno = GET_SEGNO(sbi, FDEV(range.dev_num).end_blk);
 
-	f2fs_abort_atomic_write(inode, true);
+	start_segno = sm->last_victim[FLUSH_DEVICE];
+	if (start_segno < dev_start_segno || start_segno >= dev_end_segno)
+		start_segno = dev_start_segno;
+	end_segno = min(start_segno + range.segments, dev_end_segno);
 
-	inode_unlock(inode);
+	while (start_segno < end_segno) {
+		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
+			ret = -EBUSY;
+			goto out;
+		}
+		sm->last_victim[GC_CB] = end_segno + 1;
+		sm->last_victim[GC_GREEDY] = end_segno + 1;
+		sm->last_victim[ALLOC_NEXT] = end_segno + 1;
 
+		gc_control.victim_segno = start_segno;
+		ret = f2fs_gc(sbi, &gc_control);
+		if (ret == -EAGAIN)
+			ret = 0;
+		else if (ret < 0)
+			break;
+		start_segno++;
+	}
+out:
 	mnt_drop_write_file(filp);
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 	return ret;
 }
 
-static int f2fs_ioc_shutdown(struct file *filp, unsigned long arg)
+static int f2fs_ioc_get_features(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct super_block *sb = sbi->sb;
-	__u32 in;
-	int ret = 0;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
-
-	if (get_user(in, (__u32 __user *)arg))
-		return -EFAULT;
-
-	if (in != F2FS_GOING_DOWN_FULLSYNC) {
-		ret = mnt_want_write_file(filp);
-		if (ret) {
-			if (ret == -EROFS) {
-				ret = 0;
-				f2fs_stop_checkpoint(sbi, false,
-						STOP_CP_REASON_SHUTDOWN);
-				set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-				trace_f2fs_shutdown(sbi, in, ret);
-			}
-			return ret;
-		}
-	}
-
-	switch (in) {
-	case F2FS_GOING_DOWN_FULLSYNC:
-		ret = freeze_bdev(sb->s_bdev);
-		if (ret)
-			goto out;
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		thaw_bdev(sb->s_bdev);
-		break;
-	case F2FS_GOING_DOWN_METASYNC:
-		/* do checkpoint only */
-		ret = f2fs_sync_fs(sb, 1);
-		if (ret) {
-			if (ret == -EIO)
-				ret = 0;
-			goto out;
-		}
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		break;
-	case F2FS_GOING_DOWN_NOSYNC:
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		break;
-	case F2FS_GOING_DOWN_METAFLUSH:
-		f2fs_sync_meta_pages(sbi, META, LONG_MAX, FS_META_IO);
-		f2fs_stop_checkpoint(sbi, false, STOP_CP_REASON_SHUTDOWN);
-		set_sbi_flag(sbi, SBI_IS_SHUTDOWN);
-		break;
-	case F2FS_GOING_DOWN_NEED_FSCK:
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		set_sbi_flag(sbi, SBI_CP_DISABLED_QUICK);
-		set_sbi_flag(sbi, SBI_IS_DIRTY);
-		/* do checkpoint only */
-		ret = f2fs_sync_fs(sb, 1);
-		if (ret == -EIO)
-			ret = 0;
-		goto out;
-	default:
-		ret = -EINVAL;
-		goto out;
-	}
+	u32 sb_feature = le32_to_cpu(F2FS_I_SB(inode)->raw_super->feature);
 
-	f2fs_stop_gc_thread(sbi);
-	f2fs_stop_discard_thread(sbi);
+	/* Must validate to set it with SQLite behavior in Android. */
+	sb_feature |= F2FS_FEATURE_ATOMIC_WRITE;
 
-	f2fs_drop_discard_cmd(sbi);
-	clear_opt(sbi, DISCARD);
+	return put_user(sb_feature, (u32 __user *)arg);
+}
 
-	f2fs_update_time(sbi, REQ_TIME);
-out:
-	if (in != F2FS_GOING_DOWN_FULLSYNC)
-		mnt_drop_write_file(filp);
+#ifdef CONFIG_QUOTA
+int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
+{
+	struct dquot *transfer_to[MAXQUOTAS] = {};
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct super_block *sb = sbi->sb;
+	int err;
 
-	trace_f2fs_shutdown(sbi, in, ret);
+	transfer_to[PRJQUOTA] = dqget(sb, make_kqid_projid(kprojid));
+	if (IS_ERR(transfer_to[PRJQUOTA]))
+		return PTR_ERR(transfer_to[PRJQUOTA]);
 
-	return ret;
+	err = __dquot_transfer(inode, transfer_to);
+	if (err)
+		set_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);
+	dqput(transfer_to[PRJQUOTA]);
+	return err;
 }
 
-static int f2fs_ioc_fitrim(struct file *filp, unsigned long arg)
+static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
 {
-	struct inode *inode = file_inode(filp);
-	struct super_block *sb = inode->i_sb;
-	struct fstrim_range range;
-	int ret;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode *ri = NULL;
+	kprojid_t kprojid;
+	int err;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (!f2fs_sb_has_project_quota(sbi)) {
+		if (projid != F2FS_DEF_PROJID)
+			return -EOPNOTSUPP;
+		else
+			return 0;
+	}
 
-	if (!f2fs_hw_support_discard(F2FS_SB(sb)))
+	if (!f2fs_has_extra_attr(inode))
 		return -EOPNOTSUPP;
 
-	if (copy_from_user(&range, (struct fstrim_range __user *)arg,
-				sizeof(range)))
-		return -EFAULT;
+	kprojid = make_kprojid(&init_user_ns, (projid_t)projid);
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	if (projid_eq(kprojid, fi->i_projid))
+		return 0;
 
-	range.minlen = max((unsigned int)range.minlen,
-			   bdev_discard_granularity(sb->s_bdev));
-	ret = f2fs_trim_fs(F2FS_SB(sb), &range);
-	mnt_drop_write_file(filp);
-	if (ret < 0)
-		return ret;
+	err = -EPERM;
+	/* Is it quota file? Do not allow user to mess with it */
+	if (IS_NOQUOTA(inode))
+		return err;
 
-	if (copy_to_user((struct fstrim_range __user *)arg, &range,
-				sizeof(range)))
-		return -EFAULT;
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))
+		return -EOVERFLOW;
+
+	err = f2fs_dquot_initialize(inode);
+	if (err)
+		return err;
+
+	f2fs_lock_op(sbi);
+	err = f2fs_transfer_project_quota(inode, kprojid);
+	if (err)
+		goto out_unlock;
+
+	fi->i_projid = kprojid;
+	inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
+out_unlock:
+	f2fs_unlock_op(sbi);
+	return err;
+}
+#else
+int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
+{
 	return 0;
 }
 
-static bool uuid_is_nonzero(__u8 u[16])
+static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
 {
-	int i;
+	if (projid != F2FS_DEF_PROJID)
+		return -EOPNOTSUPP;
+	return 0;
+}
+#endif
 
-	for (i = 0; i < 16; i++)
-		if (u[i])
-			return true;
-	return false;
+int f2fs_fileattr_get(struct dentry *dentry, struct fileattr *fa)
+{
+	struct inode *inode = d_inode(dentry);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	u32 fsflags = f2fs_iflags_to_fsflags(fi->i_flags);
+
+	if (IS_ENCRYPTED(inode))
+		fsflags |= FS_ENCRYPT_FL;
+	if (IS_VERITY(inode))
+		fsflags |= FS_VERITY_FL;
+	if (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode))
+		fsflags |= FS_INLINE_DATA_FL;
+	if (is_inode_flag_set(inode, FI_PIN_FILE))
+		fsflags |= FS_NOCOW_FL;
+	if (!may_compress)
+		fsflags &= ~FS_COMPR_FL;
+
+	fileattr_fill_flags(fa, fsflags & F2FS_GETTABLE_FS_FL);
+
+	if (f2fs_sb_has_project_quota(F2FS_I_SB(inode)))
+		fa->fsx_projid = from_kprojid(&init_user_ns, fi->i_projid);
+
+	return 0;
 }
 
-static int f2fs_ioc_set_encryption_policy(struct file *filp, unsigned long arg)
+int f2fs_fileattr_set(struct user_namespace *mnt_userns,
+		      struct dentry *dentry, struct fileattr *fa)
 {
-	struct inode *inode = file_inode(filp);
+	struct inode *inode = d_inode(dentry);
+	u32 fsflags = fa->flags, mask = F2FS_SETTABLE_FS_FL;
+	u32 iflags;
+	int err;
 
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(inode)))
+	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
+		return -EIO;
+	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
+		return -ENOSPC;
+	if (fsflags & ~F2FS_GETTABLE_FS_FL)
+		return -EOPNOTSUPP;
+	fsflags &= F2FS_SETTABLE_FS_FL;
+	if (!fa->flags_valid)
+		mask &= FS_COMMON_FL;
+
+	iflags = f2fs_fsflags_to_iflags(fsflags);
+	if (f2fs_mask_flags(inode->i_mode, iflags) != iflags)
 		return -EOPNOTSUPP;
 
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	err = f2fs_setflags_common(inode, iflags, f2fs_fsflags_to_iflags(mask));
+	if (!err)
+		err = f2fs_ioc_setproject(inode, fa->fsx_projid);
 
-	return fscrypt_ioctl_set_policy(filp, (const void __user *)arg);
+	return err;
 }
 
-static int f2fs_ioc_get_encryption_policy(struct file *filp, unsigned long arg)
+int f2fs_pin_file_control(struct inode *inode, bool inc)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
-	return fscrypt_ioctl_get_policy(filp, (void __user *)arg);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+
+	/* Use i_gc_failures for normal file as a risk signal. */
+	if (inc)
+		f2fs_i_gc_failures_write(inode,
+				fi->i_gc_failures[GC_FAILURE_PIN] + 1);
+
+	if (fi->i_gc_failures[GC_FAILURE_PIN] > sbi->gc_pin_file_threshold) {
+		f2fs_warn(sbi, "%s: Enable GC = ino %lx after %x GC trials",
+			  __func__, inode->i_ino,
+			  fi->i_gc_failures[GC_FAILURE_PIN]);
+		clear_inode_flag(inode, FI_PIN_FILE);
+		return -EAGAIN;
+	}
+	return 0;
 }
 
-static int f2fs_ioc_get_encryption_pwsalt(struct file *filp, unsigned long arg)
+static int f2fs_ioc_set_pin_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	u8 encrypt_pw_salt[16];
-	int err;
+	__u32 pin;
+	int ret = 0;
 
-	if (!f2fs_sb_has_encrypt(sbi))
-		return -EOPNOTSUPP;
+	if (get_user(pin, (__u32 __user *)arg))
+		return -EFAULT;
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		return err;
+	if (!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	f2fs_down_write(&sbi->sb_lock);
+	if (f2fs_readonly(F2FS_I_SB(inode)->sb))
+		return -EROFS;
 
-	if (uuid_is_nonzero(sbi->raw_super->encrypt_pw_salt))
-		goto got_it;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-	/* update superblock with uuid */
-	generate_random_uuid(sbi->raw_super->encrypt_pw_salt);
+	inode_lock(inode);
 
-	err = f2fs_commit_super(sbi, false);
-	if (err) {
-		/* undo new data */
-		memset(sbi->raw_super->encrypt_pw_salt, 0, 16);
-		goto out_err;
+	if (f2fs_is_atomic_file(inode)) {
+		ret = -EINVAL;
+		goto out;
 	}
-got_it:
-	memcpy(encrypt_pw_salt, sbi->raw_super->encrypt_pw_salt, 16);
-out_err:
-	f2fs_up_write(&sbi->sb_lock);
-	mnt_drop_write_file(filp);
 
-	if (!err && copy_to_user((__u8 __user *)arg, encrypt_pw_salt, 16))
-		err = -EFAULT;
+	if (!pin) {
+		clear_inode_flag(inode, FI_PIN_FILE);
+		f2fs_i_gc_failures_write(inode, 0);
+		goto done;
+	}
 
-	return err;
-}
+	if (f2fs_should_update_outplace(inode, NULL)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-static int f2fs_ioc_get_encryption_policy_ex(struct file *filp,
-					     unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret)
+			goto out;
+	}
+#endif
 
-	return fscrypt_ioctl_get_policy_ex(filp, (void __user *)arg);
-}
+	if (f2fs_pin_file_control(inode, false)) {
+		ret = -EAGAIN;
+		goto out;
+	}
 
-static int f2fs_ioc_add_encryption_key(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto out;
 
-	return fscrypt_ioctl_add_key(filp, (void __user *)arg);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (f2fs_compressed_file(inode)) {
+		if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+			ret = f2fs_reserve_compress_blocks(inode, NULL);
+			if (ret < 0)
+				goto out;
+		}
+		ret = f2fs_decompress_inode(inode);
+		if (ret < 0)
+			goto out;
+	}
+#endif
+
+	if (!f2fs_disable_compressed_file(inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
+
+	set_inode_flag(inode, FI_PIN_FILE);
+	ret = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
+done:
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+out:
+	inode_unlock(inode);
+	mnt_drop_write_file(filp);
+	return ret;
 }
 
-static int f2fs_ioc_remove_encryption_key(struct file *filp, unsigned long arg)
+static int f2fs_ioc_get_pin_file(struct file *filp, unsigned long arg)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	struct inode *inode = file_inode(filp);
+	__u32 pin = 0;
 
-	return fscrypt_ioctl_remove_key(filp, (void __user *)arg);
+	if (is_inode_flag_set(inode, FI_PIN_FILE))
+		pin = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
+	return put_user(pin, (u32 __user *)arg);
 }
 
-static int f2fs_ioc_remove_encryption_key_all_users(struct file *filp,
-						    unsigned long arg)
+int f2fs_precache_extents(struct inode *inode)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_map_blocks map;
+	pgoff_t m_next_extent;
+	loff_t end;
+	int err;
+
+	if (is_inode_flag_set(inode, FI_NO_EXTENT))
 		return -EOPNOTSUPP;
 
-	return fscrypt_ioctl_remove_key_all_users(filp, (void __user *)arg);
-}
+	map.m_lblk = 0;
+	map.m_pblk = 0;
+	map.m_next_pgofs = NULL;
+	map.m_next_extent = &m_next_extent;
+	map.m_seg_type = NO_CHECK_TYPE;
+	map.m_may_create = false;
+	end = max_file_blocks(inode);
 
-static int f2fs_ioc_get_encryption_key_status(struct file *filp,
-					      unsigned long arg)
-{
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	while (map.m_lblk < end) {
+		map.m_len = end - map.m_lblk;
 
-	return fscrypt_ioctl_get_key_status(filp, (void __user *)arg);
+		f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
+		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRECACHE);
+		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
+		if (err)
+			return err;
+
+		map.m_lblk = m_next_extent;
+	}
+
+	return 0;
 }
 
-static int f2fs_ioc_get_encryption_nonce(struct file *filp, unsigned long arg)
+static int f2fs_ioc_precache_extents(struct file *filp)
 {
-	if (!f2fs_sb_has_encrypt(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
-
-	return fscrypt_ioctl_get_nonce(filp, (void __user *)arg);
+	return f2fs_precache_extents(file_inode(filp));
 }
 
-static int f2fs_ioc_gc(struct file *filp, unsigned long arg)
+static int f2fs_ioc_resize_fs(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_gc_control gc_control = { .victim_segno = NULL_SEGNO,
-			.no_bg_gc = false,
-			.should_migrate_blocks = false,
-			.nr_free_secs = 0 };
-	__u32 sync;
-	int ret;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
+	__u64 block_count;
 
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
-	if (get_user(sync, (__u32 __user *)arg))
-		return -EFAULT;
-
 	if (f2fs_readonly(sbi->sb))
 		return -EROFS;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
-
-	if (!sync) {
-		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
-			ret = -EBUSY;
-			goto out;
-		}
-	} else {
-		f2fs_down_write(&sbi->gc_lock);
-	}
+	if (copy_from_user(&block_count, (void __user *)arg,
+			   sizeof(block_count)))
+		return -EFAULT;
 
-	gc_control.init_gc_type = sync ? FG_GC : BG_GC;
-	gc_control.err_gc_skipped = sync;
-	ret = f2fs_gc(sbi, &gc_control);
-out:
-	mnt_drop_write_file(filp);
-	return ret;
+	return f2fs_resize_fs(filp, block_count);
 }
 
-static int __f2fs_ioc_gc_range(struct file *filp, struct f2fs_gc_range *range)
+static int f2fs_ioc_enable_verity(struct file *filp, unsigned long arg)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
-	struct f2fs_gc_control gc_control = {
-			.init_gc_type = range->sync ? FG_GC : BG_GC,
-			.no_bg_gc = false,
-			.should_migrate_blocks = false,
-			.err_gc_skipped = range->sync,
-			.nr_free_secs = 0 };
-	u64 end;
-	int ret;
+	struct inode *inode = file_inode(filp);
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
 
-	end = range->start + range->len;
-	if (end < range->start || range->start < MAIN_BLKADDR(sbi) ||
-					end >= MAX_BLKADDR(sbi))
-		return -EINVAL;
+	if (!f2fs_sb_has_verity(F2FS_I_SB(inode))) {
+		f2fs_warn(F2FS_I_SB(inode),
+			  "Can't enable fs-verity on inode %lu: the verity feature is not enabled on this filesystem",
+			  inode->i_ino);
+		return -EOPNOTSUPP;
+	}
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	return fsverity_ioctl_enable(filp, (const void __user *)arg);
+}
 
-do_more:
-	if (!range->sync) {
-		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
-			ret = -EBUSY;
-			goto out;
-		}
-	} else {
-		f2fs_down_write(&sbi->gc_lock);
-	}
+static int f2fs_ioc_measure_verity(struct file *filp, unsigned long arg)
+{
+	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
 
-	gc_control.victim_segno = GET_SEGNO(sbi, range->start);
-	ret = f2fs_gc(sbi, &gc_control);
-	if (ret) {
-		if (ret == -EBUSY)
-			ret = -EAGAIN;
-		goto out;
-	}
-	range->start += CAP_BLKS_PER_SEC(sbi);
-	if (range->start <= end)
-		goto do_more;
-out:
-	mnt_drop_write_file(filp);
-	return ret;
+	return fsverity_ioctl_measure(filp, (void __user *)arg);
 }
 
-static int f2fs_ioc_gc_range(struct file *filp, unsigned long arg)
+static int f2fs_ioc_read_verity_metadata(struct file *filp, unsigned long arg)
 {
-	struct f2fs_gc_range range;
+	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
+		return -EOPNOTSUPP;
 
-	if (copy_from_user(&range, (struct f2fs_gc_range __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
-	return __f2fs_ioc_gc_range(filp, &range);
+	return fsverity_ioctl_read_metadata(filp, (const void __user *)arg);
 }
 
-static int f2fs_ioc_write_checkpoint(struct file *filp)
+static int f2fs_ioc_getfslabel(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	int ret;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
-
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+	char *vbuf;
+	int count;
+	int err = 0;
 
-	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
-		f2fs_info(sbi, "Skipping Checkpoint. Checkpoints currently disabled.");
-		return -EINVAL;
-	}
+	vbuf = f2fs_kzalloc(sbi, MAX_VOLUME_NAME, GFP_KERNEL);
+	if (!vbuf)
+		return -ENOMEM;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	f2fs_down_read(&sbi->sb_lock);
+	count = utf16s_to_utf8s(sbi->raw_super->volume_name,
+			ARRAY_SIZE(sbi->raw_super->volume_name),
+			UTF16_LITTLE_ENDIAN, vbuf, MAX_VOLUME_NAME);
+	f2fs_up_read(&sbi->sb_lock);
 
-	ret = f2fs_sync_fs(sbi->sb, 1);
+	if (copy_to_user((char __user *)arg, vbuf,
+				min(FSLABEL_MAX, count)))
+		err = -EFAULT;
 
-	mnt_drop_write_file(filp);
-	return ret;
+	kfree(vbuf);
+	return err;
 }
 
-static int f2fs_defragment_range(struct f2fs_sb_info *sbi,
-					struct file *filp,
-					struct f2fs_defragment *range)
+static int f2fs_ioc_setfslabel(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_map_blocks map = { .m_next_extent = NULL,
-					.m_seg_type = NO_CHECK_TYPE,
-					.m_may_create = false };
-	struct extent_info ei = {};
-	pgoff_t pg_start, pg_end, next_pgofs;
-	unsigned int blk_per_seg = sbi->blocks_per_seg;
-	unsigned int total = 0, sec_num;
-	block_t blk_end = 0;
-	bool fragmented = false;
-	int err;
-
-	pg_start = range->start >> PAGE_SHIFT;
-	pg_end = (range->start + range->len) >> PAGE_SHIFT;
-
-	f2fs_balance_fs(sbi, true);
-
-	inode_lock(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	char *vbuf;
+	int err = 0;
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED) ||
-		f2fs_is_atomic_file(inode)) {
-		err = -EINVAL;
-		goto unlock_out;
-	}
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
 
-	/* if in-place-update policy is enabled, don't waste time here */
-	set_inode_flag(inode, FI_OPU_WRITE);
-	if (f2fs_should_update_inplace(inode, NULL)) {
-		err = -EINVAL;
-		goto out;
-	}
+	vbuf = strndup_user((const char __user *)arg, FSLABEL_MAX);
+	if (IS_ERR(vbuf))
+		return PTR_ERR(vbuf);
 
-	/* writeback all dirty pages in the range */
-	err = filemap_write_and_wait_range(inode->i_mapping, range->start,
-						range->start + range->len - 1);
+	err = mnt_want_write_file(filp);
 	if (err)
 		goto out;
 
-	/*
-	 * lookup mapping info in extent cache, skip defragmenting if physical
-	 * block addresses are continuous.
-	 */
-	if (f2fs_lookup_read_extent_cache(inode, pg_start, &ei)) {
-		if ((pgoff_t)ei.fofs + ei.len >= pg_end)
-			goto out;
-	}
+	f2fs_down_write(&sbi->sb_lock);
 
-	map.m_lblk = pg_start;
-	map.m_next_pgofs = &next_pgofs;
+	memset(sbi->raw_super->volume_name, 0,
+			sizeof(sbi->raw_super->volume_name));
+	utf8s_to_utf16s(vbuf, strlen(vbuf), UTF16_LITTLE_ENDIAN,
+			sbi->raw_super->volume_name,
+			ARRAY_SIZE(sbi->raw_super->volume_name));
 
-	/*
-	 * lookup mapping info in dnode page cache, skip defragmenting if all
-	 * physical block addresses are continuous even if there are hole(s)
-	 * in logical blocks.
-	 */
-	while (map.m_lblk < pg_end) {
-		map.m_len = pg_end - map.m_lblk;
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
-		if (err)
-			goto out;
+	err = f2fs_commit_super(sbi, false);
 
-		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
-			map.m_lblk = next_pgofs;
-			continue;
-		}
+	f2fs_up_write(&sbi->sb_lock);
 
-		if (blk_end && blk_end != map.m_pblk)
-			fragmented = true;
+	mnt_drop_write_file(filp);
+out:
+	kfree(vbuf);
+	return err;
+}
 
-		/* record total count of block that we're going to move */
-		total += map.m_len;
+static int f2fs_get_compress_blocks(struct inode *inode, __u64 *blocks)
+{
+	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
+		return -EOPNOTSUPP;
 
-		blk_end = map.m_pblk + map.m_len;
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
-		map.m_lblk += map.m_len;
-	}
+	*blocks = atomic_read(&F2FS_I(inode)->i_compr_blocks);
 
-	if (!fragmented) {
-		total = 0;
-		goto out;
-	}
+	return 0;
+}
 
-	sec_num = DIV_ROUND_UP(total, CAP_BLKS_PER_SEC(sbi));
+static int f2fs_ioc_get_compress_blocks(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	__u64 blocks;
+	int ret;
 
-	/*
-	 * make sure there are enough free section for LFS allocation, this can
-	 * avoid defragment running in SSR mode when free section are allocated
-	 * intensively
-	 */
-	if (has_not_enough_free_secs(sbi, 0, sec_num)) {
-		err = -EAGAIN;
-		goto out;
-	}
+	ret = f2fs_get_compress_blocks(inode, &blocks);
+	if (ret < 0)
+		return ret;
 
-	map.m_lblk = pg_start;
-	map.m_len = pg_end - pg_start;
-	total = 0;
+	return put_user(blocks, (u64 __user *)arg);
+}
 
-	while (map.m_lblk < pg_end) {
-		pgoff_t idx;
-		int cnt = 0;
+static int release_compress_blocks(struct dnode_of_data *dn, pgoff_t count)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	unsigned int released_blocks = 0;
+	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
+	block_t blkaddr;
+	int i;
 
-do_map:
-		map.m_len = pg_end - map.m_lblk;
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_DEFAULT);
-		if (err)
-			goto clear_out;
+	for (i = 0; i < count; i++) {
+		blkaddr = data_blkaddr(dn->inode, dn->node_page,
+						dn->ofs_in_node + i);
 
-		if (!(map.m_flags & F2FS_MAP_FLAGS)) {
-			map.m_lblk = next_pgofs;
-			goto check;
+		if (!__is_valid_data_blkaddr(blkaddr))
+			continue;
+		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
+					DATA_GENERIC_ENHANCE))) {
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			return -EFSCORRUPTED;
 		}
+	}
 
-		set_inode_flag(inode, FI_SKIP_WRITES);
+	while (count) {
+		int compr_blocks = 0;
 
-		idx = map.m_lblk;
-		while (idx < map.m_lblk + map.m_len && cnt < blk_per_seg) {
-			struct page *page;
+		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
+			blkaddr = f2fs_data_blkaddr(dn);
 
-			page = f2fs_get_lock_data_page(inode, idx, true);
-			if (IS_ERR(page)) {
-				err = PTR_ERR(page);
-				goto clear_out;
+			if (i == 0) {
+				if (blkaddr == COMPRESS_ADDR)
+					continue;
+				dn->ofs_in_node += cluster_size;
+				goto next;
 			}
 
-			f2fs_wait_on_page_writeback(page, DATA, true, true);
+			if (__is_valid_data_blkaddr(blkaddr))
+				compr_blocks++;
 
-			set_page_dirty(page);
-			set_page_private_gcing(page);
-			f2fs_put_page(page, 1);
+			if (blkaddr != NEW_ADDR)
+				continue;
 
-			idx++;
-			cnt++;
-			total++;
+			f2fs_set_data_blkaddr(dn, NULL_ADDR);
 		}
 
-		map.m_lblk = idx;
-check:
-		if (map.m_lblk < pg_end && cnt < blk_per_seg)
-			goto do_map;
-
-		clear_inode_flag(inode, FI_SKIP_WRITES);
+		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, false);
+		dec_valid_block_count(sbi, dn->inode,
+					cluster_size - compr_blocks);
 
-		err = filemap_fdatawrite(inode->i_mapping);
-		if (err)
-			goto out;
+		released_blocks += cluster_size - compr_blocks;
+next:
+		count -= cluster_size;
 	}
-clear_out:
-	clear_inode_flag(inode, FI_SKIP_WRITES);
-out:
-	clear_inode_flag(inode, FI_OPU_WRITE);
-unlock_out:
-	inode_unlock(inode);
-	if (!err)
-		range->len = (u64)total << PAGE_SHIFT;
-	return err;
+
+	return released_blocks;
 }
 
-static int f2fs_ioc_defragment(struct file *filp, unsigned long arg)
+static int f2fs_release_compress_blocks(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_defragment range;
-	int err;
+	pgoff_t page_idx = 0, last_idx;
+	unsigned int released_blocks = 0;
+	int ret;
+	int writecount;
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (!f2fs_sb_has_compression(sbi))
+		return -EOPNOTSUPP;
 
-	if (!S_ISREG(inode->i_mode) || f2fs_is_atomic_file(inode))
+	if (!f2fs_compressed_file(inode))
 		return -EINVAL;
 
 	if (f2fs_readonly(sbi->sb))
 		return -EROFS;
 
-	if (copy_from_user(&range, (struct f2fs_defragment __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-	/* verify alignment of offset & size */
-	if (range.start & (F2FS_BLKSIZE - 1) || range.len & (F2FS_BLKSIZE - 1))
-		return -EINVAL;
+	f2fs_balance_fs(sbi, true);
 
-	if (unlikely((range.start + range.len) >> PAGE_SHIFT >
-					max_file_blocks(inode)))
-		return -EINVAL;
+	inode_lock(inode);
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		return err;
+	writecount = atomic_read(&inode->i_writecount);
+	if ((filp->f_mode & FMODE_WRITE && writecount != 1) ||
+			(!(filp->f_mode & FMODE_WRITE) && writecount)) {
+		ret = -EBUSY;
+		goto out;
+	}
 
-	err = f2fs_defragment_range(sbi, filp, &range);
-	mnt_drop_write_file(filp);
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-	f2fs_update_time(sbi, REQ_TIME);
-	if (err < 0)
-		return err;
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
+		goto out;
 
-	if (copy_to_user((struct f2fs_defragment __user *)arg, &range,
-							sizeof(range)))
-		return -EFAULT;
+	if (!atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		ret = -EPERM;
+		goto out;
+	}
 
-	return 0;
-}
+	f2fs_info(sbi, "start release cblocks ino %lu (%pd) size %llu blocks %llu "
+		"cblocks %d\n", inode->i_ino, file_dentry(filp),
+		i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks));
 
-static int f2fs_move_file_range(struct file *file_in, loff_t pos_in,
-			struct file *file_out, loff_t pos_out, size_t len)
-{
-	struct inode *src = file_inode(file_in);
-	struct inode *dst = file_inode(file_out);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(src);
-	size_t olen = len, dst_max_i_size = 0;
-	size_t dst_osize;
-	int ret;
+	set_inode_flag(inode, FI_COMPRESS_RELEASED);
+	inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, true);
 
-	if (file_in->f_path.mnt != file_out->f_path.mnt ||
-				src->i_sb != dst->i_sb)
-		return -EXDEV;
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
 
-	if (unlikely(f2fs_readonly(src->i_sb)))
-		return -EROFS;
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 
-	if (!S_ISREG(src->i_mode) || !S_ISREG(dst->i_mode))
-		return -EINVAL;
+	while (page_idx < last_idx) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
+		if (ret) {
+			if (ret == -ENOENT) {
+				page_idx = f2fs_get_next_page_offset(&dn,
+								page_idx);
+				ret = 0;
+				continue;
+			}
+			break;
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
+		count = round_up(count, F2FS_I(inode)->i_cluster_size);
+
+		ret = release_compress_blocks(&dn, count);
+
+		f2fs_put_dnode(&dn);
+
+		if (ret < 0)
+			break;
+
+		page_idx += count;
+		released_blocks += ret;
+	}
+
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	f2fs_info(sbi, "end release cblocks ino %lu (%pd) size %llu blocks %llu "
+		"cblocks %d rblocks %u ret %d\n", inode->i_ino, file_dentry(filp),
+		i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks), released_blocks, ret);
 
-	if (IS_ENCRYPTED(src) || IS_ENCRYPTED(dst))
-		return -EOPNOTSUPP;
+out:
+	inode_unlock(inode);
 
-	if (pos_out < 0 || pos_in < 0)
-		return -EINVAL;
+	mnt_drop_write_file(filp);
 
-	if (src == dst) {
-		if (pos_in == pos_out)
-			return 0;
-		if (pos_out > pos_in && pos_out < pos_in + len)
-			return -EINVAL;
+	if (ret >= 0) {
+		ret = put_user(released_blocks, (u64 __user *)arg);
+	} else if (released_blocks &&
+			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
+			"iblocks=%llu, released=%u, compr_blocks=%u, "
+			"run fsck to fix.",
+			__func__, inode->i_ino, inode->i_blocks,
+			released_blocks,
+			atomic_read(&F2FS_I(inode)->i_compr_blocks));
 	}
 
-	inode_lock(src);
-	if (src != dst) {
-		ret = -EBUSY;
-		if (!inode_trylock(dst))
-			goto out;
-	}
+	return ret;
+}
 
-	if (f2fs_compressed_file(src) || f2fs_compressed_file(dst)) {
-		ret = -EOPNOTSUPP;
-		goto out_unlock;
-	}
+static int reserve_compress_blocks(struct dnode_of_data *dn, pgoff_t count,
+		unsigned int *reserved_blocks)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
+	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
+	block_t blkaddr;
+	int i;
 
-	if (f2fs_is_atomic_file(src) || f2fs_is_atomic_file(dst)) {
-		ret = -EINVAL;
-		goto out_unlock;
-	}
+	for (i = 0; i < count; i++) {
+		blkaddr = data_blkaddr(dn->inode, dn->node_page,
+						dn->ofs_in_node + i);
 
-	ret = -EINVAL;
-	if (pos_in + len > src->i_size || pos_in + len < pos_in)
-		goto out_unlock;
-	if (len == 0)
-		olen = len = src->i_size - pos_in;
-	if (pos_in + len == src->i_size)
-		len = ALIGN(src->i_size, F2FS_BLKSIZE) - pos_in;
-	if (len == 0) {
-		ret = 0;
-		goto out_unlock;
+		if (!__is_valid_data_blkaddr(blkaddr))
+			continue;
+		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
+					DATA_GENERIC_ENHANCE))) {
+			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
+			return -EFSCORRUPTED;
+		}
 	}
 
-	dst_osize = dst->i_size;
-	if (pos_out + olen > dst->i_size)
-		dst_max_i_size = pos_out + olen;
+	while (count) {
+		int compr_blocks = 0;
+		blkcnt_t reserved = 0;
+		blkcnt_t to_reserved;
+		int ret;
 
-	/* verify the end result is block aligned */
-	if (!IS_ALIGNED(pos_in, F2FS_BLKSIZE) ||
-			!IS_ALIGNED(pos_in + len, F2FS_BLKSIZE) ||
-			!IS_ALIGNED(pos_out, F2FS_BLKSIZE))
-		goto out_unlock;
+		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
+			blkaddr = f2fs_data_blkaddr(dn);
 
-	ret = f2fs_convert_inline_inode(src);
-	if (ret)
-		goto out_unlock;
+			if (i == 0) {
+				if (blkaddr == COMPRESS_ADDR)
+					continue;
+				dn->ofs_in_node += cluster_size;
+				goto next;
+			}
 
-	ret = f2fs_convert_inline_inode(dst);
-	if (ret)
-		goto out_unlock;
+			/*
+			 * compressed cluster was not released due to it
+			 * fails in release_compress_blocks(), so NEW_ADDR
+			 * is a possible case.
+			 */
+			if (blkaddr == NEW_ADDR) {
+				reserved++;
+				continue;
+			}
+			if (__is_valid_data_blkaddr(blkaddr)) {
+				compr_blocks++;
+				continue;
+			}
 
-	/* write out all dirty pages from offset */
-	ret = filemap_write_and_wait_range(src->i_mapping,
-					pos_in, pos_in + len);
-	if (ret)
-		goto out_unlock;
+			f2fs_set_data_blkaddr(dn,NEW_ADDR);
+		}
 
-	ret = filemap_write_and_wait_range(dst->i_mapping,
-					pos_out, pos_out + len);
-	if (ret)
-		goto out_unlock;
+		to_reserved = cluster_size - compr_blocks - reserved;
+		if (time_to_inject(sbi, FAULT_COMPRESS_RESERVE_NOSPC))
+			return -ENOSPC;
 
-	f2fs_balance_fs(sbi, true);
+		/* for the case all blocks in cluster were reserved */
+		if (to_reserved == 1) {
+			dn->ofs_in_node += cluster_size;
+			goto next;
+		}
 
-	f2fs_down_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
-	if (src != dst) {
-		ret = -EBUSY;
-		if (!f2fs_down_write_trylock(&F2FS_I(dst)->i_gc_rwsem[WRITE]))
-			goto out_src;
-	}
+		ret = inc_valid_block_count(sbi, dn->inode,
+						&to_reserved, false);
+		if (unlikely(ret))
+			return ret;
 
-	f2fs_lock_op(sbi);
-	ret = __exchange_data_block(src, dst, pos_in >> F2FS_BLKSIZE_BITS,
-				pos_out >> F2FS_BLKSIZE_BITS,
-				len >> F2FS_BLKSIZE_BITS, false);
+		if (reserved != cluster_size - compr_blocks)
+			return -ENOSPC;
+		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
+			if (f2fs_data_blkaddr(dn) == NULL_ADDR)
+				f2fs_set_data_blkaddr(dn, NEW_ADDR);
+		}
 
-	if (!ret) {
-		if (dst_max_i_size)
-			f2fs_i_size_write(dst, dst_max_i_size);
-		else if (dst_osize != dst->i_size)
-			f2fs_i_size_write(dst, dst_osize);
+		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, true);
+
+		*reserved_blocks += to_reserved;
+next:
+		count -= cluster_size;
 	}
-	f2fs_unlock_op(sbi);
 
-	if (src != dst)
-		f2fs_up_write(&F2FS_I(dst)->i_gc_rwsem[WRITE]);
-out_src:
-	f2fs_up_write(&F2FS_I(src)->i_gc_rwsem[WRITE]);
-out_unlock:
-	if (src != dst)
-		inode_unlock(dst);
-out:
-	inode_unlock(src);
-	return ret;
+	return 0;
 }
 
-static int __f2fs_ioc_move_range(struct file *filp,
-				struct f2fs_move_range *range)
+int f2fs_reserve_compress_blocks(struct inode *inode, unsigned int *ret_rsvd_blks)
 {
-	struct fd dst;
-	int err;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	pgoff_t page_idx = 0, last_idx;
+	unsigned int reserved_blocks = 0;
+	int ret;
 
-	if (!(filp->f_mode & FMODE_READ) ||
-			!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	f2fs_bug_on(sbi, !inode_is_locked(inode));
 
-	dst = fdget(range->dst_fd);
-	if (!dst.file)
-		return -EBADF;
+	if (!is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+		return -EINVAL;
 
-	if (!(dst.file->f_mode & FMODE_WRITE)) {
-		err = -EBADF;
-		goto err_out;
-	}
+	f2fs_info(sbi, "start reserve cblocks ino %lu size %llu blocks %llu "
+		"cblocks %d caller %ps\n", inode->i_ino, i_size_read(inode),
+		inode->i_blocks, atomic_read(&F2FS_I(inode)->i_compr_blocks),
+		__builtin_return_address(0));
 
-	err = mnt_want_write_file(filp);
-	if (err)
-		goto err_out;
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(inode->i_mapping);
 
-	err = f2fs_move_file_range(filp, range->pos_in, dst.file,
-					range->pos_out, range->len);
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
 
-	mnt_drop_write_file(filp);
-err_out:
-	fdput(dst);
-	return err;
-}
+	while (page_idx < last_idx) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
 
-static int f2fs_ioc_move_range(struct file *filp, unsigned long arg)
-{
-	struct f2fs_move_range range;
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
+		if (ret) {
+			if (ret == -ENOENT) {
+				page_idx = f2fs_get_next_page_offset(&dn,
+								page_idx);
+				ret = 0;
+				continue;
+			}
+			break;
+		}
 
-	if (copy_from_user(&range, (struct f2fs_move_range __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
-	return __f2fs_ioc_move_range(filp, &range);
-}
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
+		count = round_up(count, F2FS_I(inode)->i_cluster_size);
 
-static int f2fs_ioc_flush_device(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct sit_info *sm = SIT_I(sbi);
-	unsigned int start_segno = 0, end_segno = 0;
-	unsigned int dev_start_segno = 0, dev_end_segno = 0;
-	struct f2fs_flush_device range;
-	struct f2fs_gc_control gc_control = {
-			.init_gc_type = FG_GC,
-			.should_migrate_blocks = true,
-			.err_gc_skipped = true,
-			.nr_free_secs = 0 };
-	int ret;
+		ret = reserve_compress_blocks(&dn, count, &reserved_blocks);
+
+		f2fs_put_dnode(&dn);
+
+		if (ret < 0)
+			break;
+
+		page_idx += count;
+	}
+
+	filemap_invalidate_unlock(inode->i_mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+
+	if (!ret) {
+		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
+		inode->i_ctime = current_time(inode);
+		f2fs_mark_inode_dirty_sync(inode, true);
+		if (ret_rsvd_blks)
+			*ret_rsvd_blks = reserved_blocks;
+	} else if (reserved_blocks &&
+			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
+			"iblocks=%llu, reserved=%u, compr_blocks=%u, "
+			"run fsck to fix.",
+			__func__, inode->i_ino, inode->i_blocks,
+			reserved_blocks,
+			atomic_read(&F2FS_I(inode)->i_compr_blocks));
+	}
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	f2fs_info(sbi, "end reserve cblocks ino %lu size %llu blocks %llu "
+		"cblocks %d rsvd %d caller %ps ret %d\n", inode->i_ino,
+		i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks), reserved_blocks,
+		__builtin_return_address(0), ret);
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+	return ret;
+}
 
-	if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED)))
-		return -EINVAL;
+static int f2fs_ioc_reserve_compress_blocks(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	unsigned int reserved_blocks = 0;
+	int ret;
 
-	if (copy_from_user(&range, (struct f2fs_flush_device __user *)arg,
-							sizeof(range)))
-		return -EFAULT;
+	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
+		return -EOPNOTSUPP;
 
-	if (!f2fs_is_multi_device(sbi) || sbi->s_ndevs - 1 <= range.dev_num ||
-			__is_large_section(sbi)) {
-		f2fs_warn(sbi, "Can't flush %u in %d for segs_per_sec %u != 1",
-			  range.dev_num, sbi->s_ndevs, sbi->segs_per_sec);
+	if (!f2fs_compressed_file(inode))
 		return -EINVAL;
-	}
+
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
 	ret = mnt_want_write_file(filp);
 	if (ret)
 		return ret;
 
-	if (range.dev_num != 0)
-		dev_start_segno = GET_SEGNO(sbi, FDEV(range.dev_num).start_blk);
-	dev_end_segno = GET_SEGNO(sbi, FDEV(range.dev_num).end_blk);
+	if (atomic_read(&F2FS_I(inode)->i_compr_blocks))
+		goto out;
 
-	start_segno = sm->last_victim[FLUSH_DEVICE];
-	if (start_segno < dev_start_segno || start_segno >= dev_end_segno)
-		start_segno = dev_start_segno;
-	end_segno = min(start_segno + range.segments, dev_end_segno);
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
 
-	while (start_segno < end_segno) {
-		if (!f2fs_down_write_trylock(&sbi->gc_lock)) {
-			ret = -EBUSY;
-			goto out;
-		}
-		sm->last_victim[GC_CB] = end_segno + 1;
-		sm->last_victim[GC_GREEDY] = end_segno + 1;
-		sm->last_victim[ALLOC_NEXT] = end_segno + 1;
+	inode_lock(inode);
+	ret = f2fs_reserve_compress_blocks(inode, &reserved_blocks);
+	inode_unlock(inode);
 
-		gc_control.victim_segno = start_segno;
-		ret = f2fs_gc(sbi, &gc_control);
-		if (ret == -EAGAIN)
-			ret = 0;
-		else if (ret < 0)
-			break;
-		start_segno++;
-	}
 out:
 	mnt_drop_write_file(filp);
+
+	if (ret >= 0)
+		ret = put_user(reserved_blocks, (u64 __user *)arg);
+
 	return ret;
 }
 
-static int f2fs_ioc_get_features(struct file *filp, unsigned long arg)
+static int f2fs_secure_erase(struct block_device *bdev, struct inode *inode,
+		pgoff_t off, block_t block, block_t len, u32 flags)
 {
-	struct inode *inode = file_inode(filp);
-	u32 sb_feature = le32_to_cpu(F2FS_I_SB(inode)->raw_super->feature);
+	sector_t sector = SECTOR_FROM_BLOCK(block);
+	sector_t nr_sects = SECTOR_FROM_BLOCK(len);
+	int ret = 0;
 
-	/* Must validate to set it with SQLite behavior in Android. */
-	sb_feature |= F2FS_FEATURE_ATOMIC_WRITE;
+	if (flags & F2FS_TRIM_FILE_DISCARD) {
+		if (bdev_max_secure_erase_sectors(bdev))
+			ret = blkdev_issue_secure_erase(bdev, sector, nr_sects,
+					GFP_NOFS);
+		else
+			ret = blkdev_issue_discard(bdev, sector, nr_sects,
+					GFP_NOFS);
+	}
 
-	return put_user(sb_feature, (u32 __user *)arg);
+	if (!ret && (flags & F2FS_TRIM_FILE_ZEROOUT)) {
+		if (IS_ENCRYPTED(inode))
+			ret = fscrypt_zeroout_range(inode, off, block, len);
+		else
+			ret = blkdev_issue_zeroout(bdev, sector, nr_sects,
+					GFP_NOFS, 0);
+	}
+
+	return ret;
 }
 
-#ifdef CONFIG_QUOTA
-int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
+static int f2fs_sec_trim_file(struct file *filp, unsigned long arg)
 {
-	struct dquot *transfer_to[MAXQUOTAS] = {};
+	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct super_block *sb = sbi->sb;
-	int err;
+	struct address_space *mapping = inode->i_mapping;
+	struct block_device *prev_bdev = NULL;
+	struct f2fs_sectrim_range range;
+	pgoff_t index, pg_end, prev_index = 0;
+	block_t prev_block = 0, len = 0;
+	loff_t end_addr;
+	bool to_end = false;
+	int ret = 0;
 
-	transfer_to[PRJQUOTA] = dqget(sb, make_kqid_projid(kprojid));
-	if (IS_ERR(transfer_to[PRJQUOTA]))
-		return PTR_ERR(transfer_to[PRJQUOTA]);
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	err = __dquot_transfer(inode, transfer_to);
-	if (err)
-		set_sbi_flag(sbi, SBI_QUOTA_NEED_REPAIR);
-	dqput(transfer_to[PRJQUOTA]);
-	return err;
-}
+	if (copy_from_user(&range, (struct f2fs_sectrim_range __user *)arg,
+				sizeof(range)))
+		return -EFAULT;
 
-static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
-{
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_inode *ri = NULL;
-	kprojid_t kprojid;
-	int err;
+	if (range.flags == 0 || (range.flags & ~F2FS_TRIM_FILE_MASK) ||
+			!S_ISREG(inode->i_mode))
+		return -EINVAL;
 
-	if (!f2fs_sb_has_project_quota(sbi)) {
-		if (projid != F2FS_DEF_PROJID)
-			return -EOPNOTSUPP;
-		else
-			return 0;
+	if (((range.flags & F2FS_TRIM_FILE_DISCARD) &&
+			!f2fs_hw_support_discard(sbi)) ||
+			((range.flags & F2FS_TRIM_FILE_ZEROOUT) &&
+			 IS_ENCRYPTED(inode) && f2fs_is_multi_device(sbi)))
+		return -EOPNOTSUPP;
+
+	file_start_write(filp);
+	inode_lock(inode);
+
+	if (f2fs_is_atomic_file(inode) || f2fs_compressed_file(inode) ||
+			range.start >= inode->i_size) {
+		ret = -EINVAL;
+		goto err;
 	}
 
-	if (!f2fs_has_extra_attr(inode))
-		return -EOPNOTSUPP;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret)
+			goto err;
+	}
+#endif
 
-	kprojid = make_kprojid(&init_user_ns, (projid_t)projid);
+	if (range.len == 0)
+		goto err;
 
-	if (projid_eq(kprojid, fi->i_projid))
-		return 0;
+	if (inode->i_size - range.start > range.len) {
+		end_addr = range.start + range.len;
+	} else {
+		end_addr = range.len == (u64)-1 ?
+			sbi->sb->s_maxbytes : inode->i_size;
+		to_end = true;
+	}
 
-	err = -EPERM;
-	/* Is it quota file? Do not allow user to mess with it */
-	if (IS_NOQUOTA(inode))
-		return err;
+	if (!IS_ALIGNED(range.start, F2FS_BLKSIZE) ||
+			(!to_end && !IS_ALIGNED(end_addr, F2FS_BLKSIZE))) {
+		ret = -EINVAL;
+		goto err;
+	}
+
+	index = F2FS_BYTES_TO_BLK(range.start);
+	pg_end = DIV_ROUND_UP(end_addr, F2FS_BLKSIZE);
+
+	ret = f2fs_convert_inline_inode(inode);
+	if (ret)
+		goto err;
+
+	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+	filemap_invalidate_lock(mapping);
+
+	ret = filemap_write_and_wait_range(mapping, range.start,
+			to_end ? LLONG_MAX : end_addr - 1);
+	if (ret)
+		goto out;
+
+	truncate_inode_pages_range(mapping, range.start,
+			to_end ? -1 : end_addr - 1);
+
+	while (index < pg_end) {
+		struct dnode_of_data dn;
+		pgoff_t end_offset, count;
+		int i;
+
+		set_new_dnode(&dn, inode, NULL, NULL, 0);
+		ret = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+		if (ret) {
+			if (ret == -ENOENT) {
+				index = f2fs_get_next_page_offset(&dn, index);
+				continue;
+			}
+			goto out;
+		}
+
+		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
+		count = min(end_offset - dn.ofs_in_node, pg_end - index);
+		for (i = 0; i < count; i++, index++, dn.ofs_in_node++) {
+			struct block_device *cur_bdev;
+			block_t blkaddr = f2fs_data_blkaddr(&dn);
+
+			if (!__is_valid_data_blkaddr(blkaddr))
+				continue;
+
+			if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
+						DATA_GENERIC_ENHANCE)) {
+				ret = -EFSCORRUPTED;
+				f2fs_put_dnode(&dn);
+				f2fs_handle_error(sbi,
+						ERROR_INVALID_BLKADDR);
+				goto out;
+			}
+
+			cur_bdev = f2fs_target_device(sbi, blkaddr, NULL);
+			if (f2fs_is_multi_device(sbi)) {
+				int di = f2fs_target_device_index(sbi, blkaddr);
+
+				blkaddr -= FDEV(di).start_blk;
+			}
+
+			if (len) {
+				if (prev_bdev == cur_bdev &&
+						index == prev_index + len &&
+						blkaddr == prev_block + len) {
+					len++;
+				} else {
+					ret = f2fs_secure_erase(prev_bdev,
+						inode, prev_index, prev_block,
+						len, range.flags);
+					if (ret) {
+						f2fs_put_dnode(&dn);
+						goto out;
+					}
+
+					len = 0;
+				}
+			}
+
+			if (!len) {
+				prev_bdev = cur_bdev;
+				prev_index = index;
+				prev_block = blkaddr;
+				len = 1;
+			}
+		}
 
-	if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))
-		return -EOVERFLOW;
+		f2fs_put_dnode(&dn);
 
-	err = f2fs_dquot_initialize(inode);
-	if (err)
-		return err;
+		if (fatal_signal_pending(current)) {
+			ret = -EINTR;
+			goto out;
+		}
+		cond_resched();
+	}
 
-	f2fs_lock_op(sbi);
-	err = f2fs_transfer_project_quota(inode, kprojid);
-	if (err)
-		goto out_unlock;
+	if (len)
+		ret = f2fs_secure_erase(prev_bdev, inode, prev_index,
+				prev_block, len, range.flags);
+out:
+	filemap_invalidate_unlock(mapping);
+	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
+err:
+	inode_unlock(inode);
+	file_end_write(filp);
 
-	fi->i_projid = kprojid;
-	inode->i_ctime = current_time(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
-out_unlock:
-	f2fs_unlock_op(sbi);
-	return err;
-}
-#else
-int f2fs_transfer_project_quota(struct inode *inode, kprojid_t kprojid)
-{
-	return 0;
+	return ret;
 }
 
-static int f2fs_ioc_setproject(struct inode *inode, __u32 projid)
+static int f2fs_get_compress_option_v2(struct file *filp,
+				       unsigned long attr, __u16 *attr_size)
 {
-	if (projid != F2FS_DEF_PROJID)
+	struct inode *inode = file_inode(filp);
+	struct f2fs_comp_option_v2 option;
+
+	if (sizeof(option) < *attr_size)
+		*attr_size = sizeof(option);
+
+	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
 		return -EOPNOTSUPP;
-	return 0;
-}
-#endif
 
-int f2fs_fileattr_get(struct dentry *dentry, struct fileattr *fa)
-{
-	struct inode *inode = d_inode(dentry);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	u32 fsflags = f2fs_iflags_to_fsflags(fi->i_flags);
+	inode_lock_shared(inode);
 
-	if (IS_ENCRYPTED(inode))
-		fsflags |= FS_ENCRYPT_FL;
-	if (IS_VERITY(inode))
-		fsflags |= FS_VERITY_FL;
-	if (f2fs_has_inline_data(inode) || f2fs_has_inline_dentry(inode))
-		fsflags |= FS_INLINE_DATA_FL;
-	if (is_inode_flag_set(inode, FI_PIN_FILE))
-		fsflags |= FS_NOCOW_FL;
+	if (!f2fs_compressed_file(inode)) {
+		inode_unlock_shared(inode);
+		return -ENODATA;
+	}
 
-	fileattr_fill_flags(fa, fsflags & F2FS_GETTABLE_FS_FL);
+	option.algorithm = F2FS_I(inode)->i_compress_algorithm;
+	option.log_cluster_size = F2FS_I(inode)->i_log_cluster_size;
+	option.level = F2FS_I(inode)->i_compress_level;
+	option.flag = F2FS_I(inode)->i_compress_flag;
 
-	if (f2fs_sb_has_project_quota(F2FS_I_SB(inode)))
-		fa->fsx_projid = from_kprojid(&init_user_ns, fi->i_projid);
+	inode_unlock_shared(inode);
+
+	if (copy_to_user((void __user *)attr, &option, *attr_size))
+		return -EFAULT;
 
 	return 0;
 }
 
-int f2fs_fileattr_set(struct user_namespace *mnt_userns,
-		      struct dentry *dentry, struct fileattr *fa)
+static int f2fs_ioc_get_compress_option(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = d_inode(dentry);
-	u32 fsflags = fa->flags, mask = F2FS_SETTABLE_FS_FL;
-	u32 iflags;
-	int err;
-
-	if (unlikely(f2fs_cp_error(F2FS_I_SB(inode))))
-		return -EIO;
-	if (!f2fs_is_checkpoint_ready(F2FS_I_SB(inode)))
-		return -ENOSPC;
-	if (fsflags & ~F2FS_GETTABLE_FS_FL)
-		return -EOPNOTSUPP;
-	fsflags &= F2FS_SETTABLE_FS_FL;
-	if (!fa->flags_valid)
-		mask &= FS_COMMON_FL;
-
-	iflags = f2fs_fsflags_to_iflags(fsflags);
-	if (f2fs_mask_flags(inode->i_mode, iflags) != iflags)
-		return -EOPNOTSUPP;
-
-	err = f2fs_setflags_common(inode, iflags, f2fs_fsflags_to_iflags(mask));
-	if (!err)
-		err = f2fs_ioc_setproject(inode, fa->fsx_projid);
+	__u16 size = sizeof(struct f2fs_comp_option);
 
-	return err;
+	return f2fs_get_compress_option_v2(filp, arg, &size);
 }
 
-int f2fs_pin_file_control(struct inode *inode, bool inc)
+static int f2fs_set_compress_option_v2(struct file *filp,
+				       unsigned long attr, __u16 *attr_size)
 {
-	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_comp_option_v2 option;
+	/*
+	 * if compress_layout is not set, set file in fixed-input mode.
+	 * no need to shift COMPRESS_LEVEL
+	 */
+	short init_compr_flag = COMPRESS_FIXED_INPUT;
+	int ret = 0;
 
-	/* Use i_gc_failures for normal file as a risk signal. */
-	if (inc)
-		f2fs_i_gc_failures_write(inode,
-				fi->i_gc_failures[GC_FAILURE_PIN] + 1);
+	if (sizeof(option) < *attr_size)
+		*attr_size = sizeof(option);
 
-	if (fi->i_gc_failures[GC_FAILURE_PIN] > sbi->gc_pin_file_threshold) {
-		f2fs_warn(sbi, "%s: Enable GC = ino %lx after %x GC trials",
-			  __func__, inode->i_ino,
-			  fi->i_gc_failures[GC_FAILURE_PIN]);
-		clear_inode_flag(inode, FI_PIN_FILE);
-		return -EAGAIN;
-	}
-	return 0;
-}
+	if (!f2fs_sb_has_compression(sbi))
+		return -EOPNOTSUPP;
 
-static int f2fs_ioc_set_pin_file(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	__u32 pin;
-	int ret = 0;
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	if (get_user(pin, (__u32 __user *)arg))
+	if (copy_from_user(&option, (void __user *)attr, *attr_size))
 		return -EFAULT;
 
-	if (!S_ISREG(inode->i_mode))
+	if (option.log_cluster_size < MIN_COMPRESS_LOG_SIZE ||
+		option.log_cluster_size > MAX_COMPRESS_LOG_SIZE ||
+		option.algorithm >= COMPRESS_MAX)
 		return -EINVAL;
 
-	if (f2fs_readonly(F2FS_I_SB(inode)->sb))
-		return -EROFS;
-
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+	if (*attr_size == sizeof(struct f2fs_comp_option_v2)) {
+		if (!f2fs_is_compress_level_valid(option.algorithm,
+						  option.level))
+			return -EINVAL;
+		/* fix coverity error: Operands don't affect result, COMPRESS_MAX_FLAG==9, always false*/
+		//if (option.flag > BIT(COMPRESS_MAX_FLAG) - 1)
+		//	return -EINVAL;
+	}
 
+	file_start_write(filp);
 	inode_lock(inode);
 
-	if (f2fs_is_atomic_file(inode)) {
+	f2fs_down_write(&F2FS_I(inode)->i_sem);
+	if (!f2fs_compressed_file(inode)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	if (!pin) {
-		clear_inode_flag(inode, FI_PIN_FILE);
-		f2fs_i_gc_failures_write(inode, 0);
-		goto done;
-	}
-
-	if (f2fs_should_update_outplace(inode, NULL)) {
-		ret = -EINVAL;
+	if (f2fs_is_mmap_file(inode) || get_dirty_pages(inode)) {
+		ret = -EBUSY;
 		goto out;
 	}
 
-	if (f2fs_pin_file_control(inode, false)) {
-		ret = -EAGAIN;
+	if (F2FS_HAS_BLOCKS(inode)) {
+		if (*attr_size == sizeof(struct f2fs_comp_option_v2) &&
+		    F2FS_I(inode)->i_compress_algorithm == option.algorithm &&
+		    F2FS_I(inode)->i_log_cluster_size == option.log_cluster_size &&
+		    F2FS_I(inode)->i_compress_level == option.level) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+			if (option.flag & COMPRESS_ATIME_MASK) {
+				F2FS_I(inode)->i_compress_flag |= BIT(COMPRESS_ATIME);
+				inode->i_atime = current_time(inode);
+			}
+#endif
+			goto mark_dirty;
+		}
+		ret = -EFBIG;
 		goto out;
 	}
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		goto out;
-
-	if (!f2fs_disable_compressed_file(inode)) {
-		ret = -EOPNOTSUPP;
-		goto out;
+	F2FS_I(inode)->i_compress_algorithm = option.algorithm;
+	F2FS_I(inode)->i_log_cluster_size = option.log_cluster_size;
+	F2FS_I(inode)->i_cluster_size = BIT(option.log_cluster_size);
+	if (F2FS_I(inode)->i_compress_flag & COMPRESS_CHKSUM_MASK)
+		init_compr_flag |= BIT(COMPRESS_CHKSUM);
+	if (F2FS_I(inode)->i_compress_flag & COMPRESS_ATIME_MASK)
+		init_compr_flag |= BIT(COMPRESS_ATIME);
+	F2FS_I(inode)->i_compress_flag = init_compr_flag;
+	if (*attr_size == sizeof(struct f2fs_comp_option_v2)) {
+		F2FS_I(inode)->i_compress_level = option.level;
+		F2FS_I(inode)->i_compress_flag = option.flag;
+		if (f2fs_compress_layout(inode) == COMPRESS_FIXED_OUTPUT)
+			F2FS_I(inode)->i_compress_flag &= ~COMPRESS_CHKSUM_MASK;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		if (option.flag & COMPRESS_ATIME_MASK)
+			inode->i_atime = current_time(inode);
+#endif
 	}
+mark_dirty:
+	f2fs_mark_inode_dirty_sync(inode, true);
 
-	set_inode_flag(inode, FI_PIN_FILE);
-	ret = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
-done:
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	if (!f2fs_is_compress_backend_ready(inode))
+		f2fs_warn(sbi, "compression algorithm is successfully set, "
+			"but current kernel doesn't support this algorithm.");
 out:
+	f2fs_up_write(&F2FS_I(inode)->i_sem);
 	inode_unlock(inode);
-	mnt_drop_write_file(filp);
+	file_end_write(filp);
+
 	return ret;
 }
 
-static int f2fs_ioc_get_pin_file(struct file *filp, unsigned long arg)
+static int f2fs_ioc_set_compress_option(struct file *filp, unsigned long arg)
 {
-	struct inode *inode = file_inode(filp);
-	__u32 pin = 0;
+	__u16 size = sizeof(struct f2fs_comp_option);
 
-	if (is_inode_flag_set(inode, FI_PIN_FILE))
-		pin = F2FS_I(inode)->i_gc_failures[GC_FAILURE_PIN];
-	return put_user(pin, (u32 __user *)arg);
+	return f2fs_set_compress_option_v2(filp, arg, &size);
 }
 
-int f2fs_precache_extents(struct inode *inode)
+static int redirty_blocks(struct inode *inode, pgoff_t page_idx, int len)
+{
+	DEFINE_READAHEAD(ractl, NULL, NULL, inode->i_mapping, page_idx);
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+	pgoff_t redirty_idx = page_idx;
+	int i, page_len = 0, ret = 0;
+
+	page_cache_ra_unbounded(&ractl, len, 0);
+
+	for (i = 0; i < len; i++, page_idx++) {
+		if (time_to_inject(F2FS_M_SB(mapping), FAULT_COMPRESS_REDIRTY)) {
+			ret = -ENOMEM;
+			break;
+		}
+		page = read_cache_page(mapping, page_idx, NULL, NULL);
+		if (IS_ERR(page)) {
+			ret = PTR_ERR(page);
+			break;
+		}
+		page_len++;
+	}
+
+	for (i = 0; i < page_len; i++, redirty_idx++) {
+		page = find_lock_page(mapping, redirty_idx);
+
+		/* It will never fail, when page has pinned above */
+		f2fs_bug_on(F2FS_I_SB(inode), !page);
+
+		f2fs_wait_on_page_writeback(page, DATA, true, true);
+
+		set_page_dirty(page);
+		f2fs_put_page(page, 1);
+		f2fs_put_page(page, 0);
+	}
+
+	return ret;
+}
+
+int f2fs_decompress_inode(struct inode *inode)
 {
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct f2fs_inode_info *fi = F2FS_I(inode);
-	struct f2fs_map_blocks map;
-	pgoff_t m_next_extent;
-	loff_t end;
-	int err;
+	pgoff_t page_idx = 0, last_idx, cluster_idx;
+	unsigned int blk_per_seg = sbi->blocks_per_seg;
+	int ret;
+
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+		return -EPERM;
+
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
+		return ret;
+
+	if (!atomic_read(&fi->i_compr_blocks))
+		return 0;
+
+	f2fs_info(sbi, "start decompress ino %lu size %llu blocks %llu "
+		"cblocks %d caller %ps\n", inode->i_ino, i_size_read(inode),
+		inode->i_blocks, atomic_read(&F2FS_I(inode)->i_compr_blocks),
+		__builtin_return_address(0));
+
+	clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+	inode->i_ctime = current_time(inode);
+
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	last_idx >>= fi->i_log_cluster_size;
+
+	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
+		page_idx = cluster_idx << fi->i_log_cluster_size;
 
-	if (is_inode_flag_set(inode, FI_NO_EXTENT))
-		return -EOPNOTSUPP;
+		if (!f2fs_is_compressed_cluster(inode, page_idx))
+			continue;
 
-	map.m_lblk = 0;
-	map.m_pblk = 0;
-	map.m_next_pgofs = NULL;
-	map.m_next_extent = &m_next_extent;
-	map.m_seg_type = NO_CHECK_TYPE;
-	map.m_may_create = false;
-	end = max_file_blocks(inode);
+		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
+		if (ret < 0)
+			break;
 
-	while (map.m_lblk < end) {
-		map.m_len = end - map.m_lblk;
+		if (get_dirty_pages(inode) >= blk_per_seg) {
+			ret = filemap_fdatawrite(inode->i_mapping);
+			if (ret < 0)
+				break;
+		}
 
-		f2fs_down_write(&fi->i_gc_rwsem[WRITE]);
-		err = f2fs_map_blocks(inode, &map, F2FS_GET_BLOCK_PRECACHE);
-		f2fs_up_write(&fi->i_gc_rwsem[WRITE]);
-		if (err)
-			return err;
+		cond_resched();
+		if (fatal_signal_pending(current)) {
+			ret = -EINTR;
+			break;
+		}
+	}
 
-		map.m_lblk = m_next_extent;
+	if (!ret) {
+		if (time_to_inject(sbi, FAULT_COMPRESS_WRITEBACK)) {
+			ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							i_size_read(inode) / 2);
+			if (!ret)
+				ret = -EIO;
+			goto out;
+		}
+		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							LLONG_MAX);
 	}
 
-	return 0;
-}
+out:
+	if (ret)
+		f2fs_warn(sbi, "%s: The file might be partially decompressed (errno=%d). Please delete the file.",
+			  __func__, ret);
+	f2fs_info(sbi, "end decompress ino %lu size %llu blocks %llu cblocks %d ret %d\n",
+		inode->i_ino, i_size_read(inode), inode->i_blocks,
+		atomic_read(&F2FS_I(inode)->i_compr_blocks), ret);
 
-static int f2fs_ioc_precache_extents(struct file *filp)
-{
-	return f2fs_precache_extents(file_inode(filp));
+	return ret;
 }
 
-static int f2fs_ioc_resize_fs(struct file *filp, unsigned long arg)
+static int f2fs_ioc_decompress_file(struct file *filp)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(file_inode(filp));
-	__u64 block_count;
-
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+	if (!f2fs_sb_has_compression(sbi) ||
+			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
+		return -EOPNOTSUPP;
 
-	if (copy_from_user(&block_count, (void __user *)arg,
-			   sizeof(block_count)))
-		return -EFAULT;
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	return f2fs_resize_fs(filp, block_count);
-}
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
-static int f2fs_ioc_enable_verity(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
+	f2fs_balance_fs(F2FS_I_SB(inode), true);
 
-	f2fs_update_time(F2FS_I_SB(inode), REQ_TIME);
+	file_start_write(filp);
+	inode_lock(inode);
 
-	if (!f2fs_sb_has_verity(F2FS_I_SB(inode))) {
-		f2fs_warn(F2FS_I_SB(inode),
-			  "Can't enable fs-verity on inode %lu: the verity feature is not enabled on this filesystem",
-			  inode->i_ino);
-		return -EOPNOTSUPP;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode)) {
+		ret = -EACCES;
+		goto out;
 	}
+#endif
 
-	return fsverity_ioctl_enable(filp, (const void __user *)arg);
-}
-
-static int f2fs_ioc_measure_verity(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	if (!f2fs_is_compress_backend_ready(inode)) {
+		ret = -EOPNOTSUPP;
+		goto out;
+	}
 
-	return fsverity_ioctl_measure(filp, (void __user *)arg);
-}
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-static int f2fs_ioc_read_verity_metadata(struct file *filp, unsigned long arg)
-{
-	if (!f2fs_sb_has_verity(F2FS_I_SB(file_inode(filp))))
-		return -EOPNOTSUPP;
+	ret = f2fs_decompress_inode(inode);
+out:
+	inode_unlock(inode);
+	file_end_write(filp);
 
-	return fsverity_ioctl_read_metadata(filp, (const void __user *)arg);
+	return ret;
 }
 
-static int f2fs_ioc_getfslabel(struct file *filp, unsigned long arg)
+static int f2fs_ioc_compress_file(struct file *filp)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	char *vbuf;
-	int count;
-	int err = 0;
-
-	vbuf = f2fs_kzalloc(sbi, MAX_VOLUME_NAME, GFP_KERNEL);
-	if (!vbuf)
-		return -ENOMEM;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	pgoff_t page_idx = 0, last_idx, cluster_idx;
+	unsigned int blk_per_seg = sbi->blocks_per_seg;
+	int ret;
 
-	f2fs_down_read(&sbi->sb_lock);
-	count = utf16s_to_utf8s(sbi->raw_super->volume_name,
-			ARRAY_SIZE(sbi->raw_super->volume_name),
-			UTF16_LITTLE_ENDIAN, vbuf, MAX_VOLUME_NAME);
-	f2fs_up_read(&sbi->sb_lock);
+	if (!f2fs_sb_has_compression(sbi) || !may_compress ||
+			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
+		return -EOPNOTSUPP;
 
-	if (copy_to_user((char __user *)arg, vbuf,
-				min(FSLABEL_MAX, count)))
-		err = -EFAULT;
+	if (!(filp->f_mode & FMODE_WRITE))
+		return -EBADF;
 
-	kfree(vbuf);
-	return err;
-}
+	if (!f2fs_compressed_file(inode))
+		return -EINVAL;
 
-static int f2fs_ioc_setfslabel(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	char *vbuf;
-	int err = 0;
+	f2fs_balance_fs(sbi, true);
 
-	if (!capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	file_start_write(filp);
+	inode_lock(inode);
 
-	vbuf = strndup_user((const char __user *)arg, FSLABEL_MAX);
-	if (IS_ERR(vbuf))
-		return PTR_ERR(vbuf);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_is_deduped_inode(inode)) {
+		ret = -EACCES;
+		goto out;
+	}
+#endif
 
-	err = mnt_want_write_file(filp);
-	if (err)
+	if (!f2fs_is_compress_backend_ready(inode)) {
+		ret = -EOPNOTSUPP;
 		goto out;
+	}
 
-	f2fs_down_write(&sbi->sb_lock);
+	f2fs_info(sbi, "compress ino %lu (%pd) size %llu blocks %llu released %d\n",
+		inode->i_ino, file_dentry(filp), i_size_read(inode),
+		inode->i_blocks, is_inode_flag_set(inode, FI_COMPRESS_RELEASED));
 
-	memset(sbi->raw_super->volume_name, 0,
-			sizeof(sbi->raw_super->volume_name));
-	utf8s_to_utf16s(vbuf, strlen(vbuf), UTF16_LITTLE_ENDIAN,
-			sbi->raw_super->volume_name,
-			ARRAY_SIZE(sbi->raw_super->volume_name));
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
-	err = f2fs_commit_super(sbi, false);
+	inode->i_ctime = current_time(inode);
 
-	f2fs_up_write(&sbi->sb_lock);
+	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
+	if (ret)
+		goto out;
 
-	mnt_drop_write_file(filp);
-out:
-	kfree(vbuf);
-	return err;
-}
+	set_inode_flag(inode, FI_ENABLE_COMPRESS);
 
-static int f2fs_get_compress_blocks(struct file *filp, unsigned long arg)
-{
-	struct inode *inode = file_inode(filp);
-	__u64 blocks;
+	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	last_idx >>= fi->i_log_cluster_size;
 
-	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
-		return -EOPNOTSUPP;
+	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
+		page_idx = cluster_idx << fi->i_log_cluster_size;
 
-	if (!f2fs_compressed_file(inode))
-		return -EINVAL;
+		if (f2fs_is_sparse_cluster(inode, page_idx))
+			continue;
 
-	blocks = atomic_read(&F2FS_I(inode)->i_compr_blocks);
-	return put_user(blocks, (u64 __user *)arg);
-}
+		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
+		if (ret < 0)
+			break;
 
-static int release_compress_blocks(struct dnode_of_data *dn, pgoff_t count)
-{
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	unsigned int released_blocks = 0;
-	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
-	block_t blkaddr;
-	int i;
+		if (get_dirty_pages(inode) >= blk_per_seg) {
+			ret = filemap_fdatawrite(inode->i_mapping);
+			if (ret < 0)
+				break;
+		}
 
-	for (i = 0; i < count; i++) {
-		blkaddr = data_blkaddr(dn->inode, dn->node_page,
-						dn->ofs_in_node + i);
+		cond_resched();
+		if (fatal_signal_pending(current)) {
+			ret = -EINTR;
+			break;
+		}
+	}
 
-		if (!__is_valid_data_blkaddr(blkaddr))
-			continue;
-		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
-					DATA_GENERIC_ENHANCE))) {
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			return -EFSCORRUPTED;
+	if (!ret) {
+		if (time_to_inject(sbi, FAULT_COMPRESS_WRITEBACK)) {
+			ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							i_size_read(inode) / 2);
+			if (!ret)
+				ret = -EIO;
+			goto next;
 		}
+		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
+							LLONG_MAX);
+	}
+next:
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
+		clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+		F2FS_I(inode)->i_flags |= F2FS_NOCOMP_FL;
+		f2fs_mark_inode_dirty_sync(inode, true);
 	}
+#else
+	clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+#endif
+
+	if (ret)
+		f2fs_warn(sbi, "%s: The file might be partially compressed (errno=%d). Please delete the file.",
+			  __func__, ret);
+out:
+	f2fs_info(sbi, "end compress ino %lu (%pd) size %llu blocks %llu cblocks %d ret %d\n",
+		inode->i_ino, file_dentry(filp), i_size_read(inode),
+		inode->i_blocks, atomic_read(&F2FS_I(inode)->i_compr_blocks), ret);
+
+	inode_unlock(inode);
+	file_end_write(filp);
+
+	return ret;
+}
+
+#ifdef CONFIG_F2FS_APPBOOST
+#define BOOST_MAX_FILES 1019
+#define BOOST_FILE_STATE_FINISH 1
+#define F2FS_BOOSTFILE_VERSION 0xF2F5
+#define BOOSTFILE_MAX_BITMAP (1<<20)
+#define PRELOAD_MAX_TIME	(2000)
+
+/* structure on disk */
+struct merge_summary_dinfo {
+	__le32 num;
+	__le32 version;
+	__le32 state;
+	__le32 tail;
+	__le32 checksum;
+	__le32 fsize[BOOST_MAX_FILES];
+};
 
-	while (count) {
-		int compr_blocks = 0;
+struct merge_extent_dinfo {
+	__le32 index;
+	__le32 length;
+	__le32 index_in_mfile;
+};
 
-		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
-			blkaddr = f2fs_data_blkaddr(dn);
+struct merge_file_dinfo {
+	__le32 ino;
+	__le32 extent_count;
+	__le32 i_generation;
+	__le32 REV;
+	__le64 mtime;
+	struct merge_extent_dinfo extents[0];
+};
 
-			if (i == 0) {
-				if (blkaddr == COMPRESS_ADDR)
-					continue;
-				dn->ofs_in_node += cluster_size;
-				goto next;
-			}
+/* inmem manage structure */
+struct merge_summary {
+	int num;
+	int version;
+	int state;
+	int tail;
+	u32 checksum;
+	int fsize[BOOST_MAX_FILES];
+};
 
-			if (__is_valid_data_blkaddr(blkaddr))
-				compr_blocks++;
+struct merge_extent {
+	unsigned index;
+	unsigned length;
+	unsigned index_in_mfile;
+};
 
-			if (blkaddr != NEW_ADDR)
-				continue;
+struct merge_file {
+	unsigned ino;
+	unsigned extent_count;
+	unsigned i_generation;
+	unsigned REV;
+	u64 mtime;
+	struct merge_extent extents[0];
+};
 
-			f2fs_set_data_blkaddr(dn, NULL_ADDR);
-		}
+/* manage structure in f2fs inode info */
+struct fi_merge_manage {
+	int num;
+	unsigned long cur_blocks;
+	struct list_head list;
+};
 
-		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, false);
-		dec_valid_block_count(sbi, dn->inode,
-					cluster_size - compr_blocks);
+struct file_list_node {
+	struct list_head list;
+	struct list_head ext_list;
+	u64 bitmax;
+	unsigned long *bitmap;
+	struct merge_file merge_file;
+};
 
-		released_blocks += cluster_size - compr_blocks;
-next:
-		count -= cluster_size;
-	}
+struct extent_list_node {
+	struct list_head list;
+	struct merge_extent extent;
+};
 
-	return released_blocks;
+static bool f2fs_appboost_enable(struct f2fs_sb_info *sbi)
+{
+	return sbi->appboost;
 }
 
-static int f2fs_release_compress_blocks(struct file *filp, unsigned long arg)
+static unsigned int f2fs_appboost_maxblocks(struct f2fs_sb_info *sbi)
 {
-	struct inode *inode = file_inode(filp);
+	return sbi->appboost_max_blocks;
+}
+
+static int f2fs_file_read(struct file *file, loff_t offset, unsigned char *data, unsigned int size)
+{
+	struct inode *inode = file_inode(file);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	pgoff_t page_idx = 0, last_idx;
-	unsigned int released_blocks = 0;
-	int ret;
-	int writecount;
+	if (time_to_inject(sbi, FAULT_READ_ERROR)) {
+		return -EIO;
+	}
+	return kernel_read(file, data, size, &offset);
+}
 
-	if (!f2fs_sb_has_compression(sbi))
-		return -EOPNOTSUPP;
+static int f2fs_file_write(struct file *file, loff_t off, unsigned char *data, unsigned int size)
+{
+	struct inode *inode = file_inode(file);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct address_space *mapping = inode->i_mapping;
+	const struct address_space_operations *a_ops = mapping->a_ops;
+	loff_t offset = off & (PAGE_SIZE - 1);
+	size_t towrite = size;
+	struct page *page;
+	void *fsdata = NULL;
+	char *kaddr;
+	int err = 0;
+	int tocopy;
 
-	if (!f2fs_compressed_file(inode))
-		return -EINVAL;
+	if (time_to_inject(sbi, FAULT_WRITE_ERROR)) {
+		return -EIO;
+	}
+	// if no set this, prepare_write_begin will return 0 directly and get the error block
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 0)
+	set_inode_flag(inode, FI_NO_PREALLOC);
+#endif
+	while (towrite > 0) {
+		tocopy = min_t(unsigned long, PAGE_SIZE - offset, towrite);
+retry:
+		err = a_ops->write_begin(NULL, mapping, off, tocopy,
+								&page, &fsdata);
+		if (unlikely(err)) {
+			if (err == -ENOMEM) {
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 0)
+				congestion_wait(BLK_RW_ASYNC,
+							DEFAULT_IO_TIMEOUT);
+#else
+				f2fs_io_schedule_timeout(DEFAULT_IO_TIMEOUT);
+#endif
+				goto retry;
+			}
+			break;
+		}
 
-	if (f2fs_readonly(sbi->sb))
-		return -EROFS;
+		kaddr = kmap_atomic(page);
+		memcpy(kaddr + offset, data, tocopy);
+		kunmap_atomic(kaddr);
+		flush_dcache_page(page);
+		a_ops->write_end(NULL, mapping, off, tocopy, tocopy,
+							page, fsdata);
+		offset = 0;
+		towrite -= tocopy;
+		off += tocopy;
+		data += tocopy;
+		cond_resched();
+	}
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(5, 15, 0)
+	clear_inode_flag(inode, FI_NO_PREALLOC);
+#endif
+	if (size == towrite)
+		return err;
 
-	f2fs_balance_fs(sbi, true);
+	inode->i_mtime = inode->i_ctime = current_time(inode);
+	f2fs_mark_inode_dirty_sync(inode, false);
 
-	inode_lock(inode);
+	return size - towrite;
+}
 
-	writecount = atomic_read(&inode->i_writecount);
-	if ((filp->f_mode & FMODE_WRITE && writecount != 1) ||
-			(!(filp->f_mode & FMODE_WRITE) && writecount)) {
-		ret = -EBUSY;
-		goto out;
+static struct fi_merge_manage *f2fs_init_merge_manage(struct inode *inode)
+{
+	struct fi_merge_manage *fmm;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	fmm = f2fs_kmalloc(sbi, sizeof(struct fi_merge_manage), GFP_KERNEL);
+	if (!fmm) {
+		return NULL;
 	}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
-		ret = -EINVAL;
-		goto out;
-	}
+	INIT_LIST_HEAD(&fmm->list);
+	fmm->num = 0;
+	fmm->cur_blocks = 0;
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret)
-		goto out;
+	return fmm;
+}
 
-	set_inode_flag(inode, FI_COMPRESS_RELEASED);
-	inode->i_ctime = current_time(inode);
-	f2fs_mark_inode_dirty_sync(inode, true);
+void f2fs_boostfile_free(struct inode *inode)
+{
+	struct fi_merge_manage *fmm;
+	struct file_list_node *fm_node, *fm_tmp;
+	struct extent_list_node *fm_ext_node, *fm_ext_tmp;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 
-	if (!atomic_read(&F2FS_I(inode)->i_compr_blocks))
-		goto out;
+	if (!fi->i_boostfile) {
+		return;
+	}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
+	fmm = (struct fi_merge_manage *)fi->i_boostfile;
+	list_for_each_entry_safe(fm_node, fm_tmp, &fmm->list, list) {
+		kvfree(fm_node->bitmap);
 
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+		list_for_each_entry_safe(fm_ext_node, fm_ext_tmp, &fm_node->ext_list, list) {
+			list_del(&fm_ext_node->list);
+			kfree(fm_ext_node);
+		}
 
-	while (page_idx < last_idx) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
+		list_del(&fm_node->list);
+		kfree(fm_node);
+	}
+	kfree(fmm);
+	fi->i_boostfile = NULL;
+}
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
-		if (ret) {
-			if (ret == -ENOENT) {
-				page_idx = f2fs_get_next_page_offset(&dn,
-								page_idx);
-				ret = 0;
-				continue;
-			}
-			break;
+static struct file_list_node *f2fs_search_merge_file(struct fi_merge_manage *fmm, unsigned ino)
+{
+	struct file_list_node *fm_node, *fm_tmp;
+
+	list_for_each_entry_safe(fm_node, fm_tmp, &fmm->list, list) {
+		if (fm_node->merge_file.ino == ino) {
+			return fm_node;
 		}
+	}
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
-		count = round_up(count, F2FS_I(inode)->i_cluster_size);
+	return NULL;
+}
 
-		ret = release_compress_blocks(&dn, count);
+static int _f2fs_insert_merge_extent(struct f2fs_sb_info* sbi,
+				     struct file_list_node *fm_node, unsigned start,
+				     unsigned end, struct fi_merge_manage *fmm, u32 max_blocks)
+{
+	struct extent_list_node *ext_node;
+	struct extent_list_node *ext_tail;
+	unsigned length;
 
-		f2fs_put_dnode(&dn);
+	if (fmm->cur_blocks >= max_blocks)
+		return -EOVERFLOW;
 
-		if (ret < 0)
-			break;
+	// update the end
+	if (((end - start + 1) + fmm->cur_blocks) >= max_blocks)
+		end = (max_blocks - fmm->cur_blocks) + start - 1;
 
-		page_idx += count;
-		released_blocks += ret;
+	length = end - start + 1;
+	ext_tail = list_last_entry(&(fm_node->ext_list), struct extent_list_node, list);
+	if (ext_tail) {
+		if (start == (ext_tail->extent.index + ext_tail->extent.length)) {
+			ext_tail->extent.length += length;
+			fmm->cur_blocks += length;
+			return 0;
+		}
 	}
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-out:
-	inode_unlock(inode);
+	ext_node = (struct extent_list_node *)f2fs_kmalloc(sbi, sizeof(struct extent_list_node), GFP_KERNEL);
+	if (!ext_node) {
+		return -ENOMEM;
+	}
+	ext_node->extent.index = start;
+	ext_node->extent.length = length;
+	ext_node->extent.index_in_mfile = 0;
 
-	mnt_drop_write_file(filp);
+	INIT_LIST_HEAD(&ext_node->list);
+	list_add_tail(&(ext_node->list), &(fm_node->ext_list));
+	fm_node->merge_file.extent_count++;
+	fmm->cur_blocks += length;
 
-	if (ret >= 0) {
-		ret = put_user(released_blocks, (u64 __user *)arg);
-	} else if (released_blocks &&
-			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
-			"iblocks=%llu, released=%u, compr_blocks=%u, "
-			"run fsck to fix.",
-			__func__, inode->i_ino, inode->i_blocks,
-			released_blocks,
-			atomic_read(&F2FS_I(inode)->i_compr_blocks));
-	}
+	return 0;
 
-	return ret;
 }
 
-static int reserve_compress_blocks(struct dnode_of_data *dn, pgoff_t count,
-		unsigned int *reserved_blocks)
+static int f2fs_insert_merge_extent(struct fi_merge_manage *fmm, struct f2fs_sb_info *sbi,
+				 struct file_list_node *fm_node, struct merge_extent *fm_ext)
 {
-	struct f2fs_sb_info *sbi = F2FS_I_SB(dn->inode);
-	int cluster_size = F2FS_I(dn->inode)->i_cluster_size;
-	block_t blkaddr;
-	int i;
-
-	for (i = 0; i < count; i++) {
-		blkaddr = data_blkaddr(dn->inode, dn->node_page,
-						dn->ofs_in_node + i);
+	unsigned low = fm_ext->index;
+	unsigned high = low + fm_ext->length - 1;
+	unsigned start = 1, end = 0;
+	unsigned i;
+	bool spliting = false;
+	int ret = 0;
 
-		if (!__is_valid_data_blkaddr(blkaddr))
-			continue;
-		if (unlikely(!f2fs_is_valid_blkaddr(sbi, blkaddr,
-					DATA_GENERIC_ENHANCE))) {
-			f2fs_handle_error(sbi, ERROR_INVALID_BLKADDR);
-			return -EFSCORRUPTED;
-		}
+	if (high >= fm_node->bitmax) {
+		f2fs_warn(sbi, "f2fs_insert_merge_extent range bad value [%u,%u,%llu]",
+							low, high, fm_node->bitmax);
+		return -EINVAL;
 	}
 
-	while (count) {
-		int compr_blocks = 0;
-		blkcnt_t reserved = 0;
-		blkcnt_t to_reserved;
-		int ret;
-
-		for (i = 0; i < cluster_size; i++) {
-			blkaddr = data_blkaddr(dn->inode, dn->node_page,
-						dn->ofs_in_node + i);
-
-			if (i == 0) {
-				if (blkaddr != COMPRESS_ADDR) {
-					dn->ofs_in_node += cluster_size;
-					goto next;
-				}
-				continue;
-			}
+	for (i = low; i <= high; i++) {
+		if (test_and_set_bit(i, fm_node->bitmap)) {
+			if (spliting) {
+				ret = _f2fs_insert_merge_extent(sbi, fm_node, start,
+						end, fmm, f2fs_appboost_maxblocks(sbi));
+				if (ret)
+					return ret;
 
-			/*
-			 * compressed cluster was not released due to it
-			 * fails in release_compress_blocks(), so NEW_ADDR
-			 * is a possible case.
-			 */
-			if (blkaddr == NEW_ADDR) {
-				reserved++;
+				// reset
+				spliting = false;
+				start = 1;
+				end = 0;
+			} else {
 				continue;
 			}
-			if (__is_valid_data_blkaddr(blkaddr)) {
-				compr_blocks++;
+		} else {
+			if (spliting) {
+				end = i;
 				continue;
+			} else {
+				start = i;
+				end = i;
+				spliting = true;
 			}
 		}
+	}
 
-		to_reserved = cluster_size - compr_blocks - reserved;
+	if (end >= start && spliting)
+		ret = _f2fs_insert_merge_extent(sbi, fm_node, start,
+						end, fmm, f2fs_appboost_maxblocks(sbi));
 
-		/* for the case all blocks in cluster were reserved */
-		if (reserved && to_reserved == 1) {
-			dn->ofs_in_node += cluster_size;
-			goto next;
-		}
+	return ret;
+}
 
-		ret = inc_valid_block_count(sbi, dn->inode,
-						&to_reserved, false);
-		if (unlikely(ret))
-			return ret;
+static int f2fs_insert_merge_file_user(struct fi_merge_manage *fmm, struct f2fs_sb_info *sbi,
+					struct file_list_node *fm_node, struct merge_file_user *fm_u)
+{
+	int ret = 0;
+	struct merge_extent *fm_ext = NULL;
+	int i;
 
-		for (i = 0; i < cluster_size; i++, dn->ofs_in_node++) {
-			if (f2fs_data_blkaddr(dn) == NULL_ADDR)
-				f2fs_set_data_blkaddr(dn, NEW_ADDR);
-		}
+	fm_ext = (struct merge_extent *)f2fs_kvmalloc(sbi,
+				sizeof(struct merge_extent) * fm_u->extent_count, GFP_KERNEL);
+	if (!fm_ext) {
+		ret = -ENOMEM;
+		goto fail;
+	}
 
-		f2fs_i_compr_blocks_update(dn->inode, compr_blocks, true);
+	if (copy_from_user(fm_ext, (struct merge_extent __user *)fm_u->extents,
+		sizeof(struct merge_extent) * fm_u->extent_count)) {
+		ret = -EFAULT;
+		goto fail;
+	}
 
-		*reserved_blocks += to_reserved;
-next:
-		count -= cluster_size;
+	for (i = 0; i < fm_u->extent_count; i++) {
+		if (fm_ext[i].length == 0) {
+			f2fs_warn(sbi, "f2fs_ioc_merge_user check ext length == 0!");
+			ret = -EINVAL;
+			goto fail;
+		}
+
+		ret = f2fs_insert_merge_extent(fmm, sbi, fm_node, &fm_ext[i]);
+		if (ret) {
+			goto fail;
+		}
 	}
 
-	return 0;
+fail:
+	if (fm_ext)
+		kvfree(fm_ext);
+
+	return ret;
 }
 
-static int f2fs_reserve_compress_blocks(struct file *filp, unsigned long arg)
+static int f2fs_ioc_start_file_merge(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	pgoff_t page_idx = 0, last_idx;
-	unsigned int reserved_blocks = 0;
-	int ret;
-
-	if (!f2fs_sb_has_compression(sbi))
-		return -EOPNOTSUPP;
-
-	if (!f2fs_compressed_file(inode))
-		return -EINVAL;
+	struct inode *inode_source;
+	struct fi_merge_manage *fmm;
+	struct file_list_node *fm_node;
+	struct merge_file_user fm_u;
+	int ret = 0;
+	loff_t i_size;
 
 	if (f2fs_readonly(sbi->sb))
 		return -EROFS;
 
-	ret = mnt_want_write_file(filp);
-	if (ret)
-		return ret;
-
-	f2fs_balance_fs(sbi, true);
+	if  (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
 
-	inode_lock(inode);
+	if (!inode_trylock(inode))
+		return -EAGAIN;
 
-	if (!is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
-		ret = -EINVAL;
-		goto unlock_inode;
+	if (!(filp->f_flags & __O_TMPFILE)) {
+		f2fs_warn(sbi, "f2fs_ioc_start_file_merge check flags failed!");
+		inode_unlock(inode);
+		return -EINVAL;
 	}
 
-	if (atomic_read(&F2FS_I(inode)->i_compr_blocks))
-		goto unlock_inode;
-
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(inode->i_mapping);
+	if (!fi->i_boostfile) {
+		fi->i_boostfile = f2fs_init_merge_manage(inode);
+		if (!fi->i_boostfile) {
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge init private failed!");
+			inode_unlock(inode);
+			return -ENOMEM;
+		}
+	}
 
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
+	fmm = fi->i_boostfile;
+	if (fmm->num >= BOOST_MAX_FILES) {
+		f2fs_warn(sbi, "f2fs_ioc_start_file_merge num overflow!");
+		ret = -EFAULT;
+		goto fail;
+	}
 
-	while (page_idx < last_idx) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
+	if (copy_from_user(&fm_u, (struct merge_file_user __user *)arg,
+		sizeof(struct merge_file_user))) {
+		ret = -EFAULT;
+		goto fail;
+	}
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, page_idx, LOOKUP_NODE);
-		if (ret) {
-			if (ret == -ENOENT) {
-				page_idx = f2fs_get_next_page_offset(&dn,
-								page_idx);
-				ret = 0;
-				continue;
-			}
-			break;
+	fm_node = f2fs_search_merge_file(fmm, fm_u.ino);
+	if (!fm_node) {
+		inode_source = f2fs_iget(sbi->sb, fm_u.ino);
+		if (IS_ERR(inode_source)) {
+			ret = PTR_ERR(inode_source);
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge no found ino=%d", fm_u.ino);
+			goto fail;
 		}
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, last_idx - page_idx);
-		count = round_up(count, F2FS_I(inode)->i_cluster_size);
+		if (is_bad_inode(inode_source)) {
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-		ret = reserve_compress_blocks(&dn, count, &reserved_blocks);
+		if (!inode_trylock(inode_source)) {
+			iput(inode_source);
+			ret = -EAGAIN;
+			goto fail;
+		}
 
-		f2fs_put_dnode(&dn);
+		i_size = i_size_read(inode_source);
+		if (DIV_ROUND_UP(i_size, PAGE_SIZE) > BOOSTFILE_MAX_BITMAP) {
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge ino=%d, i_size=%lld", fm_u.ino, i_size);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EFAULT;
+			goto fail;
+		}
 
-		if (ret < 0)
-			break;
+		if (fm_u.mtime != timespec64_to_ns(&inode_source->i_mtime) ||
+			fm_u.i_generation != inode_source->i_generation) {
+			f2fs_warn(sbi, "f2fs_ioc_start_file_merge EKEYEXPIRED ino=%d", fm_u.ino);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_inode_support_dedup(sbi, inode_source)
+		    && !is_inode_flag_set(inode_source, FI_DATA_UN_MODIFY))
+			set_inode_flag(inode_source, FI_DATA_UN_MODIFY);
+#endif
+		inode_unlock(inode_source);
+		iput(inode_source);
+		fm_node = (struct file_list_node *)f2fs_kmalloc(sbi,
+						sizeof(struct file_list_node), GFP_KERNEL);
+		if (!fm_node) {
+			ret = -ENOMEM;
+			goto fail;
+		}
+		fm_node->merge_file.ino = fm_u.ino;
+		fm_node->merge_file.extent_count = 0;
+		fm_node->merge_file.mtime = fm_u.mtime;
+		fm_node->merge_file.i_generation = fm_u.i_generation;
+		fm_node->bitmax = DIV_ROUND_UP(i_size, PAGE_SIZE);
+		fm_node->bitmap = (unsigned long*)f2fs_kvzalloc(sbi,
+						f2fs_bitmap_size(fm_node->bitmax), GFP_KERNEL);
+		if (!fm_node->bitmap) {
+			kfree(fm_node);
+			fm_node = NULL;
+			ret = -ENOMEM;
+			goto fail;
+		}
 
-		page_idx += count;
+		INIT_LIST_HEAD(&(fm_node->ext_list));
+		list_add_tail(&(fm_node->list), &(fmm->list));
+		fmm->num++;
+	} else {
+		if (fm_node->merge_file.i_generation != fm_u.i_generation ||
+			fm_node->merge_file.mtime  != fm_u.mtime) {
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 	}
 
-	filemap_invalidate_unlock(inode->i_mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-
-	if (!ret) {
-		clear_inode_flag(inode, FI_COMPRESS_RELEASED);
-		inode->i_ctime = current_time(inode);
-		f2fs_mark_inode_dirty_sync(inode, true);
+	if (fm_u.extent_count > f2fs_appboost_maxblocks(sbi)) {
+		ret = -EKEYEXPIRED;
+		goto fail;
 	}
-unlock_inode:
+
+	ret = f2fs_insert_merge_file_user(fmm, sbi, fm_node, &fm_u);
+	// if return EOVERFLOW, we support max blocks
+	if (ret == -EOVERFLOW)
+		ret = 0;
+fail:
 	inode_unlock(inode);
-	mnt_drop_write_file(filp);
+	return ret;
+}
 
-	if (!ret) {
-		ret = put_user(reserved_blocks, (u64 __user *)arg);
-	} else if (reserved_blocks &&
-			atomic_read(&F2FS_I(inode)->i_compr_blocks)) {
-		set_sbi_flag(sbi, SBI_NEED_FSCK);
-		f2fs_warn(sbi, "%s: partial blocks were released i_ino=%lx "
-			"iblocks=%llu, reserved=%u, compr_blocks=%u, "
-			"run fsck to fix.",
-			__func__, inode->i_ino, inode->i_blocks,
-			reserved_blocks,
-			atomic_read(&F2FS_I(inode)->i_compr_blocks));
-	}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+static void f2fs_file_read_pages(struct inode *inode)
+{
+	struct backing_dev_info *bdi;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 15, 0)
+	DEFINE_READAHEAD(ractl, NULL, NULL, inode->i_mapping, 0);
+#else
+	DEFINE_READAHEAD(ractl, NULL, inode->i_mapping, 0);
+#endif
+	unsigned long max_blocks = (inode->i_size + PAGE_SIZE - 1) >> PAGE_SHIFT;
+	unsigned long nr_to_read = 0;
+	unsigned long index = 0;
 
-	return ret;
+	bdi = inode_to_bdi(inode);
+	if (!bdi)
+		return;
+
+	while (1) {
+		ractl._index = index;
+		/* equal to POSIX_FADV_SEQUENTIAL */
+		nr_to_read = min(2 * bdi->ra_pages, max_blocks - index + 1);
+		page_cache_ra_unbounded(&ractl, nr_to_read, 0);
+
+		index += nr_to_read;
+		if (index >= max_blocks)
+			return;
+	}
 }
+#endif
 
-static int f2fs_secure_erase(struct block_device *bdev, struct inode *inode,
-		pgoff_t off, block_t block, block_t len, u32 flags)
+static int merge_sync_file(struct f2fs_sb_info *sbi, struct file *file)
 {
-	sector_t sector = SECTOR_FROM_BLOCK(block);
-	sector_t nr_sects = SECTOR_FROM_BLOCK(len);
 	int ret = 0;
+	struct inode *inode = file->f_mapping->host;
 
-	if (flags & F2FS_TRIM_FILE_DISCARD) {
-		if (bdev_max_secure_erase_sectors(bdev))
-			ret = blkdev_issue_secure_erase(bdev, sector, nr_sects,
-					GFP_NOFS);
-		else
-			ret = blkdev_issue_discard(bdev, sector, nr_sects,
-					GFP_NOFS);
+	if (time_to_inject(sbi, FAULT_FSYNC_ERROR)) {
+		ret = 0;
+	} else {
+		ret = f2fs_do_sync_file(file, 0, LLONG_MAX, 0, 0);
 	}
 
-	if (!ret && (flags & F2FS_TRIM_FILE_ZEROOUT)) {
-		if (IS_ENCRYPTED(inode))
-			ret = fscrypt_zeroout_range(inode, off, block, len);
-		else
-			ret = blkdev_issue_zeroout(bdev, sector, nr_sects,
-					GFP_NOFS, 0);
+	if (ret != 0) {
+		f2fs_err(sbi, "f2fs_end_file_merge:failed to sync");
+		return ret;
 	}
 
+	if (time_to_inject(sbi, FAULT_FLUSH_ERROR)) {
+		ret = 0;
+	} else {
+		ret = f2fs_issue_flush(sbi, inode->i_ino);
+	}
+
+	if (ret != 0)
+		f2fs_err(sbi, "f2fs_end_file_merge:failed to flush");
+
 	return ret;
 }
 
-static int f2fs_sec_trim_file(struct file *filp, unsigned long arg)
+static void copy_summary_info_to_disk(struct merge_summary *summary,
+					struct merge_summary_dinfo *summary_dinfo)
 {
-	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct address_space *mapping = inode->i_mapping;
-	struct block_device *prev_bdev = NULL;
-	struct f2fs_sectrim_range range;
-	pgoff_t index, pg_end, prev_index = 0;
-	block_t prev_block = 0, len = 0;
-	loff_t end_addr;
-	bool to_end = false;
-	int ret = 0;
+	if (!summary || !summary_dinfo)
+		return;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	summary_dinfo->version = cpu_to_le32(summary->version);
+	summary_dinfo->state = cpu_to_le32(summary->state);
+	summary_dinfo->tail = cpu_to_le32(summary->tail);
+	summary_dinfo->checksum = cpu_to_le32(summary->checksum);
+	summary_dinfo->num = cpu_to_le32(summary->num);
+}
 
-	if (copy_from_user(&range, (struct f2fs_sectrim_range __user *)arg,
-				sizeof(range)))
-		return -EFAULT;
+static void copy_file_merge_info_to_disk(struct merge_file *info, struct merge_file_dinfo *dinfo)
+{
+	if (!info || !dinfo)
+		return;
 
-	if (range.flags == 0 || (range.flags & ~F2FS_TRIM_FILE_MASK) ||
-			!S_ISREG(inode->i_mode))
-		return -EINVAL;
+	dinfo->ino = cpu_to_le32(info->ino);
+	dinfo->extent_count = cpu_to_le32(info->extent_count);
+	dinfo->i_generation = cpu_to_le32(info->i_generation);
+	dinfo->mtime = cpu_to_le64(info->mtime);
+}
 
-	if (((range.flags & F2FS_TRIM_FILE_DISCARD) &&
-			!f2fs_hw_support_discard(sbi)) ||
-			((range.flags & F2FS_TRIM_FILE_ZEROOUT) &&
-			 IS_ENCRYPTED(inode) && f2fs_is_multi_device(sbi)))
-		return -EOPNOTSUPP;
+static void copy_extent_info_to_disk(struct merge_extent *extent,
+					struct merge_extent_dinfo *extent_dinfo)
+{
+	if (!extent || !extent_dinfo)
+		return;
 
-	file_start_write(filp);
-	inode_lock(inode);
+	extent_dinfo->index = cpu_to_le32(extent->index);
+	extent_dinfo->length = cpu_to_le32(extent->length);
+	extent_dinfo->index_in_mfile = cpu_to_le32(extent->index_in_mfile);
+}
 
-	if (f2fs_is_atomic_file(inode) || f2fs_compressed_file(inode) ||
-			range.start >= inode->i_size) {
+static int end_file_merge(struct file *filp, unsigned long arg)
+{
+	struct merge_summary *summary = NULL;
+	struct merge_summary_dinfo *summary_dinfo = NULL;
+	struct fi_merge_manage *fmm;
+	struct file_list_node *fm_node, *fm_tmp;
+	struct extent_list_node *fm_ext_node, *fm_ext_tmp;
+	struct merge_file *cur_merge_file;
+	struct file_list_node **merge_files_lists = NULL;
+	struct inode *inode = file_inode(filp);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct inode *inode_source;
+	struct page *page = NULL;
+	unsigned char *addr = NULL;
+	unsigned long merged_blocks = 0;
+	unsigned int max_blocks = f2fs_appboost_maxblocks(sbi);
+	struct merge_extent_dinfo extent_dinfo;
+	struct merge_file_dinfo file_dinfo;
+	loff_t offset = 0;
+	loff_t tail = 0;
+	int ret, i, k, result;
+
+	/* disk free space is not enough */
+	if (has_not_enough_free_secs(sbi, 0, max_blocks >> sbi->log_blocks_per_seg))
+		return -ENOSPC;
+
+	if (!inode_trylock(inode))
+		return -EAGAIN;
+
+	if (!(filp->f_flags & __O_TMPFILE)) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge check flags failed!");
 		ret = -EINVAL;
-		goto err;
+		goto fail;
 	}
 
-	if (range.len == 0)
-		goto err;
+	// when end and private is NULL, will return err
+	if (!fi->i_boostfile) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: i_boostfile is null");
+		ret = -EFAULT;
+		goto fail;
+	}
 
-	if (inode->i_size - range.start > range.len) {
-		end_addr = range.start + range.len;
-	} else {
-		end_addr = range.len == (u64)-1 ?
-			sbi->sb->s_maxbytes : inode->i_size;
-		to_end = true;
+	fmm = (struct fi_merge_manage *)fi->i_boostfile;
+	merge_files_lists = (struct file_list_node **)f2fs_kvmalloc(sbi,
+				sizeof(struct file_list_node *) * fmm->num, GFP_KERNEL);
+	if (!merge_files_lists) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: merge_files_lists is null");
+		ret = -ENOMEM;
+		goto fail;
 	}
 
-	if (!IS_ALIGNED(range.start, F2FS_BLKSIZE) ||
-			(!to_end && !IS_ALIGNED(end_addr, F2FS_BLKSIZE))) {
-		ret = -EINVAL;
-		goto err;
+	// 1. fill summary.
+	summary = f2fs_kzalloc(sbi, sizeof(struct merge_summary), GFP_KERNEL);
+	if (!summary) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: summary is null");
+		ret = -ENOMEM;
+		goto fail;
 	}
 
-	index = F2FS_BYTES_TO_BLK(range.start);
-	pg_end = DIV_ROUND_UP(end_addr, F2FS_BLKSIZE);
+	summary_dinfo = f2fs_kzalloc(sbi, sizeof(struct merge_summary_dinfo), GFP_KERNEL);
+	if (!summary_dinfo) {
+		f2fs_warn(sbi, "f2fs_ioc_end_file_merge: summary_dinfo is null");
+		ret = -ENOMEM;
+		goto fail;
+	}
 
-	ret = f2fs_convert_inline_inode(inode);
-	if (ret)
-		goto err;
+	list_for_each_entry_safe(fm_node, fm_tmp, &fmm->list, list) {
+		u64 size = fm_node->merge_file.extent_count *
+				sizeof(struct merge_extent_dinfo) + sizeof(struct merge_file_dinfo);
+		if (unlikely(size > INT_MAX)) {
+			ret = -EINVAL;
+			goto fail;
+		}
+		summary->fsize[summary->num] = size;
+		summary_dinfo->fsize[summary->num] = cpu_to_le32(size);
+		merge_files_lists[summary->num] = fm_node;
+		summary->num++;
+	}
+
+	// calc the offset
+	offset = sizeof(struct merge_summary);
+	for (i = 0; i < fmm->num; i++) {
+		offset += summary->fsize[i];
+	}
+	// align PAGE_SIZE
+	offset = (offset + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);
+
+	// write file
+	for (i = 0; i < fmm->num; i++) {
+		cur_merge_file = &(merge_files_lists[i]->merge_file);
+		inode_source = f2fs_iget(sbi->sb, cur_merge_file->ino);
+		if (IS_ERR(inode_source)) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge NOFOUND ino=%d", cur_merge_file->ino);
+			ret = PTR_ERR(inode_source);
+			goto fail;
+		}
 
-	f2fs_down_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-	filemap_invalidate_lock(mapping);
+		if (is_bad_inode(inode_source)) {
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-	ret = filemap_write_and_wait_range(mapping, range.start,
-			to_end ? LLONG_MAX : end_addr - 1);
-	if (ret)
-		goto out;
+		if (!inode_trylock(inode_source)) {
+			iput(inode_source);
+			ret = -EAGAIN;
+			goto fail;
+		}
 
-	truncate_inode_pages_range(mapping, range.start,
-			to_end ? -1 : end_addr - 1);
+		if (!S_ISREG(inode_source->i_mode)) {
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+		if (cur_merge_file->mtime != timespec64_to_ns(&inode_source->i_mtime)) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge mtime expired ino = %u, new_time = %llu, expired_time = %llu",
+						cur_merge_file->ino, timespec64_to_ns(&inode_source->i_mtime), cur_merge_file->mtime);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-	while (index < pg_end) {
-		struct dnode_of_data dn;
-		pgoff_t end_offset, count;
-		int i;
+		if (cur_merge_file->i_generation != inode_source->i_generation) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge i_generation has been changed ino = %u!",  cur_merge_file->ino);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
 
-		set_new_dnode(&dn, inode, NULL, NULL, 0);
-		ret = f2fs_get_dnode_of_data(&dn, index, LOOKUP_NODE);
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_inode_support_dedup(sbi, inode_source) &&
+			!is_inode_flag_set(inode_source, FI_DATA_UN_MODIFY)) {
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge FI_DATA_UN_MODIFY flag has been changed ino = %u!",  cur_merge_file->ino);
+                        inode_unlock(inode_source);
+                        iput(inode_source);
+                        ret = -EKEYEXPIRED;
+                        goto fail;
+		}
+#endif
+
+		ret = fscrypt_require_key(inode_source);
 		if (ret) {
-			if (ret == -ENOENT) {
-				index = f2fs_get_next_page_offset(&dn, index);
-				continue;
-			}
-			goto out;
+			f2fs_warn(sbi, "f2fs_ioc_end_file_merge get file ino = %lu encrypt info failed\n", inode_source->i_ino);
+			inode_unlock(inode_source);
+			iput(inode_source);
+			goto fail;
 		}
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(5, 10, 0)
+		f2fs_file_read_pages(inode_source);
+#endif
+		list_for_each_entry_safe(fm_ext_node, fm_ext_tmp, &merge_files_lists[i]->ext_list, list) {
+			fm_ext_node->extent.index_in_mfile = offset >> PAGE_SHIFT;
 
-		end_offset = ADDRS_PER_PAGE(dn.node_page, inode);
-		count = min(end_offset - dn.ofs_in_node, pg_end - index);
-		for (i = 0; i < count; i++, index++, dn.ofs_in_node++) {
-			struct block_device *cur_bdev;
-			block_t blkaddr = f2fs_data_blkaddr(&dn);
+			for (k = 0; k < fm_ext_node->extent.length; k++) {
+				if (time_to_inject(sbi, FAULT_PAGE_ERROR)) {
+					page = ERR_PTR(-EIO);
+				} else {
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+					if (f2fs_compressed_file(inode_source) &&
+						f2fs_is_compressed_cluster(inode_source,
+							fm_ext_node->extent.index + k)) {
+						f2fs_warn(sbi, "f2fs_ioc_end_file_merge ino = %u,(%d, %d) is compressed\n",
+								cur_merge_file->ino, fm_ext_node->extent.index, k);
+						inode_unlock(inode_source);
+						iput(inode_source);
+						ret = -EKEYEXPIRED;
+						goto fail;
+					}
+#endif
+					page = f2fs_get_lock_data_page(inode_source, fm_ext_node->extent.index + k, false);
+				}
+				if (IS_ERR(page)) {
+					f2fs_warn(sbi, "f2fs_find_data_page err (%d,%d) ino:%d",
+								fm_ext_node->extent.index, k, cur_merge_file->ino);
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EIO;
+					goto fail;
+				}
 
-			if (!__is_valid_data_blkaddr(blkaddr))
-				continue;
+				addr = kmap(page);
+				if (f2fs_file_write(filp, offset, addr, PAGE_SIZE) != PAGE_SIZE) {
+					kunmap(page);
+					flush_dcache_page(page);
+					f2fs_put_page(page, 1);
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EIO;
+					goto fail;
+				}
+				kunmap(page);
+				flush_dcache_page(page);
+				offset += PAGE_SIZE;
+				merged_blocks++;
+				f2fs_put_page(page, 1);
+				if (merged_blocks > max_blocks) {
+					f2fs_warn(sbi, "f2fs_ioc_end_file_merge excess max_blocks %lu,%u!!!",
+							merged_blocks, max_blocks);
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EFAULT;
+					goto fail;
+				}
 
-			if (!f2fs_is_valid_blkaddr(sbi, blkaddr,
-						DATA_GENERIC_ENHANCE)) {
-				ret = -EFSCORRUPTED;
-				f2fs_put_dnode(&dn);
-				f2fs_handle_error(sbi,
-						ERROR_INVALID_BLKADDR);
-				goto out;
+				if (fatal_signal_pending(current)) {
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -EINTR;
+					goto fail;
+				}
+
+				if (!f2fs_appboost_enable(sbi)) {
+					inode_unlock(inode_source);
+					iput(inode_source);
+					ret = -ENOTTY;
+					goto fail;
+				}
 			}
+		}
+		/* invalidate clean page */
+		invalidate_mapping_pages(inode_source->i_mapping, 0, -1);
+		inode_unlock(inode_source);
+		iput(inode_source);
+	}
+
+	if (unlikely(offset > INT_MAX)) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
 
-			cur_bdev = f2fs_target_device(sbi, blkaddr, NULL);
-			if (f2fs_is_multi_device(sbi)) {
-				int di = f2fs_target_device_index(sbi, blkaddr);
+	tail = offset;
 
-				blkaddr -= FDEV(di).start_blk;
-			}
+	// first write summary
+	offset = 0;
+	summary->version = F2FS_BOOSTFILE_VERSION;
+	summary->state = BOOST_FILE_STATE_FINISH;
+	summary->tail = tail;
+	summary->checksum = 0;
+	summary->checksum = f2fs_crc32(sbi, (void *)summary , sizeof(struct merge_summary));
 
-			if (len) {
-				if (prev_bdev == cur_bdev &&
-						index == prev_index + len &&
-						blkaddr == prev_block + len) {
-					len++;
-				} else {
-					ret = f2fs_secure_erase(prev_bdev,
-						inode, prev_index, prev_block,
-						len, range.flags);
-					if (ret) {
-						f2fs_put_dnode(&dn);
-						goto out;
-					}
+	copy_summary_info_to_disk(summary, summary_dinfo);
+	result = f2fs_file_write(filp, offset, (unsigned char *)summary_dinfo,
+				 sizeof(struct merge_summary_dinfo));
+	if (result != sizeof(struct merge_summary_dinfo)) {
+		ret = -EIO;
+		goto fail;
+	}
 
-					len = 0;
-				}
+	// write the file info
+	offset = sizeof(struct merge_summary_dinfo);
+	for (i = 0; i < fmm->num; i++) {
+		copy_file_merge_info_to_disk(&(merge_files_lists[i]->merge_file), &file_dinfo);
+		result = f2fs_file_write(filp, offset, (unsigned char *)&(file_dinfo),
+					 sizeof(file_dinfo));
+		if (result != sizeof(file_dinfo)) {
+			ret = -EIO;
+			goto fail;
+		}
+		offset += sizeof(file_dinfo);
+
+		list_for_each_entry_safe(fm_ext_node, fm_ext_tmp, &merge_files_lists[i]->ext_list, list) {
+			copy_extent_info_to_disk(&fm_ext_node->extent, &extent_dinfo);
+			result = f2fs_file_write(filp, offset, (unsigned char *)&extent_dinfo, sizeof(extent_dinfo));
+			if (result != sizeof(extent_dinfo)) {
+				ret = -EIO;
+				goto fail;
 			}
+			offset += sizeof(extent_dinfo);
 
-			if (!len) {
-				prev_bdev = cur_bdev;
-				prev_index = index;
-				prev_block = blkaddr;
-				len = 1;
+			if (fatal_signal_pending(current)) {
+				ret = -EINTR;
+				goto fail;
+			}
+
+			if (!f2fs_appboost_enable(sbi)) {
+				ret = -ENOTTY;
+				goto fail;
 			}
 		}
+	}
 
-		f2fs_put_dnode(&dn);
+	ret = merge_sync_file(sbi, filp);
+	if (ret)
+		goto fail;
 
-		if (fatal_signal_pending(current)) {
-			ret = -EINTR;
-			goto out;
-		}
-		cond_resched();
+	offset = tail;
+	if (time_to_inject(sbi, FAULT_WRITE_TAIL_ERROR)) {
+		summary_dinfo->checksum = F2FS_BOOSTFILE_VERSION;
+	}
+	result = f2fs_file_write(filp, offset, (unsigned char *)summary_dinfo,
+				 sizeof(struct merge_summary_dinfo));
+	if (result != sizeof(struct merge_summary_dinfo)) {
+		ret = -EIO;
+		goto fail;
 	}
 
-	if (len)
-		ret = f2fs_secure_erase(prev_bdev, inode, prev_index,
-				prev_block, len, range.flags);
-out:
-	filemap_invalidate_unlock(mapping);
-	f2fs_up_write(&F2FS_I(inode)->i_gc_rwsem[WRITE]);
-err:
-	inode_unlock(inode);
-	file_end_write(filp);
+#ifdef CONFIG_F2FS_FS_DEDUP
+        if (f2fs_inode_support_dedup(sbi, inode)) {
+                set_inode_flag(inode, FI_MERGED_FILE);
+                set_inode_flag(inode, FI_DATA_UN_MODIFY);
+        }
+#endif
+	ret = merge_sync_file(sbi, filp);
+fail:
+	f2fs_boostfile_free(inode);
+        inode_unlock(inode);
+	if (merge_files_lists)
+		kvfree(merge_files_lists);
+	if (summary)
+		kfree(summary);
+	if (summary_dinfo)
+		kfree(summary_dinfo);
 
 	return ret;
 }
 
-static int f2fs_ioc_get_compress_option(struct file *filp, unsigned long arg)
+static int f2fs_ioc_end_file_merge(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_comp_option option;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	int ret;
 
-	if (!f2fs_sb_has_compression(F2FS_I_SB(inode)))
-		return -EOPNOTSUPP;
+	if (f2fs_readonly(sbi->sb))
+		return -EROFS;
 
-	inode_lock_shared(inode);
+	if  (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
 
-	if (!f2fs_compressed_file(inode)) {
-		inode_unlock_shared(inode);
-		return -ENODATA;
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
+
+	ret = end_file_merge(filp, arg);
+
+	mnt_drop_write_file(filp);
+
+	return ret;
+}
+
+static inline bool appboost_should_abort(struct inode *inode,
+					 unsigned long boost_start, unsigned int interval)
+{
+	if (time_after(jiffies, boost_start + interval))
+		return true;
+
+	if (atomic_read(&(F2FS_I(inode)->appboost_abort))) {
+		atomic_set(&(F2FS_I(inode)->appboost_abort), 0);
+		return true;
 	}
 
-	option.algorithm = F2FS_I(inode)->i_compress_algorithm;
-	option.log_cluster_size = F2FS_I(inode)->i_log_cluster_size;
+	return false;
+}
 
-	inode_unlock_shared(inode);
+static int f2fs_ioc_abort_preload_file(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 
-	if (copy_to_user((struct f2fs_comp_option __user *)arg, &option,
-				sizeof(option)))
-		return -EFAULT;
+	if (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
+
+	atomic_set(&(F2FS_I(inode)->appboost_abort), 1);
 
 	return 0;
 }
 
-static int f2fs_ioc_set_compress_option(struct file *filp, unsigned long arg)
+static void copy_summary_info_from_disk(struct merge_summary *summary,
+					struct merge_summary_dinfo *summary_dinfo)
+{
+	int i;
+
+	if (!summary || !summary_dinfo)
+		return;
+
+	summary->num = le32_to_cpu(summary_dinfo->num);
+	summary->version = le32_to_cpu(summary_dinfo->version);
+	summary->state = le32_to_cpu(summary_dinfo->state);
+	summary->tail = le32_to_cpu(summary_dinfo->tail);
+	summary->checksum = le32_to_cpu(summary_dinfo->checksum);
+
+	for (i = 0; i < min(summary->num, BOOST_MAX_FILES); i++)
+		summary->fsize[i] = le32_to_cpu(summary_dinfo->fsize[i]);
+}
+
+static void copy_extent_info_from_disk(struct merge_extent *extent,
+					struct merge_extent_dinfo *d_extent)
+{
+	if (!extent || !d_extent)
+		return;
+
+	extent->index = le32_to_cpu(d_extent->index);
+	extent->length = le32_to_cpu(d_extent->length);
+	extent->index_in_mfile = le32_to_cpu(d_extent->index_in_mfile);
+}
+
+static void copy_file_merge_info_from_disk(struct merge_file *info, struct merge_file_dinfo *dinfo)
+{
+	if (!info || !dinfo)
+		return;
+
+	info->ino = le32_to_cpu(dinfo->ino);
+	info->extent_count = le32_to_cpu(dinfo->extent_count);
+	info->i_generation = le32_to_cpu(dinfo->i_generation);
+	info->mtime = le64_to_cpu(dinfo->mtime);
+}
+
+static int f2fs_ioc_preload_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_comp_option option;
+	struct inode *inode_source = NULL;
+	struct merge_summary *summary = NULL;
+	struct merge_summary_dinfo *summary_dinfo = NULL;
+	unsigned char *page_addr = NULL;
+	struct page *page = NULL;
+	unsigned char *buf = NULL;
+	unsigned long boost_start = jiffies;
+	/* arg indicate ms to run */
+	unsigned long interval, interval_ms;
+	loff_t pos = 0;
+	loff_t tail = 0;
+	long long pos_in = 0;
+	unsigned long to_read = 0;
 	int ret = 0;
+	int i, j, k;
+	int checksum = 0;
 
-	if (!f2fs_sb_has_compression(sbi))
-		return -EOPNOTSUPP;
-
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (!f2fs_appboost_enable(sbi))
+		return -ENOTTY;
 
-	if (copy_from_user(&option, (struct f2fs_comp_option __user *)arg,
-				sizeof(option)))
+	if (get_user(interval_ms, (unsigned long __user *)arg))
 		return -EFAULT;
 
-	if (option.log_cluster_size < MIN_COMPRESS_LOG_SIZE ||
-		option.log_cluster_size > MAX_COMPRESS_LOG_SIZE ||
-		option.algorithm >= COMPRESS_MAX)
-		return -EINVAL;
+	if (interval_ms > PRELOAD_MAX_TIME)
+		interval = PRELOAD_MAX_TIME * HZ / 1000;
+	else
+		interval = interval_ms * HZ / 1000;
 
-	file_start_write(filp);
-	inode_lock(inode);
+	if (!inode_trylock(inode))
+		return -EAGAIN;
 
-	f2fs_down_write(&F2FS_I(inode)->i_sem);
-	if (!f2fs_compressed_file(inode)) {
-		ret = -EINVAL;
-		goto out;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_inode_support_dedup(sbi, inode) &&
+			(!is_inode_flag_set(inode, FI_MERGED_FILE) ||
+			!is_inode_flag_set(inode, FI_DATA_UN_MODIFY))) {
+		f2fs_err(sbi, "merged file has something wrong\n");
+		ret = -EKEYEXPIRED;
+		goto fail;
 	}
+#endif
 
-	if (f2fs_is_mmap_file(inode) || get_dirty_pages(inode)) {
-		ret = -EBUSY;
-		goto out;
+	if (atomic_read(&(F2FS_I(inode)->appboost_abort))) {
+		atomic_set(&(F2FS_I(inode)->appboost_abort), 0);
+		ret = -EAGAIN;
+		goto fail;
 	}
 
-	if (F2FS_HAS_BLOCKS(inode)) {
-		ret = -EFBIG;
-		goto out;
+	//check head summary
+	summary = f2fs_kzalloc(sbi, sizeof(struct merge_summary), GFP_KERNEL);
+	if (!summary) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file: summary is null\n");
+		ret = -ENOMEM;
+		goto fail;
 	}
 
-	F2FS_I(inode)->i_compress_algorithm = option.algorithm;
-	F2FS_I(inode)->i_log_cluster_size = option.log_cluster_size;
-	F2FS_I(inode)->i_cluster_size = BIT(option.log_cluster_size);
-	/* Set default level */
-	if (F2FS_I(inode)->i_compress_algorithm == COMPRESS_ZSTD)
-		F2FS_I(inode)->i_compress_level = F2FS_ZSTD_DEFAULT_CLEVEL;
-	else
-		F2FS_I(inode)->i_compress_level = 0;
-	/* Adjust mount option level */
-	if (option.algorithm == F2FS_OPTION(sbi).compress_algorithm &&
-	    F2FS_OPTION(sbi).compress_level)
-		F2FS_I(inode)->i_compress_level = F2FS_OPTION(sbi).compress_level;
-	f2fs_mark_inode_dirty_sync(inode, true);
+	summary_dinfo = f2fs_kzalloc(sbi, sizeof(struct merge_summary_dinfo), GFP_KERNEL);
+	if (!summary_dinfo) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file: summary_dinfo is null\n");
+		ret = -ENOMEM;
+		goto fail;
+	}
 
-	if (!f2fs_is_compress_backend_ready(inode))
-		f2fs_warn(sbi, "compression algorithm is successfully set, "
-			"but current kernel doesn't support this algorithm.");
-out:
-	f2fs_up_write(&F2FS_I(inode)->i_sem);
-	inode_unlock(inode);
-	file_end_write(filp);
+	to_read = sizeof(struct merge_summary_dinfo);
+	if (f2fs_file_read(filp, 0, (unsigned char*)summary_dinfo, to_read) != to_read) {
+		ret = -EIO;
+		goto fail;
+	}
 
-	return ret;
-}
+	copy_summary_info_from_disk(summary, summary_dinfo);
 
-static int redirty_blocks(struct inode *inode, pgoff_t page_idx, int len)
-{
-	DEFINE_READAHEAD(ractl, NULL, NULL, inode->i_mapping, page_idx);
-	struct address_space *mapping = inode->i_mapping;
-	struct page *page;
-	pgoff_t redirty_idx = page_idx;
-	int i, page_len = 0, ret = 0;
+	checksum = summary->checksum;
+	summary->checksum = 0;
+	if (!f2fs_crc_valid(sbi, checksum, summary, sizeof(struct merge_summary))) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
 
-	page_cache_ra_unbounded(&ractl, len, 0);
+	if (summary->version != F2FS_BOOSTFILE_VERSION) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file boost file version mismatch!\n");
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
 
-	for (i = 0; i < len; i++, page_idx++) {
-		page = read_cache_page(mapping, page_idx, NULL, NULL);
-		if (IS_ERR(page)) {
-			ret = PTR_ERR(page);
-			break;
-		}
-		page_len++;
+	if (summary->state != BOOST_FILE_STATE_FINISH) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file boost file not ready!\n");
+		ret = -EKEYEXPIRED;
+		goto fail;
 	}
 
-	for (i = 0; i < page_len; i++, redirty_idx++) {
-		page = find_lock_page(mapping, redirty_idx);
+	//check tail summary
+	tail = summary->tail;
+	to_read = sizeof(struct merge_summary);
+	if (f2fs_file_read(filp, tail, (unsigned char*)summary_dinfo, to_read) != to_read) {
+		ret = -EIO;
+		goto fail;
+	}
 
-		/* It will never fail, when page has pinned above */
-		f2fs_bug_on(F2FS_I_SB(inode), !page);
+	copy_summary_info_from_disk(summary, summary_dinfo);
+	if (checksum != summary->checksum) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	summary->checksum = 0;
+	if (!f2fs_crc_valid(sbi, checksum, summary, sizeof(struct merge_summary))) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	if (summary->num > BOOST_MAX_FILES) {
+		ret = -EKEYEXPIRED;
+		goto fail;
+	}
+
+	pos += sizeof(struct merge_summary);
+	for (i = 0; i < summary->num; i++) {
+		pos += summary->fsize[i];
+	}
+
+	buf = f2fs_kvmalloc(sbi, pos - sizeof(struct merge_summary), GFP_KERNEL);
+	if (!buf) {
+		ret = -ENOMEM;
+		goto fail;
+	}
+
+	to_read = pos - sizeof(struct merge_summary_dinfo);
+	if (f2fs_file_read(filp, sizeof(struct merge_summary), buf, to_read) != to_read) {
+		f2fs_err(sbi, "f2fs_ioc_preload_file read buf failed!\n");
+                ret = -EIO;
+		goto fail;
+	}
+
+	// align the pos
+	pos = (pos + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1);
+	for (i = 0; i < summary->num; i++) {
+		u64 real_size = 0;
+		struct merge_file merge_file;
+		struct merge_file_dinfo *merge_file_dinfo = (struct merge_file_dinfo *)(buf + pos_in);
+		copy_file_merge_info_from_disk(&merge_file, merge_file_dinfo);
+
+		real_size = sizeof(struct merge_file_dinfo) +
+				   merge_file.extent_count * sizeof(struct merge_extent_dinfo);
+		if (summary->fsize[i] != real_size) {
+			ret = -EKEYEXPIRED;
+			goto fail;
+		}
+
+		pos_in += summary->fsize[i];
+		inode_source = f2fs_iget(sbi->sb, merge_file.ino);
+		if (IS_ERR(inode_source)) {
+			f2fs_err(sbi, "f2fs_ioc_preload_file no found!\n");
+			ret = -EFAULT;
+			goto fail;
+		}
+
+		if (is_bad_inode(inode_source)) {
+			ret = -EKEYEXPIRED;
+			goto fail_iput_source;
+		}
+
+		if (!S_ISREG(inode_source->i_mode)) {
+			ret = -EKEYEXPIRED;
+			goto fail_iput_source;
+		}
+
+		if (!inode_trylock(inode_source)) {
+			ret = -EAGAIN;
+			goto fail_iput_source;
+		}
+
+		if (merge_file.mtime != timespec64_to_ns(&inode_source->i_mtime)) {
+			f2fs_warn(sbi, "f2fs_ioc_preload_file file_merge has been changed! ino = %u, source_time = %llu, merge_time = %llu\n",
+								merge_file.ino, timespec64_to_ns(&inode_source->i_mtime), merge_file.mtime);
+			ret = -EKEYEXPIRED;
+			goto fail_unlock_source;
+		}
+
+		if (merge_file.i_generation != inode_source->i_generation) {
+                        f2fs_warn(sbi, "f2fs_ioc_preload_file i_generation has been changed!");
+			ret = -EKEYEXPIRED;
+			goto fail_unlock_source;
+		}
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_inode_support_dedup(sbi, inode_source) &&
+				!is_inode_flag_set(inode_source, FI_DATA_UN_MODIFY)) {
+			f2fs_err(sbi, "source file has modified\n");
+			ret = -EKEYEXPIRED;
+			goto fail_unlock_source;
+		}
+#endif
+
+		ret = fscrypt_require_key(inode_source);
+		if (ret) {
+			f2fs_warn(sbi, "f2fs_ioc_preload_file get file ino = %lu encrypt info failed\n", inode_source->i_ino);
+			goto fail_unlock_source;
+		}
+
+		for (j = 0; j < merge_file.extent_count; j++) {
+			struct merge_extent extent;
+			struct merge_extent extent_next;
+			copy_extent_info_from_disk(&extent, &(merge_file_dinfo->extents[j]));
+			if (pos >> PAGE_SHIFT != extent.index_in_mfile) {
+				f2fs_err(sbi, "f2fs_ioc_preload_file invalid index in merge file\n");
+				ret = -EKEYEXPIRED;
+				goto fail_unlock_source;
+			}
+
+			if (j < merge_file.extent_count - 1) {
+				copy_extent_info_from_disk(&extent_next, &(merge_file_dinfo->extents[j + 1]));
+				if (extent.index_in_mfile + extent.length != extent_next.index_in_mfile) {
+					f2fs_err(sbi, "f2fs_ioc_preload_file invalid extent len in merge file\n");
+					ret = -EKEYEXPIRED;
+					goto fail_unlock_source;
+				}
+			}
+
+			for (k = 0; k < extent.length; k++, pos += PAGE_SIZE) {
+				if (time_to_inject(sbi, FAULT_PAGE_ERROR)) {
+					page = NULL;
+				} else {
+					page = f2fs_pagecache_get_page(inode_source->i_mapping, extent.index + k,
+								FGP_LOCK | FGP_CREAT, GFP_NOFS);
+				}
+				if (!page) {
+					f2fs_warn(sbi, "f2fs_ioc_preload_file can not get page cache!\n");
+					ret = -ENOMEM;
+					goto fail_unlock_source;
+				}
+
+				if (PageUptodate(page)) {
+					f2fs_put_page(page, 1);
+					continue;
+				}
+				page_addr = kmap(page);
+				if (f2fs_file_read(filp, pos, page_addr, PAGE_SIZE) != PAGE_SIZE) {
+					kunmap(page);
+					flush_dcache_page(page);
+					f2fs_put_page(page, 1);
+					ret = -EIO;
+					goto fail_unlock_source;
+				}
+				kunmap(page);
+				flush_dcache_page(page);
+				SetPageUptodate(page);
+				f2fs_put_page(page, 1);
+
+				if (appboost_should_abort(inode, boost_start, interval)) {
+					ret = -EINTR;
+					f2fs_err(sbi, "f2fs_ioc_preload_file failed timeout!\n");
+					goto fail_unlock_source;
+				}
+
+				if (fatal_signal_pending(current)) {
+					ret = -EINTR;
+					goto fail_unlock_source;
+				}
+
+				if (!f2fs_appboost_enable(sbi)) {
+					ret = -ENOTTY;
+					goto fail_unlock_source;
+				}
+			}
+		}
+		inode_unlock(inode_source);
+		iput(inode_source);
+	}
+fail_unlock_source:
+	if (ret)
+		inode_unlock(inode_source);
+fail_iput_source:
+	if (ret)
+		iput(inode_source);
+fail:
+	atomic_set(&(F2FS_I(inode)->appboost_abort), 0);
+	/* invalidate boostfile clean page */
+	invalidate_mapping_pages(inode->i_mapping, 0, -1);
+	inode_unlock(inode);
+	if (buf)
+		kvfree(buf);
+	if (summary)
+		kfree(summary);
+	if (summary_dinfo)
+		kfree(summary_dinfo);
+	return ret;
+}
+#endif
 
-		f2fs_wait_on_page_writeback(page, DATA, true, true);
+static bool extra_attr_fits_in_inode(struct inode *inode, int field)
+{
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri;
 
-		set_page_dirty(page);
-		f2fs_put_page(page, 1);
-		f2fs_put_page(page, 0);
+	switch (field) {
+	case F2FS_EXTRA_ATTR_TOTAL_SIZE:
+	case F2FS_EXTRA_ATTR_ISIZE:
+	case F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE:
+		return true;
+	case F2FS_EXTRA_ATTR_PROJID:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_projid))
+			return false;
+		return true;
+	case F2FS_EXTRA_ATTR_INODE_CHKSUM:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_inode_checksum))
+			return false;
+		return true;
+	case F2FS_EXTRA_ATTR_CRTIME:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_crtime))
+			return false;
+		return true;
+	case F2FS_EXTRA_ATTR_COMPR_BLOCKS:
+	case F2FS_EXTRA_ATTR_COMPR_OPTION:
+		if (!F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_compr_blocks))
+			return false;
+		return true;
+	default:
+		f2fs_bug_on(F2FS_I_SB(inode), 1);
+		return false;
 	}
-
-	return ret;
 }
 
-static int f2fs_ioc_decompress_file(struct file *filp)
+static int f2fs_ioc_get_extra_attr(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
-	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct f2fs_inode_info *fi = F2FS_I(inode);
-	pgoff_t page_idx = 0, last_idx, cluster_idx;
-	unsigned int blk_per_seg = sbi->blocks_per_seg;
-	int ret;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_extra_attr attr;
+	u32 chksum;
+	int ret = 0;
 
-	if (!f2fs_sb_has_compression(sbi) ||
-			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
+	if (!f2fs_has_extra_attr(inode))
 		return -EOPNOTSUPP;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+		return -EFAULT;
 
-	if (!f2fs_compressed_file(inode))
+	if (attr.field >= F2FS_EXTRA_ATTR_MAX)
 		return -EINVAL;
 
-	f2fs_balance_fs(sbi, true);
-
-	file_start_write(filp);
-	inode_lock(inode);
+	if (!extra_attr_fits_in_inode(inode, attr.field))
+		return -EOPNOTSUPP;
 
-	if (!f2fs_is_compress_backend_ready(inode)) {
-		ret = -EOPNOTSUPP;
-		goto out;
+	switch (attr.field) {
+	case F2FS_EXTRA_ATTR_TOTAL_SIZE:
+		attr.attr = F2FS_TOTAL_EXTRA_ATTR_SIZE;
+		break;
+	case F2FS_EXTRA_ATTR_ISIZE:
+		attr.attr = fi->i_extra_isize;
+		break;
+	case F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE:
+		if (!f2fs_has_inline_xattr(inode))
+			return -EOPNOTSUPP;
+		attr.attr = get_inline_xattr_addrs(inode);
+		break;
+	case F2FS_EXTRA_ATTR_PROJID:
+		if (!f2fs_sb_has_project_quota(F2FS_I_SB(inode)))
+			return -EOPNOTSUPP;
+		attr.attr = from_kprojid(&init_user_ns, fi->i_projid);
+		break;
+	case F2FS_EXTRA_ATTR_INODE_CHKSUM:
+		ret = f2fs_inode_chksum_get(sbi, inode, &chksum);
+		if (ret)
+			return ret;
+		attr.attr = chksum;
+		break;
+	case F2FS_EXTRA_ATTR_CRTIME:
+		if (!f2fs_sb_has_inode_crtime(sbi))
+			return -EOPNOTSUPP;
+		if (attr.attr_size == sizeof(struct timespec64)) {
+			if (put_timespec64(&fi->i_crtime,
+					(void __user *)(uintptr_t)attr.attr))
+				return -EFAULT;
+		} else if (attr.attr_size == sizeof(struct old_timespec32)) {
+			if (put_old_timespec32(&fi->i_crtime,
+					(void __user *)(uintptr_t)attr.attr))
+				return -EFAULT;
+		} else {
+			return -EINVAL;
+		}
+		break;
+	case F2FS_EXTRA_ATTR_COMPR_BLOCKS:
+		if (attr.attr_size != sizeof(__u64))
+			return -EINVAL;
+		ret = f2fs_get_compress_blocks(inode, &attr.attr);
+		break;
+	case F2FS_EXTRA_ATTR_COMPR_OPTION:
+		/* fix coverity error: Untrusted value as argument attr.attr_size*/
+		if (attr.attr_size > sizeof(struct f2fs_comp_option_v2))
+			return -EINVAL;
+		ret = f2fs_get_compress_option_v2(filp, attr.attr,
+						  &attr.attr_size);
+		break;
+	default:
+		return -EINVAL;
 	}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
-		ret = -EINVAL;
-		goto out;
-	}
+	if (ret < 0)
+		return ret;
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret)
-		goto out;
+	if (copy_to_user((void __user *)arg, &attr, sizeof(attr)))
+		return -EFAULT;
 
-	if (!atomic_read(&fi->i_compr_blocks))
-		goto out;
+	return 0;
+}
 
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-	last_idx >>= fi->i_log_cluster_size;
+static int f2fs_ioc_set_extra_attr(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_extra_attr attr;
+	struct page *ipage;
+	void *inline_addr;
+	int ret;
 
-	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
-		page_idx = cluster_idx << fi->i_log_cluster_size;
+	if (!f2fs_has_extra_attr(inode))
+		return -EOPNOTSUPP;
 
-		if (!f2fs_is_compressed_cluster(inode, page_idx))
-			continue;
+	if (copy_from_user(&attr, (void __user *)arg, sizeof(attr)))
+		return -EFAULT;
 
-		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
-		if (ret < 0)
-			break;
+	if (attr.field >= F2FS_EXTRA_ATTR_MAX)
+		return -EINVAL;
 
-		if (get_dirty_pages(inode) >= blk_per_seg) {
-			ret = filemap_fdatawrite(inode->i_mapping);
-			if (ret < 0)
-				break;
-		}
+	if (!extra_attr_fits_in_inode(inode, attr.field))
+		return -EOPNOTSUPP;
 
-		cond_resched();
-		if (fatal_signal_pending(current)) {
-			ret = -EINTR;
-			break;
+	switch (attr.field) {
+	case F2FS_EXTRA_ATTR_TOTAL_SIZE:
+	case F2FS_EXTRA_ATTR_ISIZE:
+	case F2FS_EXTRA_ATTR_PROJID:
+	case F2FS_EXTRA_ATTR_INODE_CHKSUM:
+	case F2FS_EXTRA_ATTR_CRTIME:
+	case F2FS_EXTRA_ATTR_COMPR_BLOCKS:
+		/* read only attribtues */
+		return -EOPNOTSUPP;
+	case F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE:
+		if (!f2fs_sb_has_flexible_inline_xattr(sbi) ||
+		    !f2fs_has_inline_xattr(inode))
+			return -EOPNOTSUPP;
+		if (attr.attr < MIN_INLINE_XATTR_SIZE ||
+		    attr.attr > MAX_INLINE_XATTR_SIZE)
+			return -EINVAL;
+		inode_lock(inode);
+		f2fs_lock_op(sbi);
+		f2fs_down_write(&F2FS_I(inode)->i_xattr_sem);
+		if (i_size_read(inode) || F2FS_I(inode)->i_xattr_nid) {
+			/*
+			 * it is not allowed to set this field if the inode
+			 * has data or xattr node
+			 */
+			ret = -EFBIG;
+			goto xattr_out_unlock;
 		}
+		ipage = f2fs_get_node_page(sbi, inode->i_ino);
+		if (IS_ERR(ipage)) {
+			ret = PTR_ERR(ipage);
+			goto xattr_out_unlock;
+		}
+		inline_addr = inline_xattr_addr(inode, ipage);
+		if (!IS_XATTR_LAST_ENTRY(XATTR_FIRST_ENTRY(inline_addr))) {
+			ret = -EFBIG;
+		} else {
+			struct f2fs_xattr_header *hdr;
+			struct f2fs_xattr_entry *ent;
+
+			F2FS_I(inode)->i_inline_xattr_size = (int)attr.attr;
+			inline_addr = inline_xattr_addr(inode, ipage);
+			hdr = XATTR_HDR(inline_addr);
+			ent = XATTR_FIRST_ENTRY(inline_addr);
+			hdr->h_magic = cpu_to_le32(F2FS_XATTR_MAGIC);
+			hdr->h_refcount = cpu_to_le32(1);
+			memset(ent, 0, attr.attr - sizeof(*hdr));
+			set_page_dirty(ipage);
+			ret = 0;
+		}
+		f2fs_put_page(ipage, 1);
+xattr_out_unlock:
+		f2fs_up_write(&F2FS_I(inode)->i_xattr_sem);
+		f2fs_unlock_op(sbi);
+		inode_unlock(inode);
+		if (!ret)
+			f2fs_balance_fs(sbi, true);
+		break;
+	case F2FS_EXTRA_ATTR_COMPR_OPTION:
+		/* fix coverity error: Untrusted value as argument attr.attr_size*/
+		if (attr.attr_size > sizeof(struct f2fs_comp_option_v2))
+			return -EINVAL;
+		ret = f2fs_set_compress_option_v2(filp, attr.attr,
+						  &attr.attr_size);
+		break;
+	default:
+		return -EINVAL;
 	}
 
-	if (!ret)
-		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
-							LLONG_MAX);
-
-	if (ret)
-		f2fs_warn(sbi, "%s: The file might be partially decompressed (errno=%d). Please delete the file.",
-			  __func__, ret);
-out:
-	inode_unlock(inode);
-	file_end_write(filp);
-
 	return ret;
 }
 
-static int f2fs_ioc_compress_file(struct file *filp)
+#ifdef CONFIG_F2FS_SEQZONE
+static long f2fs_ioc_set_seqzone_file(struct file *filp, unsigned long arg)
 {
 	struct inode *inode = file_inode(filp);
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
-	struct f2fs_inode_info *fi = F2FS_I(inode);
-	pgoff_t page_idx = 0, last_idx, cluster_idx;
-	unsigned int blk_per_seg = sbi->blocks_per_seg;
-	int ret;
-
-	if (!f2fs_sb_has_compression(sbi) ||
-			F2FS_OPTION(sbi).compress_mode != COMPR_MODE_USER)
-		return -EOPNOTSUPP;
+	int ret = 0;
 
-	if (!(filp->f_mode & FMODE_WRITE))
-		return -EBADF;
+	if (!f2fs_sb_has_seqzone(sbi)) {
+		return -ENOTSUPP;
+	}
 
-	if (!f2fs_compressed_file(inode))
+	if (!S_ISREG(inode->i_mode))
 		return -EINVAL;
 
-	f2fs_balance_fs(sbi, true);
+	ret = mnt_want_write_file(filp);
+	if (ret)
+		return ret;
 
-	file_start_write(filp);
 	inode_lock(inode);
-
-	if (!f2fs_is_compress_backend_ready(inode)) {
-		ret = -EOPNOTSUPP;
+	if (!IS_ENCRYPTED(inode)) {
+		ret = -EINVAL;
 		goto out;
 	}
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+	if (F2FS_HAS_BLOCKS(inode)) {
 		ret = -EINVAL;
 		goto out;
 	}
 
-	ret = filemap_write_and_wait_range(inode->i_mapping, 0, LLONG_MAX);
-	if (ret)
+	if (is_inode_flag_set(inode, FI_COMPRESSED_FILE)) {
+		ret = -ENOTSUPP;
 		goto out;
-
-	set_inode_flag(inode, FI_ENABLE_COMPRESS);
-
-	last_idx = DIV_ROUND_UP(i_size_read(inode), PAGE_SIZE);
-	last_idx >>= fi->i_log_cluster_size;
-
-	for (cluster_idx = 0; cluster_idx < last_idx; cluster_idx++) {
-		page_idx = cluster_idx << fi->i_log_cluster_size;
-
-		if (f2fs_is_sparse_cluster(inode, page_idx))
-			continue;
-
-		ret = redirty_blocks(inode, page_idx, fi->i_cluster_size);
-		if (ret < 0)
-			break;
-
-		if (get_dirty_pages(inode) >= blk_per_seg) {
-			ret = filemap_fdatawrite(inode->i_mapping);
-			if (ret < 0)
-				break;
-		}
-
-		cond_resched();
-		if (fatal_signal_pending(current)) {
-			ret = -EINTR;
-			break;
-		}
 	}
 
-	if (!ret)
-		ret = filemap_write_and_wait_range(inode->i_mapping, 0,
-							LLONG_MAX);
-
-	clear_inode_flag(inode, FI_ENABLE_COMPRESS);
+	if (f2fs_inode_support_dedup(sbi, inode)) {
+		set_inode_flag(inode, FI_SEQZONE);
+		f2fs_mark_inode_dirty_sync(inode, true);
+	}
 
-	if (ret)
-		f2fs_warn(sbi, "%s: The file might be partially compressed (errno=%d). Please delete the file.",
-			  __func__, ret);
 out:
 	inode_unlock(inode);
-	file_end_write(filp);
-
+	mnt_drop_write_file(filp);
 	return ret;
 }
+static long f2fs_ioc_get_seqzone_file(struct file *filp, unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+
+	int seqzone = f2fs_seqzone_file(inode) ? 1 : 0;
+	if (copy_to_user((int __user*)arg, &seqzone, sizeof(seqzone))) {
+		return -1;
+	}
+	return 0;
+}
+#endif
 
 static long __f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 {
+	int ret = 0;
+
 	switch (cmd) {
 	case FS_IOC_GETVERSION:
 		return f2fs_ioc_getversion(filp, arg);
@@ -4407,11 +8155,11 @@ static long __f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 	case FS_IOC_SETFSLABEL:
 		return f2fs_ioc_setfslabel(filp, arg);
 	case F2FS_IOC_GET_COMPRESS_BLOCKS:
-		return f2fs_get_compress_blocks(filp, arg);
+		return f2fs_ioc_get_compress_blocks(filp, arg);
 	case F2FS_IOC_RELEASE_COMPRESS_BLOCKS:
 		return f2fs_release_compress_blocks(filp, arg);
 	case F2FS_IOC_RESERVE_COMPRESS_BLOCKS:
-		return f2fs_reserve_compress_blocks(filp, arg);
+		return f2fs_ioc_reserve_compress_blocks(filp, arg);
 	case F2FS_IOC_SEC_TRIM_FILE:
 		return f2fs_sec_trim_file(filp, arg);
 	case F2FS_IOC_GET_COMPRESS_OPTION:
@@ -4422,9 +8170,62 @@ static long __f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
 		return f2fs_ioc_decompress_file(filp);
 	case F2FS_IOC_COMPRESS_FILE:
 		return f2fs_ioc_compress_file(filp);
+#ifdef CONFIG_F2FS_APPBOOST
+	case F2FS_IOC_START_MERGE_FILE:
+		return f2fs_ioc_start_file_merge(filp, arg);
+	case F2FS_IOC_END_MERGE_FILE:
+		return f2fs_ioc_end_file_merge(filp, arg);
+	case F2FS_IOC_PRELOAD_FILE:
+		return f2fs_ioc_preload_file(filp, arg);
+	case F2FS_IOC_ABORT_PRELOAD_FILE:
+		return f2fs_ioc_abort_preload_file(filp, arg);
+#endif
+	case F2FS_IOC_GET_EXTRA_ATTR:
+		return f2fs_ioc_get_extra_attr(filp, arg);
+	case F2FS_IOC_SET_EXTRA_ATTR:
+		return f2fs_ioc_set_extra_attr(filp, arg);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	case F2FS_IOC_DEDUP_CREATE:
+		ret = f2fs_ioc_create_layered_inode(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_FILE:
+		ret = f2fs_ioc_dedup_file(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_REVOKE:
+		ret = f2fs_ioc_dedup_revoke(filp, arg);
+		break;
+	case F2FS_IOC_CLONE_FILE:
+		ret = f2fs_ioc_clone_file(filp, arg);
+		break;
+	case F2FS_IOC_MODIFY_CHECK:
+		ret = f2fs_ioc_modify_check(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_PERM_CHECK:
+		ret = f2fs_ioc_dedup_permission_check(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_GET_FILE_INFO:
+		ret = f2fs_ioc_get_dedupd_file_info(filp, arg);
+		break;
+	case F2FS_IOC_DEDUP_GET_SYS_INFO:
+		ret = f2fs_ioc_get_dedup_sysinfo(filp, arg);
+		break;
+	case F2FS_IOC_SNAPSHOT_CREATE:
+		ret = f2fs_ioc_create_snapshot(filp, arg);
+		break;
+	case F2FS_IOC_SNAPSHOT_PREPARE:
+		ret = f2fs_ioc_prepare_snapshot(filp, arg);
+		break;
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+	case F2FS_IOC_SET_SEQZONE_FILE:
+		return f2fs_ioc_set_seqzone_file(filp, arg);
+	case F2FS_IOC_GET_SEQZONE_FILE:
+		return f2fs_ioc_get_seqzone_file(filp, arg);
+#endif
 	default:
 		return -ENOTTY;
 	}
+	return ret;
 }
 
 long f2fs_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
@@ -4452,6 +8253,11 @@ static bool f2fs_should_use_dio(struct inode *inode, struct kiocb *iocb,
 	if (f2fs_force_buffered_io(inode, iov_iter_rw(iter)))
 		return false;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if(is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		return false;
+#endif
+
 	/*
 	 * Direct I/O not aligned to the disk's logical_block_size will be
 	 * attempted, but will fail with -EINVAL.
@@ -4544,7 +8350,12 @@ static void f2fs_trace_rw_file_path(struct kiocb *iocb, size_t count, int rw)
 	buf = f2fs_kmalloc(F2FS_I_SB(inode), PATH_MAX, GFP_KERNEL);
 	if (!buf)
 		return;
+#ifdef CONFIG_F2FS_APPBOOST
+	buf = strcpy(buf, "/data");
+	path = dentry_path_raw(file_dentry(iocb->ki_filp), buf + 5, PATH_MAX - 5);
+#else
 	path = dentry_path_raw(file_dentry(iocb->ki_filp), buf, PATH_MAX);
+#endif
 	if (IS_ERR(path))
 		goto free_buf;
 	if (rw == WRITE)
@@ -4576,6 +8387,15 @@ static ssize_t f2fs_file_read_iter(struct kiocb *iocb, struct iov_iter *to)
 	if (f2fs_should_use_dio(inode, iocb, to)) {
 		ret = f2fs_dio_read_iter(iocb, to);
 	} else {
+		struct backing_dev_info *bdi = inode_to_bdi(inode);
+		struct file *file = iocb->ki_filp;
+
+		if (!(file->f_mode & FMODE_RANDOM) &&
+		    file->f_ra.ra_pages == bdi->ra_pages &&
+		    f2fs_compressed_file(inode)) {
+			file->f_ra.ra_pages = bdi->ra_pages * MIN_RA_MUL;
+		}
+
 		ret = filemap_read(iocb, to, 0);
 		if (ret > 0)
 			f2fs_update_iostat(F2FS_I_SB(inode), inode,
@@ -4596,8 +8416,15 @@ static ssize_t f2fs_write_checks(struct kiocb *iocb, struct iov_iter *from)
 	if (IS_IMMUTABLE(inode))
 		return -EPERM;
 
-	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED))
+	if (is_inode_flag_set(inode, FI_COMPRESS_RELEASED)) {
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+		err = f2fs_reserve_compress_blocks(inode, NULL);
+		if (err < 0)
+			return err;
+#else
 		return -EPERM;
+#endif
+	}
 
 	count = generic_write_checks(iocb, from);
 	if (count <= 0)
@@ -4628,7 +8455,11 @@ static int f2fs_preallocate_blocks(struct kiocb *iocb, struct iov_iter *iter,
 	int ret;
 
 	/* If it will be an out-of-place direct write, don't bother. */
+#ifdef CONFIG_F2FS_SEQZONE
+	if (dio && (f2fs_lfs_mode(sbi) || f2fs_seqzone_file(inode)))
+#else
 	if (dio && f2fs_lfs_mode(sbi))
+#endif
 		return 0;
 	/*
 	 * Don't preallocate holes aligned to DIO_SKIP_HOLES which turns into
@@ -4873,6 +8704,22 @@ static ssize_t f2fs_file_write_iter(struct kiocb *iocb, struct iov_iter *from)
 		inode_lock(inode);
 	}
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	CLEAR_IFLAG_IF_SET(inode, F2FS_NOCOMP_FL);
+#endif
+
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (!is_inode_flag_set(inode, FI_SNAPSHOT_PREPARED))
+		mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		ret = f2fs_revoke_deduped_inode(inode, __func__);
+		if (ret) {
+			inode_unlock(inode);
+			goto out;
+		}
+	}
+#endif
+
 	ret = f2fs_write_checks(iocb, from);
 	if (ret <= 0)
 		goto out_unlock;
@@ -4949,6 +8796,9 @@ static int f2fs_file_fadvise(struct file *filp, loff_t offset, loff_t len,
 		bdi = inode_to_bdi(mapping->host);
 		filp->f_ra.ra_pages = bdi->ra_pages *
 			F2FS_I_SB(inode)->seq_file_ra_mul;
+		if (f2fs_compressed_file(inode) &&
+		    filp->f_ra.ra_pages < (bdi->ra_pages * COMPR_RA_MUL))
+			filp->f_ra.ra_pages = bdi->ra_pages * COMPR_RA_MUL;
 		spin_lock(&filp->f_lock);
 		filp->f_mode &= ~FMODE_RANDOM;
 		spin_unlock(&filp->f_lock);
@@ -5069,6 +8919,30 @@ long f2fs_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
 	case F2FS_IOC_SET_COMPRESS_OPTION:
 	case F2FS_IOC_DECOMPRESS_FILE:
 	case F2FS_IOC_COMPRESS_FILE:
+#ifdef CONFIG_F2FS_APPBOOST
+	case F2FS_IOC_START_MERGE_FILE:
+	case F2FS_IOC_END_MERGE_FILE:
+	case F2FS_IOC_PRELOAD_FILE:
+	case F2FS_IOC_ABORT_PRELOAD_FILE:
+#endif
+#ifdef CONFIG_F2FS_FS_DEDUP
+	case F2FS_IOC_DEDUP_CREATE:
+	case F2FS_IOC_DEDUP_FILE:
+	case F2FS_IOC_DEDUP_REVOKE:
+	case F2FS_IOC_CLONE_FILE:
+	case F2FS_IOC_MODIFY_CHECK:
+	case F2FS_IOC_DEDUP_PERM_CHECK:
+	case F2FS_IOC_DEDUP_GET_FILE_INFO:
+	case F2FS_IOC_DEDUP_GET_SYS_INFO:
+	case F2FS_IOC_SNAPSHOT_CREATE:
+	case F2FS_IOC_SNAPSHOT_PREPARE:
+#endif
+	case F2FS_IOC_GET_EXTRA_ATTR:
+	case F2FS_IOC_SET_EXTRA_ATTR:
+#ifdef CONFIG_F2FS_SEQZONE
+	case F2FS_IOC_SET_SEQZONE_FILE:
+	case F2FS_IOC_GET_SEQZONE_FILE:
+#endif
 		break;
 	default:
 		return -ENOIOCTLCMD;
diff --git a/fs/f2fs/gc.c b/fs/f2fs/gc.c
old mode 100644
new mode 100755
index a23340cd1..36d1d713b
--- a/fs/f2fs/gc.c
+++ b/fs/f2fs/gc.c
@@ -1367,7 +1367,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 
 	/* allocate block address */
 	err = f2fs_allocate_data_block(fio.sbi, NULL, fio.old_blkaddr, &newaddr,
-				&sum, type, NULL);
+				&sum, type, NULL, 0);
 	if (err) {
 		f2fs_put_page(mpage, 1);
 		/* filesystem should shutdown, no need to recovery block */
@@ -1388,7 +1388,7 @@ static int move_data_block(struct inode *inode, block_t bidx,
 				page_address(mpage), PAGE_SIZE);
 	f2fs_put_page(mpage, 1);
 
-	f2fs_invalidate_internal_cache(fio.sbi, fio.old_blkaddr, 1);
+	f2fs_invalidate_internal_cache(fio.sbi, fio.old_blkaddr);
 
 	set_page_dirty(fio.encrypted_page);
 	if (clear_page_dirty_for_io(fio.encrypted_page))
diff --git a/fs/f2fs/inode.c b/fs/f2fs/inode.c
index f143b0c41..7cf41bef8 100644
--- a/fs/f2fs/inode.c
+++ b/fs/f2fs/inode.c
@@ -190,6 +190,27 @@ void f2fs_inode_chksum_set(struct f2fs_sb_info *sbi, struct page *page)
 	ri->i_inode_checksum = cpu_to_le32(f2fs_inode_chksum(sbi, page));
 }
 
+int f2fs_inode_chksum_get(struct f2fs_sb_info *sbi,
+			  struct inode *inode, u32 *chksum)
+{
+	struct page *ipage;
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct f2fs_inode *ri;
+
+	if (!f2fs_sb_has_inode_chksum(sbi) ||
+	    !f2fs_has_extra_attr(inode) ||
+	    !F2FS_FITS_IN_INODE(ri, fi->i_extra_isize, i_inode_checksum))
+		return -EOPNOTSUPP;
+
+	ipage = f2fs_get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(ipage))
+		return PTR_ERR(ipage);
+
+	*chksum = f2fs_inode_chksum(sbi, ipage);
+	f2fs_put_page(ipage, true);
+	return 0;
+}
+
 static bool sanity_check_inode(struct inode *inode, struct page *node_page)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -322,6 +343,44 @@ static void init_idisk_time(struct inode *inode)
 	fi->i_disk_time[2] = inode->i_mtime;
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+/* should init dedup flags before */
+static int init_inner_inode(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+	struct page *node_page = NULL;
+	struct f2fs_inode *ri = NULL;
+	struct inode *inner = NULL;
+	nid_t inner_ino;
+
+	if (!f2fs_is_outer_inode(inode))
+		return 0;
+
+	if (time_to_inject(sbi, FAULT_DEDUP_INIT_INNER))
+		return -EIO;
+
+	node_page = f2fs_get_node_page(sbi, inode->i_ino);
+	if (IS_ERR(node_page))
+		return PTR_ERR(node_page);
+
+	ri = F2FS_INODE(node_page);
+	inner_ino = le32_to_cpu(ri->i_inner_ino);
+
+	inner = f2fs_iget(sbi->sb, inner_ino);
+	if (unlikely(IS_ERR(inner))) {
+		f2fs_err(sbi, "inode[%lu] iget inner ino[%u] fail",
+				inode->i_ino, inner_ino);
+		f2fs_put_page(node_page, 1);
+		return PTR_ERR(inner);
+	}
+
+	fi->inner_inode = inner;
+	f2fs_put_page(node_page, 1);
+	return 0;
+}
+#endif
+
 static int do_read_inode(struct inode *inode)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
@@ -440,14 +499,19 @@ static int do_read_inode(struct inode *inode)
 			fi->i_log_cluster_size = ri->i_log_cluster_size;
 			compress_flag = le16_to_cpu(ri->i_compress_flag);
 			fi->i_compress_level = compress_flag >>
-						COMPRESS_LEVEL_OFFSET;
+						COMPRESS_LEVEL;
 			fi->i_compress_flag = compress_flag &
-					GENMASK(COMPRESS_LEVEL_OFFSET - 1, 0);
+					GENMASK(COMPRESS_LEVEL - 1, 0);
 			fi->i_cluster_size = BIT(fi->i_log_cluster_size);
 			set_inode_flag(inode, FI_COMPRESSED_FILE);
 		}
 	}
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_inode_support_dedup(sbi, inode))
+		get_dedup_flags_info(inode, ri);
+#endif
+
 	init_idisk_time(inode);
 
 	/* Need all the flag bits */
@@ -460,6 +524,9 @@ static int do_read_inode(struct inode *inode)
 		return -EFSCORRUPTED;
 	}
 
+#ifdef CONFIG_F2FS_APPBOOST
+	atomic_set(&F2FS_I(inode)->appboost_abort, 0);
+#endif
 	f2fs_put_page(node_page, 1);
 
 	stat_inc_inline_xattr(inode);
@@ -550,6 +617,11 @@ struct inode *f2fs_iget(struct super_block *sb, unsigned long ino)
 		ret = -EIO;
 		goto bad_inode;
 	}
+#ifdef CONFIG_F2FS_FS_DEDUP
+	ret = init_inner_inode(inode);
+	if (ret)
+		goto bad_inode;
+#endif
 	f2fs_set_inode_flags(inode);
 
 	unlock_new_inode(inode);
@@ -581,6 +653,9 @@ void f2fs_update_inode(struct inode *inode, struct page *node_page)
 {
 	struct f2fs_inode *ri;
 	struct extent_tree *et = F2FS_I(inode)->extent_tree[EX_READ];
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct inode *inner;
+#endif
 
 	f2fs_wait_on_page_writeback(node_page, NODE, true, true);
 	set_page_dirty(node_page);
@@ -665,11 +740,24 @@ void f2fs_update_inode(struct inode *inode, struct page *node_page)
 				F2FS_I(inode)->i_compress_algorithm;
 			compress_flag = F2FS_I(inode)->i_compress_flag |
 				F2FS_I(inode)->i_compress_level <<
-						COMPRESS_LEVEL_OFFSET;
+						COMPRESS_LEVEL;
 			ri->i_compress_flag = cpu_to_le16(compress_flag);
 			ri->i_log_cluster_size =
 				F2FS_I(inode)->i_log_cluster_size;
 		}
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_sb_has_dedup(F2FS_I_SB(inode)) &&
+			F2FS_FITS_IN_INODE(ri, F2FS_I(inode)->i_extra_isize,
+				i_dedup_flags)) {
+			set_raw_dedup_flags(inode, ri);
+
+			inner = F2FS_I(inode)->inner_inode;
+			if (inner)
+				ri->i_inner_ino = cpu_to_le32(inner->i_ino);
+			else
+				ri->i_inner_ino = 0;
+		}
+#endif
 	}
 
 	__set_inode_rdev(inode, node_page);
@@ -735,6 +823,33 @@ int f2fs_write_inode(struct inode *inode, struct writeback_control *wbc)
 	return 0;
 }
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+static void f2fs_dec_inner_link(struct inode *inode)
+{
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	struct inode *inner = NULL;
+	int err = 0;
+
+	inner = get_inner_inode(inode);
+	if (!inner)
+		return;
+
+	if (time_to_inject(sbi, FAULT_DEDUP_ORPHAN_INODE))
+		err = -ENOSPC;
+	else
+		err = f2fs_acquire_orphan_inode(sbi);
+	if (err) {
+		set_sbi_flag(sbi, SBI_NEED_FSCK);
+		put_inner_inode(inner);
+		return;
+	}
+	f2fs_drop_deduped_link(inner);
+
+	trace_f2fs_dedup_dec_inner_link(inode, inner);
+	put_inner_inode(inner);
+}
+#endif
+
 /*
  * Called at the last iput() if i_nlink is zero
  */
@@ -799,6 +914,10 @@ void f2fs_evict_inode(struct inode *inode)
 		f2fs_lock_op(sbi);
 		err = f2fs_remove_inode_page(inode);
 		f2fs_unlock_op(sbi);
+#ifdef CONFIG_F2FS_FS_DEDUP
+		if (f2fs_is_outer_inode(inode))
+			f2fs_dec_inner_link(inode);
+#endif
 		if (err == -ENOENT) {
 			err = 0;
 
@@ -831,6 +950,10 @@ void f2fs_evict_inode(struct inode *inode)
 	if (!is_sbi_flag_set(sbi, SBI_IS_FREEZING))
 		sb_end_intwrite(inode->i_sb);
 no_delete:
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (F2FS_I(inode)->inner_inode)
+		iput(F2FS_I(inode)->inner_inode);
+#endif
 	dquot_drop(inode);
 
 	stat_dec_inline_xattr(inode);
diff --git a/fs/f2fs/lz4armv8/lz4accel.c b/fs/f2fs/lz4armv8/lz4accel.c
new file mode 100644
index 000000000..dae52ab3b
--- /dev/null
+++ b/fs/f2fs/lz4armv8/lz4accel.c
@@ -0,0 +1,49 @@
+#include "lz4accel.h"
+#include <asm/cputype.h>
+
+#ifdef CONFIG_CFI_CLANG
+static inline int
+__cfi_lz4_decompress_asm(uint8_t **dst_ptr, uint8_t *dst_begin,
+			 uint8_t *dst_end, const uint8_t **src_ptr,
+			 const uint8_t *src_end, bool dip)
+{
+	return _lz4_decompress_asm(dst_ptr, dst_begin, dst_end,
+				   src_ptr, src_end, dip);
+}
+
+static inline int
+__cfi_lz4_decompress_asm_noprfm(uint8_t **dst_ptr, uint8_t *dst_begin,
+				uint8_t *dst_end, const uint8_t **src_ptr,
+				const uint8_t *src_end, bool dip)
+{
+	return _lz4_decompress_asm_noprfm(dst_ptr, dst_begin, dst_end,
+					  src_ptr, src_end, dip);
+}
+
+#define _lz4_decompress_asm		__cfi_lz4_decompress_asm
+#define _lz4_decompress_asm_noprfm	__cfi_lz4_decompress_asm_noprfm
+#endif
+
+int lz4_decompress_asm_select(uint8_t **dst_ptr, uint8_t *dst_begin,
+			      uint8_t *dst_end, const uint8_t **src_ptr,
+			      const uint8_t *src_end, bool dip) {
+	const unsigned i = smp_processor_id();
+
+	switch(read_cpuid_part_number()) {
+	case ARM_CPU_PART_CORTEX_A53:
+		lz4_decompress_asm_fn[i] = _lz4_decompress_asm_noprfm;
+		return _lz4_decompress_asm_noprfm(dst_ptr, dst_begin, dst_end,
+						  src_ptr, src_end, dip);
+	}
+	lz4_decompress_asm_fn[i] = _lz4_decompress_asm;
+	return _lz4_decompress_asm(dst_ptr, dst_begin, dst_end,
+				   src_ptr, src_end, dip);
+}
+
+int (*lz4_decompress_asm_fn[NR_CPUS])(uint8_t **dst_ptr, uint8_t *dst_begin,
+	uint8_t *dst_end, const uint8_t **src_ptr,
+	const uint8_t *src_end, bool dip)
+__read_mostly = {
+	[0 ... NR_CPUS-1]  = lz4_decompress_asm_select,
+};
+
diff --git a/fs/f2fs/lz4armv8/lz4accel.h b/fs/f2fs/lz4armv8/lz4accel.h
new file mode 100644
index 000000000..a8862c44a
--- /dev/null
+++ b/fs/f2fs/lz4armv8/lz4accel.h
@@ -0,0 +1,56 @@
+#include <linux/types.h>
+#include <asm/simd.h>
+
+#define LZ4_FAST_MARGIN                (128)
+
+#if defined(CONFIG_ARM64) && defined(CONFIG_KERNEL_MODE_NEON)
+#include <asm/neon.h>
+#include <asm/cputype.h>
+
+asmlinkage int _lz4_decompress_asm(uint8_t **dst_ptr, uint8_t *dst_begin,
+				   uint8_t *dst_end, const uint8_t **src_ptr,
+				   const uint8_t *src_end, bool dip);
+
+asmlinkage int _lz4_decompress_asm_noprfm(uint8_t **dst_ptr, uint8_t *dst_begin,
+					  uint8_t *dst_end, const uint8_t **src_ptr,
+					  const uint8_t *src_end, bool dip);
+
+static inline int lz4_decompress_accel_enable(void)
+{
+	return	may_use_simd();
+}
+
+extern int (*lz4_decompress_asm_fn[])(uint8_t **dst_ptr, uint8_t *dst_begin,
+	uint8_t *dst_end, const uint8_t **src_ptr,
+	const uint8_t *src_end, bool dip);
+
+static inline ssize_t lz4_decompress_asm(
+	uint8_t **dst_ptr, uint8_t *dst_begin, uint8_t *dst_end,
+	const uint8_t **src_ptr, const uint8_t *src_end, bool dip)
+{
+	int ret;
+
+	kernel_neon_begin();
+	ret = lz4_decompress_asm_fn[smp_processor_id()](dst_ptr, dst_begin,
+							dst_end, src_ptr,
+							src_end, dip);
+	kernel_neon_end();
+	return (ssize_t)ret;
+}
+
+#define __ARCH_HAS_LZ4_ACCELERATOR
+
+#else
+
+static inline int lz4_decompress_accel_enable(void)
+{
+	return	0;
+}
+
+static inline ssize_t lz4_decompress_asm(
+	uint8_t **dst_ptr, uint8_t *dst_begin, uint8_t *dst_end,
+	const uint8_t **src_ptr, const uint8_t *src_end, bool dip)
+{
+	return 0;
+}
+#endif
diff --git a/fs/f2fs/lz4armv8/lz4armv8.S b/fs/f2fs/lz4armv8/lz4armv8.S
new file mode 100644
index 000000000..b9e6e0f71
--- /dev/null
+++ b/fs/f2fs/lz4armv8/lz4armv8.S
@@ -0,0 +1,298 @@
+#include <linux/linkage.h>
+#include <asm/assembler.h>
+
+/**
+ * @para:
+ * x0 = current destination address ptr
+ * x1 = destination start position
+ * x2 = destination end position
+ * x3 = current source address ptr
+ * x4 = source end position
+ * x5 = flag for DIP
+ * @ret:
+ * 0 on success, -1 on failure
+ */
+
+/* caller-saved temporary registers */
+#define match_length		x9
+#define literal_length		x10
+#define copy_from_ptr		x11	/* copy source ptr*/
+#define copy_to_ptr		x12	/* copy destination ptr*/
+#define w_tmp			w13	/* temp var */
+#define tmp			x13
+#define w_offset		w14
+#define offset			x14
+#define permtable_addr		x15
+
+/* callee-saved registers */
+#define cplen_table_addr	x19
+#define save_dst		x20
+#define save_src		x21
+#define offset_src_ptr		x22
+#define w_tmp_match_length	w23
+#define tmp_match_length	x23
+
+
+/* x3 >= x4 src overflow */
+.macro check_src_overflow
+	cmp	x3, x4
+	b.hs	Done
+.endm
+
+.macro check_src_overflow1
+	cmp	x3, x4
+	b.hs	Done1
+.endm
+/* x0 >= x2 dst overflow */
+.macro check_dst_overflow
+	cmp	x0, x2
+	b.hs	Done
+.endm
+
+.macro check_dst_overflow1
+	cmp	x0, x2
+	b.hs	Done1
+.endm
+
+.altmacro
+.macro lz4_decompress_asm_generic	doprfm=1
+	stp     x29, x30, [sp, #-16]!
+	mov     x29, sp
+	stp	x19, x20, [sp, #-16]!
+	stp	x21, x22, [sp, #-16]!
+	stp	x23, x24, [sp, #-16]!
+	stp	x3, x0, [sp, #-16]!		/* push src and dst in stack */
+	ldr	x3, [x3]				/* x3 = *src_ptr */
+	ldr	x0, [x0]				/* x0 = *dst_ptr */
+	adr_l	permtable_addr, Permtable
+	adr_l	cplen_table_addr, Copylength_table
+
+1:
+	/*
+	 * Lz4_decompress_begin:
+	 * save current dst and src ,ensure when return from asm routine
+	 * current both of "dst" and "src" save good position.
+	 */
+	mov	save_dst, x0
+	mov	save_src, x3
+
+	check_dst_overflow
+	check_src_overflow
+
+.if \doprfm
+	add tmp, x0, #512
+	cmp x2, tmp
+	b.ls 2f
+	prfm pstl2strm,[x0,#512]
+.endif
+
+	/* Decode Token Byte: */
+2:
+	/* Decode_token:*/
+	ldrb	w_tmp, [x3], #1		/* read Token Byte */
+	lsr	literal_length, tmp, #4	/* get literal_length */
+	and	tmp_match_length, tmp, #0xf	/* get match_length */
+	add	match_length, tmp_match_length, #4	/* match_length >=4 */
+
+	/*
+	 * literal_length <= 14 : no more literal length byte,fllowing zero
+	 * or more bytes are liteal bytes.
+	 */
+	cmp	literal_length, #14
+	b.ls	6f
+
+	/*
+	 * literal_length == 15 : more literal length bytes after TokenByte.
+	 * continue decoding more literal length bytes.
+	 */
+3:
+	/* Get_literal_length: */
+	check_src_overflow
+	ldrb	w_tmp, [x3], #1
+	add	literal_length, literal_length, tmp
+	cmp	tmp, #255
+	b.eq	3b
+
+/* literal copy */
+4:
+	/* Copy_long_literal_hs_15: */
+	mov	copy_from_ptr, x3
+	mov	copy_to_ptr, x0
+	add	x3, x3, literal_length
+	add	x0, x0, literal_length
+	check_dst_overflow
+	check_src_overflow
+5:
+	/* Copy_long_literal_loop: */
+	ldr	q0, [copy_from_ptr], #16
+	str	q0, [copy_to_ptr], #16
+
+	cmp	x0, copy_to_ptr
+	b.ls	7f
+	b	5b
+
+6:
+	/* Copy_literal_lt_15: */
+	ldr q0, [x3]
+	str q0, [x0]
+	add	x3, x3, literal_length
+	add	x0, x0, literal_length
+
+	/* Decode offset and match_length */
+	/* Decode_offset_matchlength: */
+7:
+	mov	offset_src_ptr, x3
+	ldrh	w_offset, [x3], #2	/* 2Byte:offset bytes */
+	cbz	offset, Failed			/* match_length == 0 is invalid */
+	sub	copy_from_ptr, x0, offset
+	cmp	copy_from_ptr, x1
+	b.lo	Failed
+	mov	copy_to_ptr, x0
+	/*
+	 * set x0 to the end of "match copy";
+	 */
+	add	x0, x0, match_length
+	cmp	match_length, #19
+	b.lo	9f
+	/*
+	 * continue decoding more match length bytes.
+	 */
+8:
+	/* Get_long_matchlength: */
+	check_src_overflow1
+	ldrb	w_tmp, [x3], #1
+	add	x0, x0, tmp
+	add	match_length, match_length, tmp
+	cmp	tmp, #255
+	b.eq	8b
+
+	/*
+	 * here got the matchlength,start "match copy".
+	 */
+9:
+	/* Copy_match_begin: */
+	check_dst_overflow1
+	cmp	offset , match_length
+	b.hs	14f
+
+10:
+	/* Cond_offset_lt_matchlength: */
+	cmp	offset , #32
+	b.hs	14f
+
+11:
+	/* Copy_offset_lt_32: */
+	ldr	q1, [copy_from_ptr]
+	add	tmp, permtable_addr, offset, lsl #5
+	ldp	q2, q3, [tmp]
+	tbl	v0.16b, {v1.16b}, v2.16b
+	tbl	v1.16b, {v1.16b}, v3.16b
+	cmp     offset , #16
+	b.lo    12f
+	ldp     q0, q1, [copy_from_ptr]
+12:
+	/* Copy_match_perm: */
+	ldrb	w_tmp, [cplen_table_addr, offset]
+	stp	q0, q1, [copy_to_ptr]
+	add	copy_to_ptr, copy_to_ptr, tmp
+	cmp	x0, copy_to_ptr
+	b.ls	1b
+13:
+	/* Copy_offset_lt_32_loop: */
+	stp	q0, q1, [copy_to_ptr]
+	add	copy_to_ptr, copy_to_ptr, tmp
+	stp	q0, q1, [copy_to_ptr]
+	add	copy_to_ptr, copy_to_ptr, tmp
+	cmp	x0, copy_to_ptr
+	b.hi	13b
+	b	1b
+
+/* offset >= match */
+14:
+	/* Cond_offset_ge_matchlength: */
+	ldr	q0, [copy_from_ptr], #16
+	str	q0, [copy_to_ptr], #16
+
+	cmp	x0, copy_to_ptr
+	b.ls	1b
+15:
+	/* Copy_offset_ge_match_loop: */
+	ldp	q0, q1, [copy_from_ptr], #32
+	stp	q0, q1, [copy_to_ptr], #32
+
+	cmp	x0, copy_to_ptr
+	b.hi	15b
+	b	1b
+.endm
+
+.text
+.p2align 4
+
+SYM_FUNC_START(_lz4_decompress_asm)
+	lz4_decompress_asm_generic
+SYM_FUNC_END(_lz4_decompress_asm)
+
+SYM_INNER_LABEL(Failed, SYM_L_LOCAL)
+	mov	tmp, #-1
+	b	Exit_here
+
+Done1:
+	cbz	x5, Done
+	sub	save_src, offset_src_ptr, #1
+	strb	w_tmp_match_length, [save_src]
+	add	save_dst,save_dst,literal_length
+Done:
+	mov	tmp, #0
+
+Exit_here:
+	ldp	x3, x0, [sp], #16
+	str	save_src, [x3]
+	str	save_dst, [x0]
+	ldp	x23, x24, [sp], #16
+	ldp	x21, x22, [sp], #16
+	ldp	x19, x20, [sp], #16
+	mov	x0, tmp
+	ldp     x29, x30, [sp], #16
+	ret     x30
+
+/*
+ * In case of offset <= 31 < matchlength ,expand the pattern and store in
+ * repeating pattern size(RPS),store the RPS in Copylength_table.
+ * case 1): 1 <= offset <= 15
+ * expand the pattern according to the Permtable and store their repeating pattern in q0 q1;
+ * RPS = 32 - (32 % offset) offset <= 31
+ * case 2): offset >= 16
+ * read the pattern and store in q0 q1.
+ * RPS = offset.
+ */
+.pushsection	".rodata", "a"
+.p2align 8
+Permtable:
+.byte 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0  //offset = 0
+.byte 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0  //offset = 1
+.byte 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1  //offset = 2
+.byte 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1  //offset = 3
+.byte 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3  //offset = 4
+.byte 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1  //offset = 5
+.byte 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1  //offset = 6
+.byte 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3  //offset = 7
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7  //offset = 8
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4  //offset = 9
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1  //offset = 10
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9  //offset = 11
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11, 0, 1, 2, 3, 4, 5, 6, 7  //offset = 12
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12, 0, 1, 2, 3, 4, 5  //offset = 13
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13, 0, 1, 2, 3  //offset = 14
+.byte 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9,10,11,12,13,14, 0, 1  //offset = 15
+
+.p2align 8
+Copylength_table:
+.byte 32,32,32,30,32,30,30,28,32,27,30,22,24,26,28,30  // 0  .. 15
+.byte 16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31  // 16 .. 31
+.popsection
+
+.text
+.p2align 4
+SYM_FUNC_START(_lz4_decompress_asm_noprfm)
+	lz4_decompress_asm_generic	0
+SYM_FUNC_END(_lz4_decompress_asm_noprfm)
diff --git a/fs/f2fs/namei.c b/fs/f2fs/namei.c
old mode 100644
new mode 100755
index 194493206..00df3cbff
--- a/fs/f2fs/namei.c
+++ b/fs/f2fs/namei.c
@@ -23,7 +23,7 @@
 #include <trace/events/f2fs.h>
 
 static inline bool is_extension_exist(const unsigned char *s, const char *sub,
-						bool tmp_ext)
+						bool tmp_ext, bool tmp_dot)
 {
 	size_t slen = strlen(s);
 	size_t sublen = strlen(sub);
@@ -49,13 +49,27 @@ static inline bool is_extension_exist(const unsigned char *s, const char *sub,
 	for (i = 1; i < slen - sublen; i++) {
 		if (s[i] != '.')
 			continue;
-		if (!strncasecmp(s + i + 1, sub, sublen))
-			return true;
+		if (!strncasecmp(s + i + 1, sub, sublen)) {
+			if (!tmp_dot)
+				return true;
+			if (i == slen - sublen - 1 || s[i + 1 + sublen] == '.')
+				return true;
+		}
 	}
 
 	return false;
 }
 
+static inline bool is_temperature_extension(const unsigned char *s, const char *sub)
+{
+	return is_extension_exist(s, sub, true, false);
+}
+
+static inline bool is_compress_extension(const unsigned char *s, const char *sub)
+{
+	return is_extension_exist(s, sub, true, true);
+}
+
 int f2fs_update_extension_list(struct f2fs_sb_info *sbi, const char *name,
 							bool hot, bool set)
 {
@@ -148,7 +162,7 @@ static void set_compress_new_inode(struct f2fs_sb_info *sbi, struct inode *dir,
 	cold_count = le32_to_cpu(sbi->raw_super->extension_count);
 	hot_count = sbi->raw_super->hot_ext_count;
 	for (i = cold_count; i < cold_count + hot_count; i++)
-		if (is_extension_exist(name, extlist[i], false))
+		if (is_temperature_extension(name, extlist[i]))
 			break;
 	f2fs_up_read(&sbi->sb_lock);
 	if (i < (cold_count + hot_count))
@@ -156,12 +170,12 @@ static void set_compress_new_inode(struct f2fs_sb_info *sbi, struct inode *dir,
 
 	/* Don't compress unallowed extension. */
 	for (i = 0; i < noext_cnt; i++)
-		if (is_extension_exist(name, noext[i], false))
+		if (is_compress_extension(name, noext[i]))
 			return;
 
 	/* Compress wanting extension. */
 	for (i = 0; i < ext_cnt; i++) {
-		if (is_extension_exist(name, ext[i], false)) {
+		if (is_compress_extension(name, ext[i])) {
 			set_compress_context(inode);
 			return;
 		}
@@ -189,17 +203,33 @@ static void set_file_temperature(struct f2fs_sb_info *sbi, struct inode *inode,
 	cold_count = le32_to_cpu(sbi->raw_super->extension_count);
 	hot_count = sbi->raw_super->hot_ext_count;
 	for (i = 0; i < cold_count + hot_count; i++)
-		if (is_extension_exist(name, extlist[i], true))
+		if (is_temperature_extension(name, extlist[i]))
 			break;
 	f2fs_up_read(&sbi->sb_lock);
 
 	if (i == cold_count + hot_count)
 		return;
 
-	if (i < cold_count)
+	if (i < cold_count) {
 		file_set_cold(inode);
-	else
+		/* enable seqzone global, except cold files */
+#ifdef CONFIG_F2FS_SEQZONE
+		if (IS_ENCRYPTED(inode) && f2fs_sb_has_seqzone(sbi)) {
+			if (f2fs_inode_support_dedup(sbi, inode))
+				clear_inode_flag(inode, FI_SEQZONE);
+		}
+#endif
+	} else {
 		file_set_hot(inode);
+		/* disable seqzone global, still enable hot db files */
+#ifdef CONFIG_F2FS_SEQZONE
+		if (IS_ENCRYPTED(inode) && sbi->seq_zone == ENABLE_SEQZONE_HOT_FILES &&
+			!f2fs_compressed_file(inode) && f2fs_sb_has_seqzone(sbi)) {
+			if (f2fs_inode_support_dedup(sbi, inode))
+				set_inode_flag(inode, FI_SEQZONE);
+		}
+#endif
+	}
 }
 
 static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
@@ -264,7 +294,10 @@ static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
 
 	if (f2fs_sb_has_extra_attr(sbi)) {
 		set_inode_flag(inode, FI_EXTRA_ATTR);
-		F2FS_I(inode)->i_extra_isize = F2FS_TOTAL_EXTRA_ATTR_SIZE;
+		if (sbi->oplus_feats & OPLUS_FEAT_DEDUP)
+			F2FS_I(inode)->i_extra_isize = F2FS_TOTAL_EXTRA_ATTR_SIZE;
+		else
+			F2FS_I(inode)->i_extra_isize = F2FS_LEGACY_EXTRA_ATTR_SIZE;
 	}
 
 	if (test_opt(sbi, INLINE_XATTR))
@@ -296,6 +329,16 @@ static struct inode *f2fs_new_inode(struct user_namespace *mnt_userns,
 	/* Check compression first. */
 	set_compress_new_inode(sbi, dir, inode, name);
 
+#ifdef CONFIG_F2FS_SEQZONE
+	/* enable seqzone global */
+	if (f2fs_sb_has_seqzone(sbi) && sbi->seq_zone == ENABLE_SEQZONE_EXCEPT_COLD
+			&& S_ISREG(inode->i_mode) &&
+			IS_ENCRYPTED(inode) && !f2fs_compressed_file(inode)) {
+		if (f2fs_inode_support_dedup(sbi, inode))
+			set_inode_flag(inode, FI_SEQZONE);
+	}
+#endif
+
 	/* Should enable inline_data after compression set */
 	if (test_opt(sbi, INLINE_DATA) && f2fs_may_inline_data(inode))
 		set_inode_flag(inode, FI_INLINE_DATA);
@@ -568,6 +611,7 @@ static int f2fs_unlink(struct inode *dir, struct dentry *dentry)
 	if (IS_CASEFOLDED(dir))
 		d_invalidate(dentry);
 #endif
+
 	if (IS_DIRSYNC(dir))
 		f2fs_sync_fs(sbi->sb, 1);
 fail:
@@ -1023,7 +1067,7 @@ static int f2fs_rename(struct user_namespace *mnt_userns, struct inode *old_dir,
 	}
 
 	if (old_dir_entry) {
-		if (old_dir != new_dir)
+		if (old_dir != new_dir && !whiteout)
 			f2fs_set_link(old_inode, old_dir_entry,
 						old_dir_page, new_dir);
 		else
@@ -1322,3 +1366,36 @@ const struct inode_operations f2fs_special_inode_operations = {
 	.set_acl	= f2fs_set_acl,
 	.listxattr	= f2fs_listxattr,
 };
+
+void f2fs_update_atime(struct inode *inode, bool oneshot)
+{
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	struct f2fs_inode_info *fi = F2FS_I(inode);
+
+	if (!f2fs_compressed_file(inode))
+		return;
+
+	if (!sb_start_write_trylock(inode->i_sb))
+		return;
+
+	if (!inode_trylock(inode))
+		goto out_unlock_sb;
+
+	if (IS_RDONLY(inode))
+		goto out_unlock_inode;
+
+	if (fi->i_compress_flag & COMPRESS_ATIME_MASK) {
+		struct timespec64 now;
+
+		if (oneshot)
+			fi->i_compress_flag &= ~COMPRESS_ATIME_MASK;
+		now = current_time(inode);
+		inode_update_time(inode, &now, S_ATIME);
+	}
+
+out_unlock_inode:
+	inode_unlock(inode);
+out_unlock_sb:
+	sb_end_write(inode->i_sb);
+#endif
+}
diff --git a/fs/f2fs/node.c b/fs/f2fs/node.c
old mode 100644
new mode 100755
index 06ec3ffa1..89af08bdc
--- a/fs/f2fs/node.c
+++ b/fs/f2fs/node.c
@@ -753,6 +753,38 @@ static int get_node_path(struct inode *inode, long block,
 	return level;
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+u32 seqzone_index(struct inode *inode,
+			struct page *node_page, unsigned int offset)
+{
+	struct f2fs_node *raw_node;
+	__le32 *addr_array;
+	int base = 0;
+	bool is_inode = IS_INODE(node_page);
+	int addrs = DEF_ADDRS_PER_BLOCK / 2;
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+	raw_node = F2FS_NODE(node_page);
+
+	if (is_inode) {
+		int xattr_addrs = 0;
+		if (f2fs_sb_has_flexible_inline_xattr(sbi))
+			xattr_addrs = le16_to_cpu(raw_node->i.i_inline_xattr_size);
+		else if (raw_node->i.i_inline & F2FS_INLINE_XATTR)
+			xattr_addrs = DEFAULT_INLINE_XATTR_ADDRS;
+
+		if (!inode)
+			/* from GC path only */
+			base = offset_in_addr(&raw_node->i);
+		else if (f2fs_has_extra_attr(inode))
+			base = get_extra_isize(inode);
+		addrs = (DEF_ADDRS_PER_INODE - base - xattr_addrs) / 2;
+	}
+
+	addr_array = blkaddr_in_node(raw_node);
+	return le32_to_cpu(addr_array[base + offset + addrs]);
+}
+#endif
+
 /*
  * Caller should call f2fs_put_dnode(dn).
  * Also, it should grab and release a rwsem by calling f2fs_lock_op() and
@@ -876,6 +908,11 @@ int f2fs_get_dnode_of_data(struct dnode_of_data *dn, pgoff_t index, int mode)
 		f2fs_update_read_extent_tree_range_compressed(dn->inode,
 					fofs, blkaddr, cluster_size, c_len);
 	}
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_seqzone_file(dn->inode))
+		dn->seqzone_index = seqzone_index(dn->inode,
+					dn->node_page, dn->ofs_in_node);
+#endif
 out:
 	return 0;
 
@@ -906,7 +943,7 @@ static int truncate_node(struct dnode_of_data *dn)
 		return err;
 
 	/* Deallocate node address */
-	f2fs_invalidate_blocks(sbi, ni.blk_addr, 1);
+	f2fs_invalidate_blocks(sbi, ni.blk_addr);
 	dec_valid_node_count(sbi, dn->inode, dn->nid == dn->inode->i_ino);
 	set_node_addr(sbi, &ni, NULL_ADDR, false);
 
@@ -2723,7 +2760,7 @@ int f2fs_recover_xattr_data(struct inode *inode, struct page *page)
 	if (err)
 		return err;
 
-	f2fs_invalidate_blocks(sbi, ni.blk_addr, 1);
+	f2fs_invalidate_blocks(sbi, ni.blk_addr);
 	dec_valid_node_count(sbi, inode, false);
 	set_node_addr(sbi, &ni, NULL_ADDR, false);
 
@@ -2743,11 +2780,11 @@ int f2fs_recover_xattr_data(struct inode *inode, struct page *page)
 	f2fs_update_inode_page(inode);
 
 	/* 3: update and set xattr node page dirty */
-	if (page) {
+	if (page)
 		memcpy(F2FS_NODE(xpage), F2FS_NODE(page),
 				VALID_XATTR_BLOCK_SIZE);
-		set_page_dirty(xpage);
-	}
+
+	set_page_dirty(xpage);
 	f2fs_put_page(xpage, 1);
 
 	return 0;
diff --git a/fs/f2fs/node.h b/fs/f2fs/node.h
index 906fb67a9..c5d23403a 100644
--- a/fs/f2fs/node.h
+++ b/fs/f2fs/node.h
@@ -428,3 +428,7 @@ static inline void set_mark(struct page *page, int mark, int type)
 }
 #define set_dentry_mark(page, mark)	set_mark(page, mark, DENT_BIT_SHIFT)
 #define set_fsync_mark(page, mark)	set_mark(page, mark, FSYNC_BIT_SHIFT)
+#ifdef CONFIG_F2FS_SEQZONE
+u32 seqzone_index(struct inode *inode,
+			struct page *node_page, unsigned int offset);
+#endif
\ No newline at end of file
diff --git a/fs/f2fs/recovery.c b/fs/f2fs/recovery.c
index 1b6c41f86..03a4f615b 100644
--- a/fs/f2fs/recovery.c
+++ b/fs/f2fs/recovery.c
@@ -287,6 +287,9 @@ static void recover_inline_flags(struct inode *inode, struct f2fs_inode *ri)
 static int recover_inode(struct inode *inode, struct page *page)
 {
 	struct f2fs_inode *raw = F2FS_INODE(page);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
+#endif
 	char *name;
 	int err;
 
@@ -334,6 +337,10 @@ static int recover_inode(struct inode *inode, struct page *page)
 				le16_to_cpu(raw->i_gc_failures);
 
 	recover_inline_flags(inode, raw);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_inode_support_dedup(sbi, inode))
+		get_dedup_flags_info(inode, raw);
+#endif
 
 	f2fs_mark_inode_dirty_sync(inode, true);
 
@@ -730,7 +737,10 @@ static int do_recover_data(struct f2fs_sb_info *sbi, struct inode *inode,
 						ERROR_INVALID_BLKADDR);
 				goto err;
 			}
-
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(dn.inode))
+				dn.seqzone_index = seqzone_index(dn.inode, page, dn.ofs_in_node);
+#endif
 			/* write dummy data page */
 			f2fs_replace_block(sbi, &dn, src, dest,
 						ni.version, false, false);
diff --git a/fs/f2fs/segment.c b/fs/f2fs/segment.c
old mode 100644
new mode 100755
index e02d2f0ec..7e185dcba
--- a/fs/f2fs/segment.c
+++ b/fs/f2fs/segment.c
@@ -201,12 +201,6 @@ void f2fs_abort_atomic_write(struct inode *inode, bool clean)
 	clear_inode_flag(inode, FI_ATOMIC_FILE);
 	if (is_inode_flag_set(inode, FI_ATOMIC_DIRTIED)) {
 		clear_inode_flag(inode, FI_ATOMIC_DIRTIED);
-		/*
-		 * The vfs inode keeps clean during commit, but the f2fs inode
-		 * doesn't. So clear the dirty state after commit and let
-		 * f2fs_mark_inode_dirty_sync ensure a consistent dirty state.
-		 */
-		f2fs_inode_synced(inode);
 		f2fs_mark_inode_dirty_sync(inode, true);
 	}
 	stat_dec_atomic_inode(inode);
@@ -222,7 +216,7 @@ void f2fs_abort_atomic_write(struct inode *inode, bool clean)
 }
 
 static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
-			block_t new_addr, block_t *old_addr, bool recover)
+			block_t new_addr, struct revoke_entry *re, bool recover)
 {
 	struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 	struct dnode_of_data dn;
@@ -247,11 +241,15 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 	}
 
 	if (recover) {
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_seqzone_file(dn.inode))
+			dn.seqzone_index = re->old_seqzone_index;
+#endif
 		/* dn.data_blkaddr is always valid */
 		if (!__is_valid_data_blkaddr(new_addr)) {
 			if (new_addr == NULL_ADDR)
 				dec_valid_block_count(sbi, inode, 1);
-			f2fs_invalidate_blocks(sbi, dn.data_blkaddr, 1);
+			f2fs_invalidate_blocks(sbi, dn.data_blkaddr);
 			f2fs_update_data_blkaddr(&dn, new_addr);
 		} else {
 			f2fs_replace_block(sbi, &dn, dn.data_blkaddr,
@@ -259,6 +257,11 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 		}
 	} else {
 		blkcnt_t count = 1;
+#ifdef CONFIG_F2FS_SEQZONE
+		pgoff_t new_seqzone_index;
+		if (f2fs_seqzone_file(dn.inode))
+			new_seqzone_index = re->old_seqzone_index;
+#endif
 
 		err = inc_valid_block_count(sbi, inode, &count, true);
 		if (err) {
@@ -266,7 +269,13 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 			return err;
 		}
 
-		*old_addr = dn.data_blkaddr;
+		re->old_addr = dn.data_blkaddr;
+#ifdef CONFIG_F2FS_SEQZONE
+		if (f2fs_seqzone_file(dn.inode)) {
+			re->old_seqzone_index = dn.seqzone_index;
+			dn.seqzone_index = new_seqzone_index;
+		}
+#endif
 		f2fs_truncate_data_blocks_range(&dn, 1);
 		dec_valid_block_count(sbi, F2FS_I(inode)->cow_inode, count);
 
@@ -277,7 +286,7 @@ static int __replace_atomic_write_block(struct inode *inode, pgoff_t index,
 	f2fs_put_dnode(&dn);
 
 	trace_f2fs_replace_atomic_write_block(inode, F2FS_I(inode)->cow_inode,
-			index, old_addr ? *old_addr : 0, new_addr, recover);
+			index, re ? re->old_addr : 0, new_addr, recover);
 	return 0;
 }
 
@@ -291,7 +300,7 @@ static void __complete_revoke_list(struct inode *inode, struct list_head *head,
 	list_for_each_entry_safe(cur, tmp, head, list) {
 		if (revoke) {
 			__replace_atomic_write_block(inode, cur->index,
-						cur->old_addr, NULL, true);
+						cur->old_addr, cur, true);
 		} else if (truncate) {
 			f2fs_truncate_hole(inode, start_index, cur->index);
 			start_index = cur->index + 1;
@@ -353,9 +362,14 @@ static int __f2fs_commit_atomic_write(struct inode *inode)
 
 			new = f2fs_kmem_cache_alloc(revoke_entry_slab, GFP_NOFS,
 							true, NULL);
+#ifdef CONFIG_F2FS_SEQZONE
+			if (f2fs_seqzone_file(dn.inode))
+				new->old_seqzone_index =
+					seqzone_index(dn.inode, dn.node_page, dn.ofs_in_node);
+#endif
 
 			ret = __replace_atomic_write_block(inode, index, blkaddr,
-							&new->old_addr, false);
+							new, false);
 			if (ret) {
 				f2fs_put_dnode(&dn);
 				kmem_cache_free(revoke_entry_slab, new);
@@ -1190,7 +1204,10 @@ static void __init_discard_policy(struct f2fs_sb_info *sbi,
 		dpolicy->min_interval = dcc->min_discard_issue_time;
 		dpolicy->mid_interval = dcc->mid_discard_issue_time;
 		dpolicy->max_interval = dcc->max_discard_issue_time;
-		dpolicy->io_aware = true;
+		if (dcc->discard_io_aware == DPOLICY_IO_AWARE_ENABLE)
+			dpolicy->io_aware = true;
+		else if (dcc->discard_io_aware == DPOLICY_IO_AWARE_DISABLE)
+			dpolicy->io_aware = false;
 		dpolicy->sync = false;
 		dpolicy->ordered = true;
 		if (utilization(sbi) > dcc->discard_urgent_util) {
@@ -2197,6 +2214,7 @@ static int create_discard_cmd_control(struct f2fs_sb_info *sbi)
 
 	dcc->discard_io_aware_gran = MAX_PLIST_NUM;
 	dcc->discard_granularity = DEFAULT_DISCARD_GRANULARITY;
+	dcc->discard_io_aware = DPOLICY_IO_AWARE_ENABLE;
 	dcc->max_ordered_discard = DEFAULT_MAX_ORDERED_DISCARD_GRANULARITY;
 	if (F2FS_OPTION(sbi).discard_unit == DISCARD_UNIT_SEGMENT)
 		dcc->discard_granularity = sbi->blocks_per_seg;
@@ -2310,38 +2328,78 @@ static void update_segment_mtime(struct f2fs_sb_info *sbi, block_t blkaddr,
 		SIT_I(sbi)->max_mtime = ctime;
 }
 
-/*
- * NOTE: when updating multiple blocks at the same time, please ensure
- * that the consecutive input blocks belong to the same segment.
- */
-static int update_sit_entry_for_release(struct f2fs_sb_info *sbi, struct seg_entry *se,
-				block_t blkaddr, unsigned int offset, int del)
+static void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)
 {
+	struct seg_entry *se;
+	unsigned int segno, offset;
+	long int new_vblocks;
 	bool exist;
 #ifdef CONFIG_F2FS_CHECK_FS
 	bool mir_exist;
 #endif
-	int i;
-	int del_count = -del;
 
-	f2fs_bug_on(sbi, GET_SEGNO(sbi, blkaddr) != GET_SEGNO(sbi, blkaddr + del_count - 1));
+	segno = GET_SEGNO(sbi, blkaddr);
+	if (segno == NULL_SEGNO)
+		return;
 
-	for (i = 0; i < del_count; i++) {
-		exist = f2fs_test_and_clear_bit(offset + i, se->cur_valid_map);
+	se = get_seg_entry(sbi, segno);
+	new_vblocks = se->valid_blocks + del;
+	offset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);
+
+	f2fs_bug_on(sbi, (new_vblocks < 0 ||
+			(new_vblocks > f2fs_usable_blks_in_seg(sbi, segno))));
+
+	se->valid_blocks = new_vblocks;
+
+	/* Update valid block bitmap */
+	if (del > 0) {
+		exist = f2fs_test_and_set_bit(offset, se->cur_valid_map);
 #ifdef CONFIG_F2FS_CHECK_FS
-		mir_exist = f2fs_test_and_clear_bit(offset + i,
+		mir_exist = f2fs_test_and_set_bit(offset,
+						se->cur_valid_map_mir);
+		if (unlikely(exist != mir_exist)) {
+			f2fs_err(sbi, "Inconsistent error when setting bitmap, blk:%u, old bit:%d",
+				 blkaddr, exist);
+			f2fs_bug_on(sbi, 1);
+		}
+#endif
+		if (unlikely(exist)) {
+			f2fs_err(sbi, "Bitmap was wrongly set, blk:%u",
+				 blkaddr);
+			f2fs_bug_on(sbi, 1);
+			se->valid_blocks--;
+			del = 0;
+		}
+
+		if (f2fs_block_unit_discard(sbi) &&
+				!f2fs_test_and_set_bit(offset, se->discard_map))
+			sbi->discard_blks--;
+
+		/*
+		 * SSR should never reuse block which is checkpointed
+		 * or newly invalidated.
+		 */
+		if (!is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
+			if (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))
+				se->ckpt_valid_blocks++;
+		}
+	} else {
+		exist = f2fs_test_and_clear_bit(offset, se->cur_valid_map);
+#ifdef CONFIG_F2FS_CHECK_FS
+		mir_exist = f2fs_test_and_clear_bit(offset,
 						se->cur_valid_map_mir);
 		if (unlikely(exist != mir_exist)) {
 			f2fs_err(sbi, "Inconsistent error when clearing bitmap, blk:%u, old bit:%d",
-				blkaddr + i, exist);
+				 blkaddr, exist);
 			f2fs_bug_on(sbi, 1);
 		}
 #endif
 		if (unlikely(!exist)) {
-			f2fs_err(sbi, "Bitmap was wrongly cleared, blk:%u", blkaddr + i);
+			f2fs_err(sbi, "Bitmap was wrongly cleared, blk:%u",
+				 blkaddr);
 			f2fs_bug_on(sbi, 1);
 			se->valid_blocks++;
-			del += 1;
+			del = 0;
 		} else if (unlikely(is_sbi_flag_set(sbi, SBI_CP_DISABLED))) {
 			/*
 			 * If checkpoints are off, we must not reuse data that
@@ -2349,7 +2407,7 @@ static int update_sit_entry_for_release(struct f2fs_sb_info *sbi, struct seg_ent
 			 * before, we must track that to know how much space we
 			 * really have.
 			 */
-			if (f2fs_test_bit(offset + i, se->ckpt_valid_map)) {
+			if (f2fs_test_bit(offset, se->ckpt_valid_map)) {
 				spin_lock(&sbi->stat_lock);
 				sbi->unusable_block_count++;
 				spin_unlock(&sbi->stat_lock);
@@ -2357,91 +2415,12 @@ static int update_sit_entry_for_release(struct f2fs_sb_info *sbi, struct seg_ent
 		}
 
 		if (f2fs_block_unit_discard(sbi) &&
-				f2fs_test_and_clear_bit(offset + i, se->discard_map))
+			f2fs_test_and_clear_bit(offset, se->discard_map))
 			sbi->discard_blks++;
-
-		if (!f2fs_test_bit(offset + i, se->ckpt_valid_map))
-			se->ckpt_valid_blocks -= 1;
-	}
-
-	return del;
-}
-
-static int update_sit_entry_for_alloc(struct f2fs_sb_info *sbi, struct seg_entry *se,
-				block_t blkaddr, unsigned int offset, int del)
-{
-	bool exist;
-#ifdef CONFIG_F2FS_CHECK_FS
-	bool mir_exist;
-#endif
-
-	exist = f2fs_test_and_set_bit(offset, se->cur_valid_map);
-#ifdef CONFIG_F2FS_CHECK_FS
-	mir_exist = f2fs_test_and_set_bit(offset,
-					se->cur_valid_map_mir);
-	if (unlikely(exist != mir_exist)) {
-		f2fs_err(sbi, "Inconsistent error when setting bitmap, blk:%u, old bit:%d",
-			blkaddr, exist);
-		f2fs_bug_on(sbi, 1);
-	}
-#endif
-	if (unlikely(exist)) {
-		f2fs_err(sbi, "Bitmap was wrongly set, blk:%u", blkaddr);
-		f2fs_bug_on(sbi, 1);
-		se->valid_blocks--;
-		del = 0;
-	}
-
-	if (f2fs_block_unit_discard(sbi) &&
-			!f2fs_test_and_set_bit(offset, se->discard_map))
-		sbi->discard_blks--;
-
-	/*
-	 * SSR should never reuse block which is checkpointed
-	 * or newly invalidated.
-	 */
-	if (!is_sbi_flag_set(sbi, SBI_CP_DISABLED)) {
-		if (!f2fs_test_and_set_bit(offset, se->ckpt_valid_map))
-			se->ckpt_valid_blocks++;
 	}
-
 	if (!f2fs_test_bit(offset, se->ckpt_valid_map))
 		se->ckpt_valid_blocks += del;
 
-	return del;
-}
-
-/*
- * If releasing blocks, this function supports updating multiple consecutive blocks
- * at one time, but please note that these consecutive blocks need to belong to the
- * same segment.
- */
-static void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)
-{
-	struct seg_entry *se;
-	unsigned int segno, offset;
-	long int new_vblocks;
-
-	segno = GET_SEGNO(sbi, blkaddr);
-	if (segno == NULL_SEGNO)
-		return;
-
-	se = get_seg_entry(sbi, segno);
-	new_vblocks = se->valid_blocks + del;
-	offset = GET_BLKOFF_FROM_SEG0(sbi, blkaddr);
-
-	f2fs_bug_on(sbi, (new_vblocks < 0 ||
-			(new_vblocks > f2fs_usable_blks_in_seg(sbi, segno))));
-
-	se->valid_blocks = new_vblocks;
-
-	/* Update valid block bitmap */
-	if (del > 0) {
-		del = update_sit_entry_for_alloc(sbi, se, blkaddr, offset, del);
-	} else {
-		del = update_sit_entry_for_release(sbi, se, blkaddr, offset, del);
-	}
-
 	__mark_sit_entry_dirty(sbi, segno);
 
 	/* update total number of valid blocks to be written in ckpt area */
@@ -2451,43 +2430,25 @@ static void update_sit_entry(struct f2fs_sb_info *sbi, block_t blkaddr, int del)
 		get_sec_entry(sbi, segno)->valid_blocks += del;
 }
 
-void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr,
-				unsigned int len)
+void f2fs_invalidate_blocks(struct f2fs_sb_info *sbi, block_t addr)
 {
 	unsigned int segno = GET_SEGNO(sbi, addr);
 	struct sit_info *sit_i = SIT_I(sbi);
-	block_t addr_start = addr, addr_end = addr + len - 1;
-	unsigned int seg_num = GET_SEGNO(sbi, addr_end) - segno + 1;
-	unsigned int i = 1, max_blocks = sbi->blocks_per_seg, cnt;
 
 	f2fs_bug_on(sbi, addr == NULL_ADDR);
 	if (addr == NEW_ADDR || addr == COMPRESS_ADDR)
 		return;
 
-	f2fs_invalidate_internal_cache(sbi, addr, len);
+	f2fs_invalidate_internal_cache(sbi, addr);
 
 	/* add it into sit main buffer */
 	down_write(&sit_i->sentry_lock);
 
-	if (seg_num == 1)
-		cnt = len;
-	else
-		cnt = max_blocks - GET_BLKOFF_FROM_SEG0(sbi, addr);
-
-	do {
-		update_segment_mtime(sbi, addr_start, 0);
-		update_sit_entry(sbi, addr_start, -cnt);
+	update_segment_mtime(sbi, addr, 0);
+	update_sit_entry(sbi, addr, -1);
 
-		/* add it into dirty seglist */
-		locate_dirty_segment(sbi, segno);
-
-		/* update @addr_start and @cnt and @segno */
-		addr_start = START_BLOCK(sbi, ++segno);
-		if (++i == seg_num)
-			cnt = GET_BLKOFF_FROM_SEG0(sbi, addr_end) + 1;
-		else
-			cnt = max_blocks;
-	} while (i <= seg_num);
+	/* add it into dirty seglist */
+	locate_dirty_segment(sbi, segno);
 
 	up_write(&sit_i->sentry_lock);
 }
@@ -2879,9 +2840,33 @@ static void __f2fs_init_atgc_curseg(struct f2fs_sb_info *sbi)
 	f2fs_up_read(&SM_I(sbi)->curseg_lock);
 
 }
+
+#ifdef CONFIG_F2FS_SEQZONE
+void __f2fs_init_seqzone_curseg(struct f2fs_sb_info *sbi)
+{
+	int i;
+	struct curseg_info *curseg;
+	for (i = CURSEG_HOT_DATA_0; i <= CURSEG_WARM_DATA_7; i++) {
+		curseg = CURSEG_I(sbi, i);
+		f2fs_down_read(&SM_I(sbi)->curseg_lock);
+		mutex_lock(&curseg->curseg_mutex);
+		down_write(&SIT_I(sbi)->sentry_lock);
+		new_curseg(sbi, i, true);
+		stat_inc_seg_type(sbi, curseg);
+		up_write(&SIT_I(sbi)->sentry_lock);
+		mutex_unlock(&curseg->curseg_mutex);
+		f2fs_up_read(&SM_I(sbi)->curseg_lock);
+	}
+}
+#endif
+
 void f2fs_init_inmem_curseg(struct f2fs_sb_info *sbi)
 {
 	__f2fs_init_atgc_curseg(sbi);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi))
+		__f2fs_init_seqzone_curseg(sbi);
+#endif
 }
 
 static void __f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi, int type)
@@ -2907,6 +2892,14 @@ static void __f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi, int type)
 void f2fs_save_inmem_curseg(struct f2fs_sb_info *sbi)
 {
 	__f2fs_save_inmem_curseg(sbi, CURSEG_COLD_DATA_PINNED);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi)) {
+		int i;
+		for (i = CURSEG_HOT_DATA_0; i <= CURSEG_WARM_DATA_7; i++) {
+			__f2fs_save_inmem_curseg(sbi, i);
+		}
+	}
+#endif
 
 	if (sbi->am.atgc_enabled)
 		__f2fs_save_inmem_curseg(sbi, CURSEG_ALL_DATA_ATGC);
@@ -2932,6 +2925,14 @@ static void __f2fs_restore_inmem_curseg(struct f2fs_sb_info *sbi, int type)
 void f2fs_restore_inmem_curseg(struct f2fs_sb_info *sbi)
 {
 	__f2fs_restore_inmem_curseg(sbi, CURSEG_COLD_DATA_PINNED);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi)) {
+		int i;
+		for (i = CURSEG_HOT_DATA_0; i <= CURSEG_WARM_DATA_7; i++) {
+			__f2fs_restore_inmem_curseg(sbi, i);
+		}
+	}
+#endif
 
 	if (sbi->am.atgc_enabled)
 		__f2fs_restore_inmem_curseg(sbi, CURSEG_ALL_DATA_ATGC);
@@ -3374,14 +3375,33 @@ static void f2fs_randomize_chunk(struct f2fs_sb_info *sbi,
 int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 		block_t old_blkaddr, block_t *new_blkaddr,
 		struct f2fs_summary *sum, int type,
-		struct f2fs_io_info *fio)
+		struct f2fs_io_info *fio, bool use_seqzone)
 {
 	struct sit_info *sit_i = SIT_I(sbi);
-	struct curseg_info *curseg = CURSEG_I(sbi, type);
+	struct curseg_info *curseg;
 	unsigned long long old_mtime;
 	bool from_gc = (type == CURSEG_ALL_DATA_ATGC);
 	struct seg_entry *se = NULL;
 	bool segment_full = false;
+	int real_type = type;
+#ifdef CONFIG_F2FS_SEQZONE
+	bool is_seqzone_file = false;
+	if (fio && fio->use_seqzone) {
+		f2fs_seqzone_fio_check(fio);
+		is_seqzone_file = true;
+	}
+
+	if ((is_seqzone_file || use_seqzone) &&
+		IS_DATASEG(type) && (CURSEG_COLD_DATA != type)) {
+		real_type = (type - CURSEG_HOT_DATA) * F2FS_NR_CPUS +
+					smp_processor_id() + CURSEG_HOT_DATA_0;
+		curseg = CURSEG_I(sbi, real_type);
+		// inmem curseg has not been inited, rollback to origin curseg.
+		if (!curseg->inited)
+			real_type = type;
+	}
+#endif
+	curseg = CURSEG_I(sbi, real_type);
 
 	f2fs_down_read(&SM_I(sbi)->curseg_lock);
 
@@ -3439,10 +3459,10 @@ int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 			get_atssr_segment(sbi, type, se->type,
 						AT_SSR, se->mtime);
 		} else {
-			if (need_new_seg(sbi, type))
-				new_curseg(sbi, type, false);
+			if (need_new_seg(sbi, real_type))
+				new_curseg(sbi, real_type, false);
 			else
-				change_curseg(sbi, type);
+				change_curseg(sbi, real_type);
 			stat_inc_seg_type(sbi, curseg);
 		}
 
@@ -3470,13 +3490,19 @@ int f2fs_allocate_data_block(struct f2fs_sb_info *sbi, struct page *page,
 
 	if (fio) {
 		struct f2fs_bio_info *io;
+		int real_temp = fio->temp;
+#ifdef CONFIG_F2FS_SEQZONE
+		real_temp = (fio->use_seqzone && fio->type == DATA && fio->temp != COLD) ?
+					fio->temp * F2FS_NR_CPUS + smp_processor_id() : fio->temp;
+		fio->real_temp = real_temp;
+#endif
 
 		if (F2FS_IO_ALIGNED(sbi))
 			fio->retry = 0;
 
 		INIT_LIST_HEAD(&fio->list);
 		fio->in_list = 1;
-		io = sbi->write_io[fio->type] + fio->temp;
+		io = sbi->write_io[fio->type] + real_temp;
 		spin_lock(&io->io_lock);
 		list_add_tail(&fio->list, &io->io_list);
 		spin_unlock(&io->io_lock);
@@ -3530,7 +3556,7 @@ static void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)
 		f2fs_down_read(&fio->sbi->io_order_lock);
 reallocate:
 	if (f2fs_allocate_data_block(fio->sbi, fio->page, fio->old_blkaddr,
-			&fio->new_blkaddr, sum, type, fio)) {
+			&fio->new_blkaddr, sum, type, fio, 0)) {
 		if (fscrypt_inode_uses_fs_layer_crypto(fio->page->mapping->host))
 			fscrypt_finalize_bounce_page(&fio->encrypted_page);
 		if (PageWriteback(fio->page))
@@ -3540,8 +3566,11 @@ static void do_write_page(struct f2fs_summary *sum, struct f2fs_io_info *fio)
 		goto out;
 	}
 
-	if (GET_SEGNO(fio->sbi, fio->old_blkaddr) != NULL_SEGNO)
-		f2fs_invalidate_internal_cache(fio->sbi, fio->old_blkaddr, 1);
+	if (GET_SEGNO(fio->sbi, fio->old_blkaddr) != NULL_SEGNO) {
+		invalidate_mapping_pages(META_MAPPING(fio->sbi),
+					fio->old_blkaddr, fio->old_blkaddr);
+		f2fs_invalidate_compress_page(fio->sbi, fio->old_blkaddr);
+	}
 
 	/* writeout dirty page into bdev */
 	f2fs_submit_page_write(fio);
@@ -3603,6 +3632,10 @@ void f2fs_outplace_write_data(struct dnode_of_data *dn,
 		f2fs_update_age_extent_cache(dn);
 	set_summary(&sum, dn->nid, dn->ofs_in_node, fio->version);
 	do_write_page(&sum, fio);
+#ifdef CONFIG_F2FS_SEQZONE
+	if (fio->use_seqzone)
+		dn->seqzone_index = fio->seqzone_index;
+#endif
 	f2fs_update_data_blkaddr(dn, fio->new_blkaddr);
 
 	f2fs_update_iostat(sbi, dn->inode, fio->io_type, F2FS_BLKSIZE);
@@ -3711,8 +3744,8 @@ void f2fs_do_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		}
 	}
 
-	f2fs_bug_on(sbi, !IS_DATASEG(type));
 	curseg = CURSEG_I(sbi, type);
+	f2fs_bug_on(sbi, !IS_DATASEG(curseg->seg_type));
 
 	mutex_lock(&curseg->curseg_mutex);
 	down_write(&sit_i->sentry_lock);
@@ -3736,7 +3769,7 @@ void f2fs_do_replace_block(struct f2fs_sb_info *sbi, struct f2fs_summary *sum,
 		update_sit_entry(sbi, new_blkaddr, 1);
 	}
 	if (GET_SEGNO(sbi, old_blkaddr) != NULL_SEGNO) {
-		f2fs_invalidate_internal_cache(sbi, old_blkaddr, 1);
+		f2fs_invalidate_internal_cache(sbi, old_blkaddr);
 		if (!from_gc)
 			update_segment_mtime(sbi, old_blkaddr, 0);
 		update_sit_entry(sbi, old_blkaddr, -1);
@@ -4514,6 +4547,10 @@ static int build_curseg(struct f2fs_sb_info *sbi)
 			array[i].seg_type = CURSEG_COLD_DATA;
 		else if (i == CURSEG_ALL_DATA_ATGC)
 			array[i].seg_type = CURSEG_COLD_DATA;
+#ifdef CONFIG_F2FS_SEQZONE
+		else if (i >= CURSEG_HOT_DATA_0 && i <= CURSEG_WARM_DATA_7)
+			array[i].seg_type = (i - CURSEG_HOT_DATA_0) / 8 + CURSEG_HOT_DATA;
+#endif
 		array[i].segno = NULL_SEGNO;
 		array[i].next_blkoff = 0;
 		array[i].inited = false;
@@ -5428,8 +5465,16 @@ int __init f2fs_create_segment_manager_caches(void)
 			sizeof(struct revoke_entry));
 	if (!revoke_entry_slab)
 		goto destroy_sit_entry_set;
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (create_page_info_slab())
+		goto destroy_revoke_entry;
+#endif
 	return 0;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+destroy_revoke_entry:
+	kmem_cache_destroy(revoke_entry_slab);
+#endif
 destroy_sit_entry_set:
 	kmem_cache_destroy(sit_entry_set_slab);
 destroy_discard_cmd:
@@ -5446,4 +5491,7 @@ void f2fs_destroy_segment_manager_caches(void)
 	kmem_cache_destroy(discard_cmd_slab);
 	kmem_cache_destroy(discard_entry_slab);
 	kmem_cache_destroy(revoke_entry_slab);
+#ifdef CONFIG_F2FS_FS_DEDUP
+	destroy_page_info_slab();
+#endif
 }
diff --git a/fs/f2fs/segment.h b/fs/f2fs/segment.h
index 77de9adbf..7c5c8148a 100644
--- a/fs/f2fs/segment.h
+++ b/fs/f2fs/segment.h
@@ -36,6 +36,28 @@ static inline void sanity_check_seg_type(struct f2fs_sb_info *sbi,
 #define IS_WARM(t)	((t) == CURSEG_WARM_NODE || (t) == CURSEG_WARM_DATA)
 #define IS_COLD(t)	((t) == CURSEG_COLD_NODE || (t) == CURSEG_COLD_DATA)
 
+#ifdef CONFIG_F2FS_SEQZONE
+#define IS_CURSEG_SEQZONE(sbi, seg)	(\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_0)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_1)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_2)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_3)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_4)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_5)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_6)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA_7)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_0)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_1)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_2)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_3)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_4)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_5)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_6)->segno) ||	\
+	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA_7)->segno))
+#else
+#define IS_CURSEG_SEQZONE(sbi, seg)	(false)
+#endif
+
 #define IS_CURSEG(sbi, seg)						\
 	(((seg) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_DATA)->segno) ||	\
@@ -44,7 +66,46 @@ static inline void sanity_check_seg_type(struct f2fs_sb_info *sbi,
 	 ((seg) == CURSEG_I(sbi, CURSEG_WARM_NODE)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_COLD_NODE)->segno) ||	\
 	 ((seg) == CURSEG_I(sbi, CURSEG_COLD_DATA_PINNED)->segno) ||	\
-	 ((seg) == CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC)->segno))
+	 ((seg) == CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC)->segno) ||	\
+	IS_CURSEG_SEQZONE(sbi, (seg)))
+
+#ifdef CONFIG_F2FS_SEQZONE
+#define IS_CURSEC_SEQZONE(sbi, secno)	(	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_0)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_1)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_2)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_3)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_4)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_5)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_6)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA_7)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_0)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_1)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_2)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_3)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_4)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_5)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_6)->segno /		\
+	  (sbi)->segs_per_sec) ||	\
+	 ((secno) == CURSEG_I(sbi, CURSEG_WARM_DATA_7)->segno /		\
+	  (sbi)->segs_per_sec))
+#else
+#define IS_CURSEC_SEQZONE(sbi, secno)	(false)
+#endif
 
 #define IS_CURSEC(sbi, secno)						\
 	(((secno) == CURSEG_I(sbi, CURSEG_HOT_DATA)->segno /		\
@@ -62,7 +123,9 @@ static inline void sanity_check_seg_type(struct f2fs_sb_info *sbi,
 	 ((secno) == CURSEG_I(sbi, CURSEG_COLD_DATA_PINNED)->segno /	\
 	  (sbi)->segs_per_sec) ||	\
 	 ((secno) == CURSEG_I(sbi, CURSEG_ALL_DATA_ATGC)->segno /	\
-	  (sbi)->segs_per_sec))
+	  (sbi)->segs_per_sec) ||	\
+	 IS_CURSEC_SEQZONE(sbi, secno))
+
 
 #define MAIN_BLKADDR(sbi)						\
 	(SM_I(sbi) ? SM_I(sbi)->main_blkaddr : 				\
@@ -220,6 +283,9 @@ struct sec_entry {
 struct revoke_entry {
 	struct list_head list;
 	block_t old_addr;		/* for revoking when fail to commit */
+#ifdef CONFIG_F2FS_SEQZONE
+	pgoff_t old_seqzone_index;	/* for revoking when fail to commit */
+#endif
 	pgoff_t index;
 };
 
diff --git a/fs/f2fs/super.c b/fs/f2fs/super.c
old mode 100644
new mode 100755
index d614a82e2..d9e90a821
--- a/fs/f2fs/super.c
+++ b/fs/f2fs/super.c
@@ -42,32 +42,56 @@
 
 static struct kmem_cache *f2fs_inode_cachep;
 
+#ifdef CONFIG_F2FS_SEQZONE
+#include <linux/of.h>
+const char *seqzone_switch = NULL;
+#endif
+
 #ifdef CONFIG_F2FS_FAULT_INJECTION
 
 const char *f2fs_fault_name[FAULT_MAX] = {
-	[FAULT_KMALLOC]			= "kmalloc",
-	[FAULT_KVMALLOC]		= "kvmalloc",
-	[FAULT_PAGE_ALLOC]		= "page alloc",
-	[FAULT_PAGE_GET]		= "page get",
-	[FAULT_ALLOC_NID]		= "alloc nid",
-	[FAULT_ORPHAN]			= "orphan",
-	[FAULT_BLOCK]			= "no more block",
-	[FAULT_DIR_DEPTH]		= "too big dir depth",
-	[FAULT_EVICT_INODE]		= "evict_inode fail",
-	[FAULT_TRUNCATE]		= "truncate fail",
-	[FAULT_READ_IO]			= "read IO error",
-	[FAULT_CHECKPOINT]		= "checkpoint error",
-	[FAULT_DISCARD]			= "discard error",
-	[FAULT_WRITE_IO]		= "write IO error",
-	[FAULT_SLAB_ALLOC]		= "slab alloc",
-	[FAULT_DQUOT_INIT]		= "dquot initialize",
-	[FAULT_LOCK_OP]			= "lock_op",
-	[FAULT_BLKADDR_VALIDITY]	= "invalid blkaddr",
-	[FAULT_BLKADDR_CONSISTENCE]	= "inconsistent blkaddr",
+	[FAULT_KMALLOC]		= "kmalloc",
+	[FAULT_KVMALLOC]	= "kvmalloc",
+	[FAULT_PAGE_ALLOC]	= "page alloc",
+	[FAULT_PAGE_GET]	= "page get",
+	[FAULT_ALLOC_NID]	= "alloc nid",
+	[FAULT_ORPHAN]		= "orphan",
+	[FAULT_BLOCK]		= "no more block",
+	[FAULT_DIR_DEPTH]	= "too big dir depth",
+	[FAULT_EVICT_INODE]	= "evict_inode fail",
+	[FAULT_TRUNCATE]	= "truncate fail",
+	[FAULT_READ_IO]		= "read IO error",
+	[FAULT_CHECKPOINT]	= "checkpoint error",
+	[FAULT_DISCARD]		= "discard error",
+	[FAULT_WRITE_IO]	= "write IO error",
+	[FAULT_SLAB_ALLOC]	= "slab alloc",
+	[FAULT_DQUOT_INIT]	= "dquot initialize",
+	[FAULT_LOCK_OP]		= "lock_op",
+	[FAULT_BLKADDR]		= "invalid blkaddr",
+#ifdef CONFIG_F2FS_APPBOOST
+	[FAULT_READ_ERROR]    = "appboost read error",
+	[FAULT_WRITE_ERROR]   = "appboost write error",
+	[FAULT_PAGE_ERROR]      = "appboost page error",
+	[FAULT_FSYNC_ERROR]     = "appboost fsync error",
+	[FAULT_FLUSH_ERROR]     = "appboost flush error",
+	[FAULT_WRITE_TAIL_ERROR]= "appboost write tail error",
+#endif
+	[FAULT_COMPRESS_REDIRTY] = "compress ioc redirty",
+	[FAULT_COMPRESS_WRITEBACK] = "compress ioc writeback",
+	[FAULT_COMPRESS_RESERVE_NOSPC] = "compress reserve nospc",
+#ifdef CONFIG_F2FS_FS_COMPRESSION
+	[FAULT_COMPRESS_VMAP] = "compress vmap",
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	[FAULT_COMPRESS_INIT_CTX] = "compress init ctx",
+	[FAULT_COMPRESS_PAGE_ARRAY] = "compress page array",
+	[FAULT_COMPRESS_LOW_RATIO] = "compress low ratio",
+	[FAULT_COMPRESS_GET_DNODE] = "compress get dnode",
+#endif
+#endif
 };
 
 int f2fs_build_fault_attr(struct f2fs_sb_info *sbi, unsigned long rate,
-							unsigned long type)
+							unsigned long long type)
 {
 	struct f2fs_fault_info *ffi = &F2FS_OPTION(sbi).fault_info;
 
@@ -170,6 +194,7 @@ enum {
 	Opt_compress_chksum,
 	Opt_compress_mode,
 	Opt_compress_cache,
+	Opt_compress_layout,
 	Opt_atgc,
 	Opt_gc_merge,
 	Opt_nogc_merge,
@@ -249,6 +274,7 @@ static match_table_t f2fs_tokens = {
 	{Opt_compress_chksum, "compress_chksum"},
 	{Opt_compress_mode, "compress_mode=%s"},
 	{Opt_compress_cache, "compress_cache"},
+	{Opt_compress_layout, "compress_layout=%s"},
 	{Opt_atgc, "atgc"},
 	{Opt_gc_merge, "gc_merge"},
 	{Opt_nogc_merge, "nogc_merge"},
@@ -622,12 +648,14 @@ static int f2fs_set_lz4hc_level(struct f2fs_sb_info *sbi, const char *str)
 {
 #ifdef CONFIG_F2FS_FS_LZ4HC
 	unsigned int level;
+#endif
 
 	if (strlen(str) == 3) {
 		F2FS_OPTION(sbi).compress_level = 0;
 		return 0;
 	}
 
+#ifdef CONFIG_F2FS_FS_LZ4HC
 	str += 3;
 
 	if (str[0] != ':') {
@@ -645,10 +673,6 @@ static int f2fs_set_lz4hc_level(struct f2fs_sb_info *sbi, const char *str)
 	F2FS_OPTION(sbi).compress_level = level;
 	return 0;
 #else
-	if (strlen(str) == 3) {
-		F2FS_OPTION(sbi).compress_level = 0;
-		return 0;
-	}
 	f2fs_info(sbi, "kernel doesn't support lz4hc compression");
 	return -EINVAL;
 #endif
@@ -662,7 +686,7 @@ static int f2fs_set_zstd_level(struct f2fs_sb_info *sbi, const char *str)
 	int len = 4;
 
 	if (strlen(str) == len) {
-		F2FS_OPTION(sbi).compress_level = F2FS_ZSTD_DEFAULT_CLEVEL;
+		F2FS_OPTION(sbi).compress_level = 0;
 		return 0;
 	}
 
@@ -690,6 +714,7 @@ static int f2fs_set_zstd_level(struct f2fs_sb_info *sbi, const char *str)
 	return 0;
 }
 #endif
+
 #endif
 
 static int parse_options(struct super_block *sb, char *options, bool is_remount)
@@ -703,6 +728,9 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 #endif
 	char *p, *name;
 	int arg = 0;
+#ifdef CONFIG_F2FS_FAULT_INJECTION
+	unsigned long long larg;
+#endif
 	kuid_t uid;
 	kgid_t gid;
 	int ret;
@@ -933,9 +961,9 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			break;
 
 		case Opt_fault_type:
-			if (args->from && match_int(args, &arg))
+			if (args->from && match_u64(args, &larg))
 				return -EINVAL;
-			if (f2fs_build_fault_attr(sbi, 0, arg))
+			if (f2fs_build_fault_attr(sbi, 0, larg))
 				return -EINVAL;
 			set_opt(sbi, FAULT_INJECTION);
 			break;
@@ -1152,6 +1180,15 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 				kfree(name);
 				return -EINVAL;
 			}
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+			if (F2FS_OPTION(sbi).compress_layout == COMPRESS_FIXED_OUTPUT &&
+			    (F2FS_OPTION(sbi).compress_algorithm != COMPRESS_LZ4 ||
+			     F2FS_OPTION(sbi).compress_level != 0)) {
+				f2fs_err(sbi, "fixed-output compress layout can only work on lz4");
+				kfree(name);
+				return -EINVAL;
+			}
+#endif
 			kfree(name);
 			break;
 		case Opt_compress_log_size:
@@ -1181,14 +1218,6 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			ext = F2FS_OPTION(sbi).extensions;
 			ext_cnt = F2FS_OPTION(sbi).compress_ext_cnt;
 
-			if (strlen(name) >= F2FS_EXTENSION_LEN ||
-				ext_cnt >= COMPRESS_EXT_NUM) {
-				f2fs_err(sbi,
-					"invalid extension length/number");
-				kfree(name);
-				return -EINVAL;
-			}
-
 			if (is_compress_extension_exist(sbi, name, true)) {
 				kfree(name);
 				break;
@@ -1210,14 +1239,6 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			noext = F2FS_OPTION(sbi).noextensions;
 			noext_cnt = F2FS_OPTION(sbi).nocompress_ext_cnt;
 
-			if (strlen(name) >= F2FS_EXTENSION_LEN ||
-				noext_cnt >= COMPRESS_EXT_NUM) {
-				f2fs_err(sbi,
-					"invalid extension length/number");
-				kfree(name);
-				return -EINVAL;
-			}
-
 			if (is_compress_extension_exist(sbi, name, false)) {
 				kfree(name);
 				break;
@@ -1259,6 +1280,32 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			}
 			set_opt(sbi, COMPRESS_CACHE);
 			break;
+		case Opt_compress_layout:
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+			if (!f2fs_sb_has_compression(sbi)) {
+				f2fs_info(sbi, "Image doesn't support compression");
+				break;
+			}
+			name = match_strdup(&args[0]);
+			/* fix coverity error: Dereference null return value name*/
+			if (name && !strcmp(name, "fixed-input")) {
+				F2FS_OPTION(sbi).compress_layout = COMPRESS_FIXED_INPUT;
+			} else if (name && !strcmp(name, "fixed-output")) {
+				if (F2FS_OPTION(sbi).compress_algorithm != COMPRESS_LZ4 ||
+				    F2FS_OPTION(sbi).compress_level != 0) {
+					f2fs_err(sbi, "fixed-output compress layout can only work on lz4");
+					kfree(name);
+					return -EINVAL;
+				}
+				F2FS_OPTION(sbi).compress_layout = COMPRESS_FIXED_OUTPUT;
+			} else {
+				f2fs_err(sbi, "Unknown compress layout");
+				kfree(name);
+				return -EINVAL;
+			}
+			kfree(name);
+#endif
+			break;
 #else
 		case Opt_compress_algorithm:
 		case Opt_compress_log_size:
@@ -1267,6 +1314,7 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 		case Opt_compress_chksum:
 		case Opt_compress_mode:
 		case Opt_compress_cache:
+		case Opt_compress_layout:
 			f2fs_info(sbi, "compression options not supported");
 			break;
 #endif
@@ -1394,7 +1442,7 @@ static int parse_options(struct super_block *sb, char *options, bool is_remount)
 			return -EINVAL;
 		}
 
-		min_size = sizeof(struct f2fs_xattr_header) / sizeof(__le32);
+		min_size = MIN_INLINE_XATTR_SIZE;
 		max_size = MAX_INLINE_XATTR_SIZE;
 
 		if (F2FS_OPTION(sbi).inline_xattr_size < min_size ||
@@ -1450,10 +1498,16 @@ static struct inode *f2fs_alloc_inode(struct super_block *sb)
 	init_f2fs_rwsem(&fi->i_gc_rwsem[READ]);
 	init_f2fs_rwsem(&fi->i_gc_rwsem[WRITE]);
 	init_f2fs_rwsem(&fi->i_xattr_sem);
-
+#ifdef CONFIG_F2FS_SEQZONE
+	fi->dio_cur_seqindex = 1;
+#endif
 	/* Will be used by directory only */
 	fi->i_dir_level = F2FS_SB(sb)->dir_level;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	atomic_set(&fi->inflight_read_io, 0);
+	init_waitqueue_head(&fi->dedup_wq);
+#endif
 	return &fi->vfs_inode;
 }
 
@@ -1581,6 +1635,9 @@ static void f2fs_dirty_inode(struct inode *inode, int flags)
 static void f2fs_free_inode(struct inode *inode)
 {
 	fscrypt_free_inode(inode);
+#ifdef CONFIG_F2FS_APPBOOST
+	f2fs_boostfile_free(inode);
+#endif
 	kmem_cache_free(f2fs_inode_cachep, F2FS_I(inode));
 }
 
@@ -1901,7 +1958,9 @@ static inline void f2fs_show_compress_options(struct seq_file *seq,
 {
 	struct f2fs_sb_info *sbi = F2FS_SB(sb);
 	char *algtype = "";
+#ifndef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
 	int i;
+#endif
 
 	if (!f2fs_sb_has_compression(sbi))
 		return;
@@ -1928,6 +1987,7 @@ static inline void f2fs_show_compress_options(struct seq_file *seq,
 	seq_printf(seq, ",compress_log_size=%u",
 			F2FS_OPTION(sbi).compress_log_size);
 
+#ifndef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
 	for (i = 0; i < F2FS_OPTION(sbi).compress_ext_cnt; i++) {
 		seq_printf(seq, ",compress_extension=%s",
 			F2FS_OPTION(sbi).extensions[i]);
@@ -1937,6 +1997,7 @@ static inline void f2fs_show_compress_options(struct seq_file *seq,
 		seq_printf(seq, ",nocompress_extension=%s",
 			F2FS_OPTION(sbi).noextensions[i]);
 	}
+#endif
 
 	if (F2FS_OPTION(sbi).compress_chksum)
 		seq_puts(seq, ",compress_chksum");
@@ -2054,7 +2115,7 @@ static int f2fs_show_options(struct seq_file *seq, struct dentry *root)
 	if (test_opt(sbi, FAULT_INJECTION)) {
 		seq_printf(seq, ",fault_injection=%u",
 				F2FS_OPTION(sbi).fault_info.inject_rate);
-		seq_printf(seq, ",fault_type=%u",
+		seq_printf(seq, ",fault_type=%llu",
 				F2FS_OPTION(sbi).fault_info.inject_type);
 	}
 #endif
@@ -2109,8 +2170,26 @@ static int f2fs_show_options(struct seq_file *seq, struct dentry *root)
 	return 0;
 }
 
+static bool is_data_partition(struct f2fs_sb_info *sbi)
+{
+	uint64_t min_data_partition_blocks = 0x800000; // 32GB
+
+	if (le64_to_cpu(sbi->raw_super->block_count) > min_data_partition_blocks)
+		return true;
+	return false;
+}
+
 static void default_options(struct f2fs_sb_info *sbi)
 {
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	int i = 0;
+#endif
+
+	if (is_data_partition(sbi)) {
+		sbi->oplus_feats = OPLUS_FEAT_COMPR;
+		sbi->oplus_feats |= OPLUS_FEAT_DEDUP;
+	}
+
 	/* init some FS parameters */
 	if (f2fs_sb_has_readonly(sbi))
 		F2FS_OPTION(sbi).active_logs = NR_CURSEG_RO_TYPE;
@@ -2130,7 +2209,20 @@ static void default_options(struct f2fs_sb_info *sbi)
 		F2FS_OPTION(sbi).compress_algorithm = COMPRESS_LZ4;
 		F2FS_OPTION(sbi).compress_log_size = MIN_COMPRESS_LOG_SIZE;
 		F2FS_OPTION(sbi).compress_ext_cnt = 0;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+		F2FS_OPTION(sbi).compress_mode = COMPR_MODE_USER;
+		F2FS_OPTION(sbi).compress_layout = COMPRESS_FIXED_OUTPUT;
+		strcpy(F2FS_OPTION(sbi).extensions[i++], "odex");
+		strcpy(F2FS_OPTION(sbi).extensions[i++], "vdex");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "so");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "dex");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "wxapkg");
+		//strcpy(F2FS_OPTION(sbi).extensions[i++], "js");
+		F2FS_OPTION(sbi).compress_ext_cnt = (unsigned char)i;
+#else
 		F2FS_OPTION(sbi).compress_mode = COMPR_MODE_FS;
+#endif
+
 	}
 	F2FS_OPTION(sbi).bggc_mode = BGGC_MODE_ON;
 	F2FS_OPTION(sbi).memory_mode = MEMORY_MODE_NORMAL;
@@ -3800,6 +3892,9 @@ static void init_sb_info(struct f2fs_sb_info *sbi)
 
 	init_f2fs_rwsem(&sbi->sb_lock);
 	init_f2fs_rwsem(&sbi->pin_sem);
+#ifdef CONFIG_F2FS_SEQZONE
+	sbi->seq_zone = 0;
+#endif
 }
 
 static int init_percpu_info(struct f2fs_sb_info *sbi)
@@ -4328,7 +4423,11 @@ static int f2fs_fill_super(struct super_block *sb, void *data, int silent)
 		}
 	}
 #endif
-
+#ifdef CONFIG_F2FS_APPBOOST
+	sbi->appboost = 0;
+#define APPBOOST_MAX_BLOCKS 51200
+	sbi->appboost_max_blocks = APPBOOST_MAX_BLOCKS;
+#endif
 	sb->s_op = &f2fs_sops;
 #ifdef CONFIG_FS_ENCRYPTION
 	sb->s_cop = &f2fs_cryptops;
@@ -4382,6 +4481,7 @@ static int f2fs_fill_super(struct super_block *sb, void *data, int silent)
 	if (err)
 		goto free_xattr_cache;
 
+
 	/* get an inode for meta space */
 	sbi->meta_inode = f2fs_iget(sb, F2FS_META_INO(sbi));
 	if (IS_ERR(sbi->meta_inode)) {
@@ -4802,6 +4902,17 @@ static void destroy_inodecache(void)
 	kmem_cache_destroy(f2fs_inode_cachep);
 }
 
+#ifdef CONFIG_F2FS_SEQZONE
+#define SEQZONE_CONFIG_PATH "/soc/oplus,f2fs/seqzone"
+static void update_seqzone_switch(void)
+{
+	struct device_node *np = of_find_node_by_path(SEQZONE_CONFIG_PATH);
+	if (np) {
+		of_property_read_string(np, "switch", &seqzone_switch);
+	}
+}
+#endif
+
 static int __init init_f2fs_fs(void)
 {
 	int err;
@@ -4812,6 +4923,10 @@ static int __init init_f2fs_fs(void)
 		return -EINVAL;
 	}
 
+#ifdef CONFIG_F2FS_SEQZONE
+	update_seqzone_switch();
+#endif
+
 	err = init_inodecache();
 	if (err)
 		goto fail;
diff --git a/fs/f2fs/sysfs.c b/fs/f2fs/sysfs.c
old mode 100644
new mode 100755
index 9c845fa6f..3436451d5
--- a/fs/f2fs/sysfs.c
+++ b/fs/f2fs/sysfs.c
@@ -39,6 +39,9 @@ enum {
 	RESERVED_BLOCKS,	/* struct f2fs_sb_info */
 	CPRC_INFO,	/* struct ckpt_req_control */
 	ATGC_INFO,	/* struct atgc_management */
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	COMP_EXT,
+#endif
 };
 
 static const char *gc_mode_names[MAX_GC_MODE] = {
@@ -89,6 +92,10 @@ static unsigned char *__struct_ptr(struct f2fs_sb_info *sbi, int struct_type)
 		return (unsigned char *)&sbi->cprc_info;
 	else if (struct_type == ATGC_INFO)
 		return (unsigned char *)&sbi->am;
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	else if (struct_type == COMP_EXT)
+		return (unsigned char *)&sbi->mount_opt;
+#endif
 	return NULL;
 }
 
@@ -196,6 +203,17 @@ static ssize_t features_show(struct f2fs_attr *a,
 	if (f2fs_sb_has_compression(sbi))
 		len += scnprintf(buf + len, PAGE_SIZE - len, "%s%s",
 				len ? ", " : "", "compression");
+#ifdef CONFIG_F2FS_FS_DEDUP
+	if (f2fs_sb_has_dedup(sbi))
+		len += snprintf(buf + len, PAGE_SIZE - len, "%s%s",
+				len ? ", " : "", "dedup");
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+	if (f2fs_sb_has_seqzone(sbi))
+		len += snprintf(buf + len, PAGE_SIZE - len, "%s%s",
+				len ? ", " : "", "seqzone");
+#endif
 	len += scnprintf(buf + len, PAGE_SIZE - len, "%s%s",
 				len ? ", " : "", "pin_file");
 	len += scnprintf(buf + len, PAGE_SIZE - len, "\n");
@@ -331,6 +349,51 @@ static ssize_t f2fs_sbi_show(struct f2fs_attr *a,
 
 	if (!strcmp(a->attr.name, "compr_new_inode"))
 		return sysfs_emit(buf, "%u\n", sbi->compr_new_inode);
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!strcmp(a->attr.name, "compress_layout")) {
+		if (F2FS_OPTION(sbi).compress_layout == COMPRESS_FIXED_OUTPUT)
+			return sysfs_emit(buf, "fixed-output");
+		return sysfs_emit(buf, "fixed-input");
+	}
+
+	if (!strcmp(a->attr.name, "compress_extension")) {
+		int len = 0, i;
+
+		f2fs_down_read(&sbi->sb_lock);
+		for (i = 0; i < F2FS_OPTION(sbi).compress_ext_cnt; i++) {
+			if (!strlen(F2FS_OPTION(sbi).extensions[i]))
+				continue;
+			len += scnprintf(buf + len, PAGE_SIZE - len, "%s\n",
+					 F2FS_OPTION(sbi).extensions[i]);
+		}
+		f2fs_up_read(&sbi->sb_lock);
+		return len;
+	}
+
+	if (!strcmp(a->attr.name, "compress_noextension")) {
+		int len = 0, i;
+
+		f2fs_down_read(&sbi->sb_lock);
+		for (i = 0; i < F2FS_OPTION(sbi).nocompress_ext_cnt; i++) {
+			if (!strlen(F2FS_OPTION(sbi).noextensions[i]))
+				continue;
+			len += scnprintf(buf + len, PAGE_SIZE - len, "%s\n",
+					 F2FS_OPTION(sbi).noextensions[i]);
+		}
+		f2fs_up_read(&sbi->sb_lock);
+		return len;
+	}
+
+	if (!strcmp(a->attr.name, "compress_log_size")) {
+		int log_size;
+
+		f2fs_down_read(&sbi->sb_lock);
+		log_size = F2FS_OPTION(sbi).compress_log_size;
+		f2fs_up_read(&sbi->sb_lock);
+		return sysfs_emit(buf, "%d\n", log_size);
+	}
+#endif
 #endif
 
 	if (!strcmp(a->attr.name, "gc_segment_mode"))
@@ -356,6 +419,16 @@ static ssize_t f2fs_sbi_show(struct f2fs_attr *a,
 	if (!strcmp(a->attr.name, "revoked_atomic_block"))
 		return sysfs_emit(buf, "%llu\n", sbi->revoked_atomic_block);
 
+#ifdef CONFIG_F2FS_APPBOOST
+	if (!strcmp(a->attr.name, "appboost")) {
+		return sysfs_emit(buf, "%u\n",
+			sbi->appboost);
+	}
+	if (!strcmp(a->attr.name, "appboost_max_blocks")) {
+		return sysfs_emit(buf, "%u\n",
+			sbi->appboost_max_blocks);
+	}
+#endif
 	ui = (unsigned int *)(ptr + a->offset);
 
 	return sysfs_emit(buf, "%u\n", *ui);
@@ -366,7 +439,7 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 			const char *buf, size_t count)
 {
 	unsigned char *ptr;
-	unsigned long t;
+	unsigned long long t;
 	unsigned int *ui;
 	ssize_t ret;
 
@@ -409,6 +482,83 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 		return ret ? ret : count;
 	}
 
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!strcmp(a->attr.name, "compress_extension")) {
+		char *token = NULL, *name = strim((char *)buf);
+		unsigned char (*ext)[F2FS_EXTENSION_LEN];
+		unsigned char extensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN];
+		unsigned char ext_cnt = 0;
+		ssize_t sz;
+
+		memset(extensions, 0, sizeof(extensions));
+		while (1) {
+			token = strsep(&name, ",");
+			if (!token)
+				break;
+			sz = strlen(token);
+			if (sz == 0)
+				continue;
+			if (sz >= F2FS_EXTENSION_LEN)
+				return -EINVAL;
+			if (ext_cnt >= COMPRESS_EXT_NUM)
+				return -EINVAL;
+			if (!strcmp(token, "disable")) {
+				memset(extensions, 0, sizeof(extensions));
+				ext_cnt = 0;
+				break;
+			}
+			memcpy(extensions[ext_cnt++], token, sz);
+		}
+
+		ext = F2FS_OPTION(sbi).extensions;
+		f2fs_down_write(&sbi->sb_lock);
+		memset(ext, 0, sizeof(F2FS_OPTION(sbi).extensions));
+		memcpy(ext, extensions, ext_cnt * F2FS_EXTENSION_LEN);
+		F2FS_OPTION(sbi).compress_ext_cnt = ext_cnt;
+		/*
+		 * no need to persist the new extensions since RUS could
+		 * update it manually
+		 */
+		f2fs_up_write(&sbi->sb_lock);
+		return count;
+	}
+
+	if (!strcmp(a->attr.name, "compress_noextension")) {
+		char *token = NULL, *name = strim((char *)buf);
+		unsigned char (*noext)[F2FS_EXTENSION_LEN];
+		unsigned char noextensions[COMPRESS_EXT_NUM][F2FS_EXTENSION_LEN];
+		unsigned char noext_cnt = 0;
+		ssize_t sz;
+
+		memset(noextensions, 0, sizeof(noextensions));
+		while (1) {
+			token = strsep(&name, ",");
+			if (!token)
+				break;
+			sz = strlen(token);
+			if (sz == 0)
+				continue;
+			if (sz >= F2FS_EXTENSION_LEN)
+				return -EINVAL;
+			if (noext_cnt >= COMPRESS_EXT_NUM)
+				return -EINVAL;
+			memcpy(noextensions[noext_cnt++], token, sz);
+		}
+
+		noext = F2FS_OPTION(sbi).noextensions;
+		f2fs_down_write(&sbi->sb_lock);
+		memset(noext, 0, sizeof(F2FS_OPTION(sbi).noextensions));
+		memcpy(noext, noextensions, noext_cnt * F2FS_EXTENSION_LEN);
+		F2FS_OPTION(sbi).nocompress_ext_cnt = noext_cnt;
+		/*
+		 * no need to persist the new noextensions since RUS could
+		 * update it manually
+		 */
+		f2fs_up_write(&sbi->sb_lock);
+		return count;
+	}
+#endif
+
 	if (!strcmp(a->attr.name, "ckpt_thread_ioprio")) {
 		const char *name = strim((char *)buf);
 		struct ckpt_req_control *cprc = &sbi->cprc_info;
@@ -443,7 +593,7 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 
 	ui = (unsigned int *)(ptr + a->offset);
 
-	ret = kstrtoul(skip_spaces(buf), 0, &t);
+	ret = kstrtoull(skip_spaces(buf), 0, &t);
 	if (ret < 0)
 		return ret;
 #ifdef CONFIG_F2FS_FAULT_INJECTION
@@ -460,7 +610,7 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 #endif
 	if (a->struct_type == RESERVED_BLOCKS) {
 		spin_lock(&sbi->stat_lock);
-		if (t > (unsigned long)(sbi->user_block_count -
+		if (t > (unsigned long long)(sbi->user_block_count -
 				F2FS_OPTION(sbi).root_reserved_blocks -
 				sbi->blocks_per_seg *
 				SM_I(sbi)->additional_reserved_segments)) {
@@ -496,6 +646,13 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 		return count;
 	}
 
+	if (!strcmp(a->attr.name, "discard_io_aware")) {
+		if (t >= DPOLICY_IO_AWARE_MAX)
+			return -EINVAL;
+		*ui = t;
+		return count;
+	}
+
 	if (!strcmp(a->attr.name, "max_ordered_discard")) {
 		if (t == 0 || t > MAX_PLIST_NUM)
 			return -EINVAL;
@@ -613,6 +770,17 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 		*ui = t;
 		return count;
 	}
+
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	if (!strcmp(a->attr.name, "compress_log_size")) {
+		if (t < MIN_COMPRESS_LOG_SIZE || t > MAX_COMPRESS_LOG_SIZE)
+			return -EINVAL;
+		f2fs_down_write(&sbi->sb_lock);
+		F2FS_OPTION(sbi).compress_log_size = (unsigned char)t;
+		f2fs_up_write(&sbi->sb_lock);
+		return count;
+	}
+#endif
 #endif
 
 	if (!strcmp(a->attr.name, "atgc_candidate_ratio")) {
@@ -659,6 +827,19 @@ static ssize_t __sbi_store(struct f2fs_attr *a,
 			return -EINVAL;
 		return count;
 	}
+#ifdef CONFIG_F2FS_APPBOOST
+	if (!strcmp(a->attr.name, "appboost")) {
+		if (t)
+			sbi->appboost = 1;
+		else
+			sbi->appboost = 0;
+		return count;
+	}
+	if (!strcmp(a->attr.name, "appboost_max_blocks")) {
+		sbi->appboost_max_blocks = t;
+		return count;
+	}
+#endif
 
 	if (!strcmp(a->attr.name, "max_fragment_hole")) {
 		if (t >= MIN_FRAGMENT_SIZE && t <= MAX_FRAGMENT_SIZE)
@@ -802,15 +983,54 @@ static void f2fs_sb_release(struct kobject *kobj)
 static ssize_t f2fs_feature_show(struct f2fs_attr *a,
 		struct f2fs_sb_info *sbi, char *buf)
 {
+	if (!strcmp(a->attr.name, "seqzone")) {
+		if (!seqzone_switch)
+			return sysfs_emit(buf, "unsupported\n");
+		else
+			return sysfs_emit(buf, seqzone_switch);
+	}
+
 	return sysfs_emit(buf, "supported\n");
 }
 
+static ssize_t f2fs_may_compr_show(struct f2fs_attr *a,
+		struct f2fs_sb_info *sbi, char *buf)
+{
+	if (!strcmp(a->attr.name, "may_compress"))
+		return sprintf(buf, "%d", may_compress ? 1 : 0);
+	else if (!strcmp(a->attr.name, "may_set_compr_fl"))
+		return sprintf(buf, "%d", may_set_compr_fl ? 1 : 0);
+	return -EINVAL;
+}
+
+static ssize_t f2fs_may_compr_store(struct f2fs_attr *a,
+			struct f2fs_sb_info *sbi, const char *buf, size_t count)
+{
+	int val, ret;
+
+	ret = kstrtoint(buf, 0, &val);
+	if (ret < 0)
+		return ret;
+	if (!strcmp(a->attr.name, "may_compress"))
+		may_compress = val == 0 ? false : true;
+	else if (!strcmp(a->attr.name, "may_set_compr_fl"))
+		may_set_compr_fl = val == 0 ? false : true;
+	return count;
+}
+
 #define F2FS_FEATURE_RO_ATTR(_name)				\
 static struct f2fs_attr f2fs_attr_##_name = {			\
 	.attr = {.name = __stringify(_name), .mode = 0444 },	\
 	.show	= f2fs_feature_show,				\
 }
 
+#define F2FS_FEATURE_RW_ATTR(_name)				\
+static struct f2fs_attr f2fs_attr_##_name = {			\
+	.attr = {.name = __stringify(_name), .mode = 0644 },	\
+	.show	= f2fs_may_compr_show,				\
+	.store	= f2fs_may_compr_store,				\
+}
+
 static ssize_t f2fs_sb_feature_show(struct f2fs_attr *a,
 		struct f2fs_sb_info *sbi, char *buf)
 {
@@ -872,6 +1092,7 @@ F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, max_discard_issue_time, max_discard_
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_io_aware_gran, discard_io_aware_gran);
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_urgent_util, discard_urgent_util);
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_granularity, discard_granularity);
+F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, discard_io_aware, discard_io_aware);
 F2FS_RW_ATTR(DCC_INFO, discard_cmd_control, max_ordered_discard, max_ordered_discard);
 F2FS_RW_ATTR(RESERVED_BLOCKS, f2fs_sb_info, reserved_blocks, reserved_blocks);
 F2FS_RW_ATTR(SM_INFO, f2fs_sm_info, ipu_policy, ipu_policy);
@@ -884,6 +1105,13 @@ F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ram_thresh, ram_thresh);
 F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, ra_nid_pages, ra_nid_pages);
 F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, dirty_nats_ratio, dirty_nats_ratio);
 F2FS_RW_ATTR(NM_INFO, f2fs_nm_info, max_roll_forward_node_blocks, max_rf_node_blocks);
+#ifdef CONFIG_F2FS_APPBOOST
+F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, appboost, appboost);
+F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, appboost_max_blocks, appboost_max_blocks);
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, seq_zone, seq_zone);
+#endif
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, max_victim_search, max_victim_search);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, migration_granularity, migration_granularity);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, dir_level, dir_level);
@@ -967,8 +1195,24 @@ F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compr_saved_block, compr_saved_block);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compr_new_inode, compr_new_inode);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compress_percent, compress_percent);
 F2FS_RW_ATTR(F2FS_SBI, f2fs_sb_info, compress_watermark, compress_watermark);
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_layout, compress_layout);
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_extension, extensions);
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_noextension, noextensions);
+F2FS_RW_ATTR(COMP_EXT, f2fs_mount_info, compress_log_size, compress_log_size);
+#endif
 #endif
 F2FS_FEATURE_RO_ATTR(pin_file);
+#ifdef CONFIG_F2FS_FS_DEDUP
+F2FS_FEATURE_RO_ATTR(dedup);
+F2FS_FEATURE_RO_ATTR(snapshot);
+#endif
+F2FS_FEATURE_RW_ATTR(may_compress);
+F2FS_FEATURE_RW_ATTR(may_set_compr_fl);
+
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_FEATURE_RO_ATTR(seqzone);
+#endif
 
 /* For ATGC */
 F2FS_RW_ATTR(ATGC_INFO, atgc_management, atgc_candidate_ratio, candidate_ratio);
@@ -1011,6 +1255,7 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(discard_io_aware_gran),
 	ATTR_LIST(discard_urgent_util),
 	ATTR_LIST(discard_granularity),
+	ATTR_LIST(discard_io_aware),
 	ATTR_LIST(max_ordered_discard),
 	ATTR_LIST(pending_discard),
 	ATTR_LIST(gc_mode),
@@ -1021,6 +1266,13 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(min_hot_blocks),
 	ATTR_LIST(min_ssr_sections),
 	ATTR_LIST(max_victim_search),
+#ifdef CONFIG_F2FS_APPBOOST
+	ATTR_LIST(appboost),
+	ATTR_LIST(appboost_max_blocks),
+#endif
+#ifdef CONFIG_F2FS_SEQZONE
+	ATTR_LIST(seq_zone),
+#endif
 	ATTR_LIST(migration_granularity),
 	ATTR_LIST(dir_level),
 	ATTR_LIST(ram_thresh),
@@ -1076,6 +1328,12 @@ static struct attribute *f2fs_attrs[] = {
 	ATTR_LIST(compr_new_inode),
 	ATTR_LIST(compress_percent),
 	ATTR_LIST(compress_watermark),
+#ifdef CONFIG_F2FS_FS_COMPRESSION_FIXED_OUTPUT
+	ATTR_LIST(compress_layout),
+	ATTR_LIST(compress_extension),
+	ATTR_LIST(compress_noextension),
+	ATTR_LIST(compress_log_size),
+#endif
 #endif
 	/* For ATGC */
 	ATTR_LIST(atgc_candidate_ratio),
@@ -1129,6 +1387,16 @@ static struct attribute *f2fs_feat_attrs[] = {
 	ATTR_LIST(compression),
 #endif
 	ATTR_LIST(pin_file),
+#ifdef CONFIG_F2FS_FS_DEDUP
+	ATTR_LIST(dedup),
+	ATTR_LIST(snapshot),
+#endif
+
+#ifdef CONFIG_F2FS_SEQZONE
+	ATTR_LIST(seqzone),
+#endif
+	ATTR_LIST(may_compress),
+	ATTR_LIST(may_set_compr_fl),
 	NULL,
 };
 ATTRIBUTE_GROUPS(f2fs_feat);
@@ -1155,8 +1423,13 @@ F2FS_SB_FEATURE_RO_ATTR(verity, VERITY);
 F2FS_SB_FEATURE_RO_ATTR(sb_checksum, SB_CHKSUM);
 F2FS_SB_FEATURE_RO_ATTR(casefold, CASEFOLD);
 F2FS_SB_FEATURE_RO_ATTR(compression, COMPRESSION);
+#ifdef CONFIG_F2FS_DEDUP
+F2FS_SB_FEATURE_RO_ATTR(dedup, DEDUP);
+#endif
 F2FS_SB_FEATURE_RO_ATTR(readonly, RO);
-
+#ifdef CONFIG_F2FS_SEQZONE
+F2FS_SB_FEATURE_RO_ATTR(seqzone, SEQZONE);
+#endif
 static struct attribute *f2fs_sb_feat_attrs[] = {
 	ATTR_LIST(sb_encryption),
 	ATTR_LIST(sb_block_zoned),
@@ -1171,7 +1444,13 @@ static struct attribute *f2fs_sb_feat_attrs[] = {
 	ATTR_LIST(sb_sb_checksum),
 	ATTR_LIST(sb_casefold),
 	ATTR_LIST(sb_compression),
+#ifdef CONFIG_F2FS_DEDUP
+	ATTR_LIST(sb_dedup),
+#endif
 	ATTR_LIST(sb_readonly),
+#ifdef CONFIG_F2FS_SEQZONE
+	ATTR_LIST(sb_seqzone),
+#endif
 	NULL,
 };
 ATTRIBUTE_GROUPS(f2fs_sb_feat);
diff --git a/fs/f2fs/verity.c b/fs/f2fs/verity.c
index 4fc95f353..36830b062 100644
--- a/fs/f2fs/verity.c
+++ b/fs/f2fs/verity.c
@@ -139,6 +139,15 @@ static int f2fs_begin_enable_verity(struct file *filp)
 	if (err)
 		return err;
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+	mark_file_modified(inode);
+	if (f2fs_is_outer_inode(inode)) {
+		err = f2fs_revoke_deduped_inode(inode, __func__);
+		if (err)
+			return err;
+	}
+#endif
+
 	set_inode_flag(inode, FI_VERITY_IN_PROGRESS);
 	return 0;
 }
diff --git a/fs/f2fs/xattr.c b/fs/f2fs/xattr.c
old mode 100644
new mode 100755
index 65437c18e..9f1fb1818
--- a/fs/f2fs/xattr.c
+++ b/fs/f2fs/xattr.c
@@ -661,14 +661,11 @@ static int __f2fs_setxattr(struct inode *inode, int index,
 	here = __find_xattr(base_addr, last_base_addr, NULL, index, len, name);
 	if (!here) {
 		if (!F2FS_I(inode)->i_xattr_nid) {
-			error = f2fs_recover_xattr_data(inode, NULL);
 			f2fs_notice(F2FS_I_SB(inode),
-				"recover xattr in inode (%lu), error(%d)",
-					inode->i_ino, error);
-			if (!error) {
-				kfree(base_addr);
-				goto retry;
-			}
+				"recover xattr in inode (%lu)", inode->i_ino);
+			f2fs_recover_xattr_data(inode, NULL);
+			kfree(base_addr);
+			goto retry;
 		}
 		f2fs_err(F2FS_I_SB(inode), "set inode (%lu) has corrupted xattr",
 								inode->i_ino);
@@ -758,12 +755,6 @@ static int __f2fs_setxattr(struct inode *inode, int index,
 		memcpy(pval, value, size);
 		last->e_value_size = cpu_to_le16(size);
 		new_hsize += newsize;
-		/*
-		 * Explicitly add the null terminator.  The unused xattr space
-		 * is supposed to always be zeroed, which would make this
-		 * unnecessary, but don't depend on that.
-		 */
-		*(u32 *)((u8 *)last + newsize) = 0;
 	}
 
 	error = write_all_xattrs(inode, new_hsize, base_addr, ipage);
diff --git a/fs/f2fs/xattr.h b/fs/f2fs/xattr.h
index 416d65277..b1811c392 100644
--- a/fs/f2fs/xattr.h
+++ b/fs/f2fs/xattr.h
@@ -83,6 +83,7 @@ struct f2fs_xattr_entry {
 				sizeof(struct f2fs_xattr_header) -	\
 				sizeof(struct f2fs_xattr_entry))
 
+#define MIN_INLINE_XATTR_SIZE (sizeof(struct f2fs_xattr_header) / sizeof(__le32))
 #define MAX_INLINE_XATTR_SIZE						\
 			(DEF_ADDRS_PER_INODE -				\
 			F2FS_TOTAL_EXTRA_ATTR_SIZE / sizeof(__le32) -	\
diff --git a/include/linux/f2fs_fs.h b/include/linux/f2fs_fs.h
index e5951002e..1cea5aeea 100644
--- a/include/linux/f2fs_fs.h
+++ b/include/linux/f2fs_fs.h
@@ -24,6 +24,8 @@
 #define NULL_ADDR		((block_t)0)	/* used as block_t addresses */
 #define NEW_ADDR		((block_t)-1)	/* used as block_t addresses */
 #define COMPRESS_ADDR		((block_t)-2)	/* used as compressed data flag */
+#define DEDUP_ADDR		((block_t)-1024)/* used as block_t addresses */
+#define SEQZONE_ADDR		((block_t)-1025)/* used as seqzone addresses */
 
 #define F2FS_BYTES_TO_BLK(bytes)	((bytes) >> F2FS_BLKSIZE_BITS)
 #define F2FS_BLK_TO_BYTES(blk)		((blk) << F2FS_BLKSIZE_BITS)
@@ -332,9 +334,15 @@ struct f2fs_inode {
 			__u8 i_compress_algorithm;	/* compress algorithm */
 			__u8 i_log_cluster_size;	/* log of cluster size */
 			__le16 i_compress_flag;		/* compress flag */
-						/* 0 bit: chksum flag
-						 * [8,15] bits: compress level
-						 */
+							/* 0 bit: chksum flag
+							 * [1,2] bits: compress layout
+							 * 3 bit: readpage updates atime
+							 * [4,7] bits: reserved
+							 * [8,15] bits: compress level
+							 */
+			__le32 i_inner_ino;     /* for layered inode */
+			__le32 i_dedup_flags;   /* dedup file attributes */
+			__le32 i_dedup_rsvd;    /* reserved for dedup */
 			__le32 i_extra_end[0];	/* for attribute size calculation */
 		} __packed;
 		__le32 i_addr[DEF_ADDRS_PER_INODE];	/* Pointers to data blocks */
diff --git a/include/trace/events/f2fs.h b/include/trace/events/f2fs.h
index 222324551..cb6be14f8 100644
--- a/include/trace/events/f2fs.h
+++ b/include/trace/events/f2fs.h
@@ -2209,8 +2209,11 @@ DECLARE_EVENT_CLASS(f2fs__rw_start,
 		__string(cmdline, command)
 		__field(pid_t, pid)
 		__field(ino_t, ino)
+#ifdef CONFIG_F2FS_APPBOOST
+		__field(u64, mtime)
+		__field(u32, i_generation)
+#endif
 	),
-
 	TP_fast_assign(
 		/*
 		 * Replace the spaces in filenames and cmdlines
@@ -2226,13 +2229,24 @@ DECLARE_EVENT_CLASS(f2fs__rw_start,
 		(void)strreplace(__get_str(cmdline), ' ', '_');
 		__entry->pid = pid;
 		__entry->ino = inode->i_ino;
+#ifdef CONFIG_F2FS_APPBOOST
+		__entry->mtime = timespec64_to_ns(&inode->i_mtime);
+		__entry->i_generation = inode->i_generation;
+#endif
 	),
-
+#ifdef CONFIG_F2FS_APPBOOST
+	TP_printk("entry_name %s, offset %llu, bytes %d, cmdline %s,"
+		" pid %d, i_size %llu, ino %lu, mtime %llu, i_generation %u",
+		__get_str(pathbuf), __entry->offset, __entry->bytes,
+		__get_str(cmdline), __entry->pid, __entry->i_size,
+		(unsigned long) __entry->ino, __entry->mtime,  __entry->i_generation)
+#else
 	TP_printk("entry_name %s, offset %llu, bytes %d, cmdline %s,"
 		" pid %d, i_size %llu, ino %lu",
 		__get_str(pathbuf), __entry->offset, __entry->bytes,
 		__get_str(cmdline), __entry->pid, __entry->i_size,
 		(unsigned long) __entry->ino)
+#endif
 );
 
 DECLARE_EVENT_CLASS(f2fs__rw_end,
@@ -2282,12 +2296,128 @@ DEFINE_EVENT(f2fs__rw_start, f2fs_datawrite_start,
 );
 
 DEFINE_EVENT(f2fs__rw_end, f2fs_datawrite_end,
-
 	TP_PROTO(struct inode *inode, loff_t offset, int bytes),
 
 	TP_ARGS(inode, offset, bytes)
 );
 
+#ifdef CONFIG_F2FS_FS_DEDUP
+DECLARE_EVENT_CLASS(f2fs__dedup_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner),
+
+	TP_STRUCT__entry(
+		__field(dev_t,  dev)
+		__field(ino_t,  ino)
+		__field(ino_t,  pino)
+		__field(loff_t, size)
+		__field(unsigned int, nlink)
+		__field(ino_t,  inner_ino)
+		__field(unsigned int, inner_nlink)
+	),
+
+	TP_fast_assign(
+		__entry->dev    = inode->i_sb->s_dev;
+		__entry->ino    = inode->i_ino;
+		__entry->pino   = F2FS_I(inode)->i_pino;
+		__entry->nlink  = inode->i_nlink;
+		__entry->size   = inode->i_size;
+		__entry->inner_ino      = inner->i_ino;
+		__entry->inner_nlink    = inner->i_nlink;
+	),
+
+	TP_printk("dev = (%d,%d), ino = %lu, pino = %lu, "
+		"i_size = %lld, i_nlink = %u, "
+		"inner = %lu, inner i_nlink = %u",
+		show_dev_ino(__entry),
+		(unsigned long)__entry->pino,
+		__entry->size,
+		(unsigned int)__entry->nlink,
+		(unsigned long)__entry->inner_ino,
+		(unsigned int)__entry->inner_nlink)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_ioc_create_layered_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_ioc_dedup_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_revoke_inode,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_revoke_fail,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_inode, f2fs_dedup_dec_inner_link,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DECLARE_EVENT_CLASS(f2fs__dedup_map,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner),
+
+	TP_STRUCT__entry(
+		__field(dev_t,  dev)
+		__field(ino_t,  ino)
+		__field(loff_t, size)
+		__field(ino_t,  inner_ino)
+		__field(unsigned int, inner_nlink)
+	),
+
+	TP_fast_assign(
+		__entry->dev    = inode->i_sb->s_dev;
+		__entry->ino    = inode->i_ino;
+		__entry->size   = inode->i_size;
+		__entry->inner_ino      = inner->i_ino;
+		__entry->inner_nlink    = inner->i_nlink;
+	),
+
+	TP_printk("dev = (%d,%d), outer ino = %lu, i_size = %lld, "
+		"map to inner ino = %lu, inner i_nlink = %u",
+		show_dev_ino(__entry),
+		__entry->size,
+		(unsigned long)__entry->inner_ino,
+		(unsigned int)__entry->inner_nlink)
+);
+
+DEFINE_EVENT(f2fs__dedup_map, f2fs_dedup_map_readpage,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+
+DEFINE_EVENT(f2fs__dedup_map, f2fs_dedup_map_blocks,
+
+	TP_PROTO(struct inode *inode, struct inode *inner),
+
+	TP_ARGS(inode, inner)
+);
+#endif /* CONFIG_F2FS_FS_DEDUP */
 #endif /* _TRACE_F2FS_H */
 
  /* This part must be outside protection */
diff --git a/include/uapi/linux/f2fs.h b/include/uapi/linux/f2fs.h
old mode 100644
new mode 100755
index 955d440be..481ab7360
--- a/include/uapi/linux/f2fs.h
+++ b/include/uapi/linux/f2fs.h
@@ -43,6 +43,50 @@
 #define F2FS_IOC_DECOMPRESS_FILE	_IO(F2FS_IOCTL_MAGIC, 23)
 #define F2FS_IOC_COMPRESS_FILE		_IO(F2FS_IOCTL_MAGIC, 24)
 #define F2FS_IOC_START_ATOMIC_REPLACE	_IO(F2FS_IOCTL_MAGIC, 25)
+#define F2FS_IOC_GET_EXTRA_ATTR		_IOR(F2FS_IOCTL_MAGIC, 26,	\
+						struct f2fs_extra_attr)
+#define F2FS_IOC_SET_EXTRA_ATTR		_IOW(F2FS_IOCTL_MAGIC, 27,	\
+						struct f2fs_extra_attr)
+#define F2FS_APPBOOST_IOC_BASE (50)
+enum {
+	APPBOOST_PRELOAD = F2FS_APPBOOST_IOC_BASE,
+	APPBOOST_START_MERGE,
+	APPBOOST_END_MERGE,
+	APPBOOST_ABORT_PRELOAD,
+	SET_SEQZONE,
+	GET_SEQZONE,
+};
+#define F2FS_IOC_PRELOAD_FILE		_IOW(F2FS_IOCTL_MAGIC,		\
+						APPBOOST_PRELOAD, __u32)
+#define F2FS_IOC_START_MERGE_FILE	_IOW(F2FS_IOCTL_MAGIC,		\
+						APPBOOST_START_MERGE, struct merge_file_user)
+#define F2FS_IOC_END_MERGE_FILE		_IO(F2FS_IOCTL_MAGIC, APPBOOST_END_MERGE)
+#define F2FS_IOC_ABORT_PRELOAD_FILE	_IO(F2FS_IOCTL_MAGIC, APPBOOST_ABORT_PRELOAD)
+#define F2FS_IOC_SET_SEQZONE_FILE	_IOW(F2FS_IOCTL_MAGIC, SET_SEQZONE, __u32)
+#define F2FS_IOC_GET_SEQZONE_FILE	_IOR(F2FS_IOCTL_MAGIC, GET_SEQZONE, __u32)
+
+#define F2FS_DEDUP_IOC_BASE	(100)
+#define F2FS_IOC_DEDUP_CREATE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 0, struct f2fs_dedup_src)
+#define F2FS_IOC_DEDUP_FILE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 1, struct f2fs_dedup_dst)
+#define F2FS_IOC_DEDUP_REVOKE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 2, struct f2fs_dedup_revoke)
+#define F2FS_IOC_DEDUP_GET_FILE_INFO	_IOR(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 3, struct f2fs_dedup_file_info)
+#define F2FS_IOC_DEDUP_GET_SYS_INFO	_IOR(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 4, struct f2fs_dedup_sys_info)
+
+#define F2FS_IOC_CLONE_FILE		_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 5, struct f2fs_clone_info)
+#define F2FS_IOC_MODIFY_CHECK	_IOWR(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 6, struct f2fs_modify_check_info)
+#define F2FS_IOC_DEDUP_PERM_CHECK \
+			_IO(F2FS_IOCTL_MAGIC, F2FS_DEDUP_IOC_BASE + 7)
+#define F2FS_IOC_SNAPSHOT_CREATE	_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 8, struct f2fs_dedup_dst)
+#define F2FS_IOC_SNAPSHOT_PREPARE	_IOW(F2FS_IOCTL_MAGIC,	\
+					F2FS_DEDUP_IOC_BASE + 9, struct f2fs_dedup_src)
 
 /*
  * should be same as XFS_IOC_GOINGDOWN.
@@ -96,4 +140,84 @@ struct f2fs_comp_option {
 	__u8 log_cluster_size;
 };
 
+struct merge_file_user {
+	unsigned ino;
+	unsigned extent_count;
+	unsigned int i_generation;
+	unsigned int REV;
+	__u64 mtime;
+	__u8 __user *extents;
+};
+
+struct f2fs_comp_option_v2 {
+	__u8 algorithm;
+	__u8 log_cluster_size;
+	__u8 level;
+	__u8 flag;
+};
+
+enum {
+	F2FS_EXTRA_ATTR_TOTAL_SIZE,		/* ro, size of extra attr area */
+	F2FS_EXTRA_ATTR_ISIZE,			/* ro, i_extra_isize */
+	F2FS_EXTRA_ATTR_INLINE_XATTR_SIZE,	/* rw, i_inline_xattr_size */
+	F2FS_EXTRA_ATTR_PROJID,			/* ro, i_projid */
+	F2FS_EXTRA_ATTR_INODE_CHKSUM,		/* ro, i_inode_chksum */
+	F2FS_EXTRA_ATTR_CRTIME,			/* ro, i_crtime, i_crtime_nsec */
+	F2FS_EXTRA_ATTR_COMPR_BLOCKS,		/* ro, i_compr_blocks */
+	F2FS_EXTRA_ATTR_COMPR_OPTION,		/* rw, i_compress_algorithm,
+						 *     i_log_cluster_size,
+						 *     i_compress_flag
+						 */
+	F2FS_EXTRA_ATTR_MAX,
+};
+
+struct f2fs_extra_attr {
+	__u8 field;		/* F2FS_EXTRA_ATTR_* */
+	__u8 rsvd1;
+	__u16 attr_size;	/* size of @attr */
+	__u32 rsvd2;
+	__u64 attr;		/* attr value or pointer */
+};
+
+/* F2FS_IOC_DEDUP_CREATE */
+struct f2fs_dedup_src {
+	int inner_fd;
+};
+
+/* F2FS_IOC_DEDUP_FILE */
+struct f2fs_dedup_dst {
+	int base_fd;
+	int tmp_fd;
+};
+
+/* F2FS_IOC_DEDUP_REVOKE */
+struct f2fs_dedup_revoke {
+	int revoke_fd;
+};
+
+/* F2FS_IOC_DEDUP_GET_FILE_INFO */
+struct f2fs_dedup_file_info {
+	short is_deduped;
+	short is_layered;
+	int group;
+};
+
+/* F2FS_IOC_DEDUP_GET_SYS_INFO */
+struct f2fs_dedup_sys_info {
+	__u64 file_count;
+	__u64 file_space;		/* MB */
+};
+
+struct f2fs_clone_info {
+	int src_fd;
+	int flags;	/* meta/data index */
+};
+
+struct f2fs_modify_check_info {
+	int flag;	/* data/meta */
+	int mode;	/* set/get/clear */
+};
+
+#define F2FS_KBYTE_SHIFT	10
+
 #endif /* _UAPI_LINUX_F2FS_H */
diff --git a/include/uapi/linux/stat.h b/include/uapi/linux/stat.h
index 7cab2c65d..47d79daed 100644
--- a/include/uapi/linux/stat.h
+++ b/include/uapi/linux/stat.h
@@ -184,6 +184,7 @@ struct statx {
 #define STATX_ATTR_IMMUTABLE		0x00000010 /* [I] File is marked immutable */
 #define STATX_ATTR_APPEND		0x00000020 /* [I] File is append-only */
 #define STATX_ATTR_NODUMP		0x00000040 /* [I] File is not to be dumped */
+#define STATX_ATTR_NOCOMPR		0x00000400 /* [I] File is worthless to compress */
 #define STATX_ATTR_ENCRYPTED		0x00000800 /* [I] File requires key to decrypt in fs */
 #define STATX_ATTR_AUTOMOUNT		0x00001000 /* Dir: Automount trigger */
 #define STATX_ATTR_MOUNT_ROOT		0x00002000 /* Root of a mount */
